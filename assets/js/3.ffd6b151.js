(window.webpackJsonp=window.webpackJsonp||[]).push([[3],{152:function(e,n,a){"use strict";a.r(n);var t=a(0),i=Object(t.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var e=this,n=e.$createElement,a=e._self._c||n;return a("div",{staticClass:"content"},[a("h1",{attrs:{id:"literature"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#literature","aria-hidden":"true"}},[e._v("#")]),e._v(" Literature")]),a("p",[e._v("In this article, we describe the basic ideas of existing methods for musical source separation (and specifically Lead/Accompaniment Separation) classified into three main categories: signal processing, audio\nmodeling and probability theory. The interested reader is strongly\nencouraged to delve into the many online courses or textbooks available\nfor a more detailed presentation of these topics, such\nas ["),a("a",{attrs:{href:"#ref-zolzer11"}},[e._v("12")]),e._v("], ["),a("a",{attrs:{href:"#ref-muller2015"}},[e._v("13")]),e._v("] for signal\nprocessing, ["),a("a",{attrs:{href:"#ref-loizou13"}},[e._v("9")]),e._v("] for speech modeling, and\n["),a("a",{attrs:{href:"#ref-jaynes2003probability"}},[e._v("14")]),e._v("], ["),a("a",{attrs:{href:"#ref-cappe2005"}},[e._v("15")]),e._v("] for\nprobability theory.")]),a("div",{staticClass:"tip custom-block"},[a("p",{staticClass:"custom-block-title"},[e._v("CITE")]),a("p",[e._v("This article is based on "),a("a",{attrs:{href:"https://ieeexplore.ieee.org/document/8336997/",target:"_blank",rel:"noopener noreferrer"}},[e._v("a publication in the IEEE Journal of Transactions")]),e._v(".\nIf you want to cite this article, please use the following reference.")]),a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("@ARTICLE{rafii18,\n  author={Z. Rafii and A. Liutkus and\n          F. R. Stöter and S. I. Mimilakis\n          and D. FitzGerald and B. Pardo},\n  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},\n  title={An Overview of Lead and Accompaniment Separation in Music},\n  year={2018},\n  volume={26},\n  number={8},\n  pages={1307-1335},\n  doi={10.1109/TASLP.2018.2825440},\n  ISSN={2329-9290},\n  month={Aug}\n}\n\n")])])]),a("h3",{attrs:{id:"signal-processing"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#signal-processing","aria-hidden":"true"}},[e._v("#")]),e._v(" Signal processing")]),a("p",[e._v("Sound is a series of pressure waves in the air. It is recorded as a\n"),a("em",[e._v("waveform")]),e._v(", a time-series of measurements of the displacement of the\nmicrophone diaphragm in response to these pressure waves. Sound is\nreproduced if a loudspeaker diaphragm is moved according to the recorded\nwaveform. Multichannel signals simply consist of several waveforms,\ncaptured by more than one microphone. Typically, music signals are\nstereophonic, containing two waveforms.")]),a("p",[e._v("Microphone displacement is typically measured at a fixed  "),a("em",[e._v("sampling\nfrequency")]),e._v(". In music processing, it is common to have sampling\nfrequencies of (44.1) kHz (the sample frequency on a compact disc) or\n(48) kHz, which are higher than the typical sampling rates of\n(16) kHz or (8) kHz used for speech in telephony. This is because\nmusical signals contain much higher frequency content than speech and\nthe goal is aesthetic beauty in addition to basic intelligibility.")]),a("p",[e._v("A time-frequency (TF) representation of sound is a matrix that encodes\nthe time-varying "),a("em",[e._v("spectrum")]),e._v(" of the waveform. Its entries are called\nTF "),a("em",[e._v("bins")]),e._v(" and encode the varying spectrum of the waveform for all time\nframes and frequency channels. The most commonly-used TF representation\nis the short time Fourier transform (STFT) ["),a("a",{attrs:{href:"#ref-mcaulay86"}},[e._v("16")]),e._v("],\nwhich has complex entries: the angle accounts for the phase, i.e., the\nactual shift of the corresponding sinusoid at that time bin and\nfrequency bin, and the magnitude accounts for the amplitude of that\nsinusoid in the signal. The magnitude (or power) of the STFT is called\n"),a("em",[e._v("spectrogram")]),e._v(". When the mixture is multichannel, the TF representation\nfor each channel is computed, leading to a three-dimensional array:\nfrequency, time and channel.")]),a("p",[e._v("A TF representation is typically used as a first step in processing the\naudio because sources tend to be less overlapped in the TF\nrepresentation than in the waveform ["),a("a",{attrs:{href:"#ref-rickard02"}},[e._v("17")]),e._v("]. This makes\nit easier to select portions of a mixture that correspond to only a\nsingle source. An STFT is typically used because it can be inverted back\nto the original waveform. Therefore, modifications made to the STFT can\nbe used to create a modified waveform. Generally, a linear mixing\nprocess is considered, i.e., the mixture signal is equal to the sum of\nthe source signals. Since the Fourier transform is a linear operation,\nthis equality holds for the STFT. While that is not the case for the\nmagnitude (or power) of the STFT, it is commonly assumed that the\nspectrograms of the sources sum to the spectrogram of the mixture.")]),a("p",[e._v("In many methods, the separated sources are obtained by "),a("em",[e._v("filtering")]),e._v(" the\nmixture. This can be understood as performing some equalization on the\nmixture, where each frequency is attenuated or kept intact. Since both\nthe lead and the accompaniment signals change over time, the filter also\nchanges. This is typically done using a TF "),a("em",[e._v("mask")]),e._v(", which, in its\nsimplest form, is defined as the gain between (0) and (1) to apply\non each element of the TF representation of the mixture (e.g., an STFT)\nin order to estimate the desired signal. Loosely speaking, it can be\nunderstood as an equalizer whose setting changes every few milliseconds.\nAfter multiplication of the mixture by a mask, the separated signal is\nrecovered through an inverse TF transform. In the multichannel setting,\nmore sophisticated filters may be designed that incorporate some delay\nand combine different channels; this is usually called "),a("em",[e._v("beamforming")]),e._v(". In\nthe frequency domain, this is often equivalent to using complex matrices\nto multiply the mixture TF representation with, instead of just scalars\nbetween (0) and (1).")]),a("p",[e._v("In practice, masks can be designed to filter the mixture in several\nways. One may estimate the spectrogram for a single source or component,\ne.g., the accompaniment, and subtract it from the mixture spectrogram,\ne.g., in order to estimate the lead ["),a("a",{attrs:{href:"#ref-boll1979"}},[e._v("18")]),e._v("]. Another way\nwould be to estimate separate spectrograms for both lead and\naccompaniment and combine them to yield a mask. For instance, a TF mask\nfor the lead can be taken as the proportion of the lead spectrogram over\nthe sum of both spectrograms, at each TF bin. Such filters are often\ncalled "),a("em",[e._v("Wiener filters")]),e._v(" ["),a("a",{attrs:{href:"#ref-wiener1975"}},[e._v("19")]),e._v("] or "),a("em",[e._v("ratio masks")]),e._v(". How\nthey are calculated may involve some additional techniques like\nexponentiation and may be understood according to assumptions regarding\nthe underlying statistics of the sources. For recent work in this area,\nand many useful pointers in designing such masks, the reader is referred\nto ["),a("a",{attrs:{href:"#ref-liutkus15c"}},[e._v("20")]),e._v("].")]),a("h3",{attrs:{id:"audio-and-speech-modeling"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#audio-and-speech-modeling","aria-hidden":"true"}},[e._v("#")]),e._v(" Audio and speech modeling")]),a("p",[e._v("It is typical in audio processing to describe audio waveforms as\nbelonging to one of two different categories, which are "),a("em",[e._v("sinusoidal\nsignals")]),e._v(" — or pure tones — and "),a("em",[e._v("noise")]),e._v(". Actually, both are just the two\nextremes in a continuum of varying "),a("em",[e._v("predictability")]),e._v(": on the one hand,\nthe shape of a sinusoidal wave in the future can reliably be guessed\nfrom previous samples. On the other hand, white noise is "),a("em",[e._v("defined")]),e._v(" as an\nunpredictable signal and its spectrogram has constant energy everywhere.\nDifferent noise profiles may then be obtained by attenuating the energy\nof some frequency regions. This in turn induces some predictability in\nthe signal, and in the extreme case where all the energy content is\nconcentrated in one frequency, a pure tone is obtained.")]),a("p",[e._v("A waveform may always be modeled as some "),a("em",[e._v("filter")]),e._v(" applied on some\n"),a("em",[e._v("excitation signal")]),e._v(". Usually, the filter is assumed to vary smoothly\nacross frequencies, hence modifying only what is called "),a("em",[e._v("the spectral\nenvelope")]),e._v(" of the signal, while the excitation signal comprises the rest.\nThis is the basis for the "),a("em",[e._v("source-filter")]),e._v(" model ["),a("a",{attrs:{href:"#ref-fant70"}},[e._v("21")]),e._v("],\nwhich is of great importance in speech modeling, and thus also in vocal\nseparation. As for speech, the filter is created by the shape of the\nvocal tract. The excitation signal is made of the glottal pulses\ngenerated by the vibration of the vocal folds. This results into\n"),a("em",[e._v("voiced")]),e._v(" speech sounds made of time-varying harmonic/sinusoidal\ncomponents. The excitation signal can also be the air flow passing\nthrough some constriction of the vocal tract. This results into\n"),a("em",[e._v("unvoiced")]),e._v(", noise-like, speech sounds. In this context, vowels are said\nto be voiced and tend to feature many sinusoids, while some phonemes\nsuch as fricatives are unvoiced and noisier.")]),a("p",[e._v("A classical tool for dissociating the envelope from the excitation is\nthe "),a("em",[e._v("cepstrum")]),e._v(" ["),a("a",{attrs:{href:"#ref-bogert1963"}},[e._v("22")]),e._v("]. It has applications for\nestimating the fundamental frequency ["),a("a",{attrs:{href:"#ref-noll64"}},[e._v("23")]),e._v("],\n["),a("a",{attrs:{href:"#ref-noll67"}},[e._v("24")]),e._v("], for deriving the Mel-frequency cepstral\ncoefficients (MFCC) ["),a("a",{attrs:{href:"#ref-david80"}},[e._v("25")]),e._v("], or for filtering signals\nthrough a so-called "),a("em",[e._v("liftering")]),e._v(" operation ["),a("a",{attrs:{href:"#ref-oppenheim69"}},[e._v("26")]),e._v("]\nthat enables modifications of either the excitation or the envelope\nparts through the source-filter\nparadigm.")]),a("p",[a("span",{attrs:{id:"fig:stylized_vocals_accompaniment",label:"fig:stylized_vocals_accompaniment"}},[e._v("[fig:stylized_vocals_accompaniment]")])]),a("p",[e._v("An advantage of the source-filter model approach is indeed that one can\ndissociate the pitched content of the signal, embodied by the position\nof its harmonics, from its TF envelope which describes where the energy\nof the sound lies. In the case of vocals, it yields the ability to\ndistinguish between the actual note being sung (pitch content) and the\nphoneme being uttered (mouth and vocal tract configuration),\nrespectively. One key feature of vocals is they typically exhibit great\nvariability in fundamental frequency over time. They can also exhibit\nlarger "),a("em",[e._v("vibratos")]),e._v(" (fundamental frequency modulations) and "),a("em",[e._v("tremolos")]),e._v("\n(amplitude modulations) in comparison to other instruments, as seen in\nthe top spectrogram in\nFigure "),a("a",{attrs:{href:"#fig:stylized_vocals_accompaniment"}},[e._v("[fig:stylized_vocals_accompaniment]")]),e._v(".")]),a("p",[e._v("A particularity of musical signals is that they typically consist of\nsequences of pitched notes. A sound gives the perception of having a\npitch if the majority of the energy in the audio signal is at\nfrequencies located at integer multiples of some fundamental frequency.\nThese integer multiples are called "),a("em",[e._v("harmonics")]),e._v(". When the fundamental\nfrequency changes, the frequencies of these harmonics also change,\nyielding the typical comb spectrograms of harmonic signals, as depicted\nin the top spectrogram in\nFigure "),a("a",{attrs:{href:"#fig:stylized_vocals_accompaniment"}},[e._v("[fig:stylized_vocals_accompaniment]")]),e._v(".\nAnother noteworthy feature of sung melodies over simple speech is that\ntheir fundamental frequencies are, in general, located at precise\nfrequency values corresponding to the musical key of the song. These\nvery peculiar features are often exploited in separation methods. For\nsimplicity reasons, we use the terms "),a("em",[e._v("pitch")]),e._v(" and "),a("em",[e._v("fundamental frequency")]),e._v("\ninterchangeably throughout the paper.")]),a("h3",{attrs:{id:"probability-theory"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#probability-theory","aria-hidden":"true"}},[e._v("#")]),e._v(" Probability theory")]),a("p",[e._v("Probability theory ["),a("a",{attrs:{href:"#ref-jaynes2003probability"}},[e._v("14")]),e._v("],\n["),a("a",{attrs:{href:"#ref-durrett2010probability"}},[e._v("27")]),e._v("] is an important framework for\ndesigning many data analysis and processing methods. Many of the methods\ndescribed in this article use it and it is far beyond the scope of this\npaper to present it rigorously. For our purpose, it will suffice to say\nthat the "),a("em",[e._v("observations")]),e._v(" consist of the mixture signals. On the other\nhand, the "),a("em",[e._v("parameters")]),e._v(" are any relevant feature about the source signal\n(such as pitch or time-varying envelope) or how the signals are mixed\n(e.g., the panning position). These parameters can be used to derive\nestimates about the target lead and accompaniment signals.")]),a("p",[e._v("We understand a probabilistic "),a("em",[e._v("model")]),e._v(" as a function of both the\nobservations and the parameters: it describes how likely the\nobservations are, given the parameters. For instance, a flat spectrum is\nlikely under the noise model, and a mixture of comb spectrograms is\nlikely under a harmonic model with the appropriate pitch parameters for\nthe sources. When the observations are given, variation in the model\ndepends only on the parameters. For some parameter value, it tells how\nlikely the observations are. Under a harmonic model for instance, pitch\nmay be estimated by finding the pitch parameter that makes the observed\nwaveform as likely as possible. Alternatively, we may want to choose\nbetween several possible models such as voiced or unvoiced. In such\ncases, "),a("em",[e._v("model selection")]),e._v(" methods are available, such as the Bayesian\ninformation criterion (BIC) ["),a("a",{attrs:{href:"#ref-schwarz78"}},[e._v("28")]),e._v("].")]),a("p",[e._v("Given these basic ideas, we briefly mention two models that are of\nparticular importance. Firstly, the hidden Markov model\n(HMM) ["),a("a",{attrs:{href:"#ref-cappe2005"}},[e._v("15")]),e._v("], ["),a("a",{attrs:{href:"#ref-rabiner89"}},[e._v("29")]),e._v("] is relevant\nfor time-varying observations. It basically defines several "),a("em",[e._v("states")]),e._v(",\neach one related to a specific model and with some probabilities for\ntransitions between them. For instance, we could define as many states\nas possible notes played by the lead guitar, each one associated with a\ntypical spectrum. The "),a("em",[e._v("Viterbi algorithm")]),e._v(" is a dynamic programming\nmethod which actually estimates the most likely sequence of states given\na sequence of observations ["),a("a",{attrs:{href:"#ref-viterbi2006"}},[e._v("30")]),e._v("]. Secondly, the\nGaussian mixture model (GMM) ["),a("a",{attrs:{href:"#ref-bishop96"}},[e._v("31")]),e._v("] is a way to\napproximate any distribution as a weighted sum of Gaussians. It is\nwidely used in clustering, because it works well with the celebrated\nExpectation-Maximization (EM) algorithm ["),a("a",{attrs:{href:"#ref-dempster77"}},[e._v("32")]),e._v("] to\nassign one particular cluster to each data point, while automatically\nestimating the clusters parameters. As we will see later, many methods\nwork by assigning each TF bin to a given source in a similar way.")]),a("h2",{attrs:{id:"modeling-the-lead-signal-harmonicity"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#modeling-the-lead-signal-harmonicity","aria-hidden":"true"}},[e._v("#")]),e._v(" Modeling the lead signal: harmonicity")]),a("p",[a("img",{attrs:{src:"https://docs.google.com/drawings/d/e/2PACX-1vS1ciSejDMm1qrkhaPSc9btYmTvnGc3p5XgxeFsI0De8I5IWYxR73ctpzu0E4Ud7S9KEWRHqcng__Q2/pub?w=592&h=403",alt:""}})]),a("h4",{attrs:{id:"the-approaches-based-on-a-harmonic-assumption-for-vocals-in-a-first-analysis-step-the-fundamental-frequency-of-the-lead-signal-is-extracted-from-it-a-separation-is-obtained-either-by-resynthesis-section-3-1-or-by-filtering-the-mixture-section-3-2-figures-figure2-pdf"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#the-approaches-based-on-a-harmonic-assumption-for-vocals-in-a-first-analysis-step-the-fundamental-frequency-of-the-lead-signal-is-extracted-from-it-a-separation-is-obtained-either-by-resynthesis-section-3-1-or-by-filtering-the-mixture-section-3-2-figures-figure2-pdf","aria-hidden":"true"}},[e._v("#")]),e._v(" The approaches based on a "),a("em",[e._v("harmonic assumption")]),e._v(" for vocals. In a first analysis step, the fundamental frequency of the lead signal is extracted. From it, a separation is obtained either by resynthesis (Section "),a("a",{attrs:{href:"#ssec:harmonicity-synthesis"}},[e._v("3.1")]),e._v("), or by filtering the mixture (Section "),a("a",{attrs:{href:"#ssec:harmonicity-combfiltering"}},[e._v("3.2")]),e._v(").](figures/Figure2.pdf)")]),a("p",[e._v("As mentioned in Section "),a("a",{attrs:{href:"#ssec:audio_and_speech_models"}},[e._v("2.2")]),e._v(", one\nparticularity of vocals is their production by the vibration of the\nvocal folds, further filtered by the vocal tract. As a consequence, sung\nmelodies are "),a("em",[e._v("mostly")]),e._v(" harmonic, as depicted in\nFigure "),a("a",{attrs:{href:"#fig:stylized_vocals_accompaniment"}},[e._v("[fig:stylized_vocals_accompaniment]")]),e._v(",\nand therefore have a fundamental frequency. If one can track the pitch\nof the vocals, one can then estimate the energy at the harmonics of the\nfundamental frequency and reconstruct the voice. This is the basis of\nthe oldest methods (as well as some more recent methods) we are aware of\nfor separating the lead signal from a musical mixture.")]),a("p",[e._v("Such methods are summarized in\nFigure "),a("a",{attrs:{href:"#fig:methods_harmonicity"}},[e._v("[fig:methods_harmonicity]")]),e._v(". In a\nfirst step, the objective is to get estimates of the time-varying\nfundamental frequency for the lead at each time frame. A second step in\nthis respect is then to track this fundamental frequency over time, in\nother words, to find the best sequence of estimates, in order to\nidentify the melody line. This can done either by a suitable pitch\ndetection method, or by exploiting the availability of the score. Such\nalgorithms typically assume that the lead corresponds to the harmonic\nsignal with strongest amplitude. For a review on the particular topic of\nmelody extraction, the reader is referred to ["),a("a",{attrs:{href:"#ref-salamon14"}},[e._v("33")]),e._v("].")]),a("p",[e._v("From this starting point, we can distinguish between two kinds of\napproaches, depending on how they exploit the pitch information.")]),a("h3",{attrs:{id:"analysis-synthesis-approaches"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#analysis-synthesis-approaches","aria-hidden":"true"}},[e._v("#")]),e._v(" Analysis-synthesis approaches")]),a("p",[e._v("The first option to obtain the separated lead signal is to resynthesize\nit using a sinusoidal model. A sinusoidal model decomposes the sound\nwith a set of sine waves of varying frequency and amplitude. If one\nknows the fundamental frequency of a pitched sound (like a singing\nvoice), as well as the spectral envelope of the recording, then one can\nreconstruct the sound by making a set of sine waves whose frequencies\nare those of the harmonics of the fundamental frequency, and whose\namplitudes are estimated from the spectral envelope of the audio. While\nthe spectral envelope of the recording is generally not exactly the same\nas the spectral envelope of the target source, it can be a reasonable\napproximation, especially assuming that different sources do not overlap\ntoo much with each other in the TF representation of the mixture.")]),a("p",[e._v("This idea allows for time-domain processing and was used in the earliest\nmethods we are aware of. In 1973, Miller proposed in\n["),a("a",{attrs:{href:"#ref-miller73"}},[e._v("34")]),e._v("] to use the homomorphic vocoder\n["),a("a",{attrs:{href:"#ref-oppenheim68"}},[e._v("35")]),e._v("] to separate the excitation function and\nimpulse response of the vocal tract. Further refinements include\nsegmenting parts of the signal as voiced, unvoiced, or silences using a\nheuristic program and manual interaction. Finally, cepstral\nliftering ["),a("a",{attrs:{href:"#ref-oppenheim69"}},[e._v("26")]),e._v("] was exploited to compensate for\nthe noise or accompaniment.")]),a("p",[e._v("Similarly, Maher used an analysis-synthesis approach\nin ["),a("a",{attrs:{href:"#ref-maher89"}},[e._v("36")]),e._v("], assuming the mixtures are composed of only\ntwo harmonic sources. In his case, pitch detection was performed on the\nSTFT and included heuristics to account for possibly colliding\nharmonics. He finally resynthesized each musical voice with a sinusoidal\nmodel.")]),a("p",[e._v("Wang proposed instantaneous and frequency-warped techniques for signal\nparameterization and source separation, with application to voice\nseparation in music ["),a("a",{attrs:{href:"#ref-wang94"}},[e._v("37")]),e._v("], ["),a("a",{attrs:{href:"#ref-wang95"}},[e._v("38")]),e._v("]. He\nintroduced a frequency-locked loop algorithm which uses multiple\nharmonically constrained trackers. He computed the estimated fundamental\nfrequency from a maximum-likelihood weighting of the tracking estimates.\nHe was then able to estimate harmonic signals such as voices from\ncomplex mixtures.")]),a("p",[e._v("Meron and Hirose proposed to separate singing voice and piano\naccompaniment ["),a("a",{attrs:{href:"#ref-meron98"}},[e._v("39")]),e._v("]. In their case, prior knowledge\nconsisting of musical scores was considered. Sinusoidal modeling as\ndescribed in ["),a("a",{attrs:{href:"#ref-quatieri92"}},[e._v("40")]),e._v("] was used.")]),a("p",[e._v("Ben-Shalom and Dubnov proposed to filter an instrument or a singing\nvoice out in such a way ["),a("a",{attrs:{href:"#ref-ben-shalom04"}},[e._v("41")]),e._v("]. They first used a\nscore alignment algorithm ["),a("a",{attrs:{href:"#ref-shalev-shwartz02"}},[e._v("42")]),e._v("], assuming a\nknown score. Then, they used the estimated pitch information to design a\nfilter based on a harmonic model ["),a("a",{attrs:{href:"#ref-serra97"}},[e._v("43")]),e._v("] and performed\nthe filtering using the linear constraint minimum variance approach\n["),a("a",{attrs:{href:"#ref-vanveen97"}},[e._v("44")]),e._v("]. They additionally used a heuristic to deal\nwith the unvoiced parts of the singing voice.")]),a("p",[e._v("Zhang and Zhang proposed an approach based on harmonic structure\nmodeling ["),a("a",{attrs:{href:"#ref-zhang05"}},[e._v("45")]),e._v("], ["),a("a",{attrs:{href:"#ref-zhang06"}},[e._v("46")]),e._v("]. They first\nextracted harmonic structures for singing voice and background music\nsignals using a sinusoidal model ["),a("a",{attrs:{href:"#ref-serra97"}},[e._v("43")]),e._v("], by extending\nthe pitch estimation algorithm in ["),a("a",{attrs:{href:"#ref-terhardt79"}},[e._v("47")]),e._v("]. Then, they\nused the clustering algorithm in ["),a("a",{attrs:{href:"#ref-zhang03"}},[e._v("48")]),e._v("] to learn\nharmonic structure models for the background music signals. Finally,\nthey extracted the harmonic structures for all the instruments to\nreconstruct the background music signals and subtract them from the\nmixture, leaving only the singing voice signal.")]),a("p",[e._v("More recently, Fujihara et al. proposed an accompaniment reduction\nmethod for singer identification ["),a("a",{attrs:{href:"#ref-fujihara05"}},[e._v("49")]),e._v("],\n["),a("a",{attrs:{href:"#ref-fujihara10"}},[e._v("50")]),e._v("]. After fundamental frequency estimation\nusing ["),a("a",{attrs:{href:"#ref-goto04"}},[e._v("51")]),e._v("], they extracted the harmonic structure of\nthe melody, i.e., the power and phase of the sinusoidal components at\nfundamental frequency and harmonics. Finally, they resynthesized the\naudio signal of the melody using the sinusoidal model in\n["),a("a",{attrs:{href:"#ref-moorer05"}},[e._v("52")]),e._v("].")]),a("p",[e._v("Similarly, Mesaros et al. proposed a vocal separation method to help\nwith singer identification ["),a("a",{attrs:{href:"#ref-mesaros07"}},[e._v("53")]),e._v("]. They first applied\na melody transcription system ["),a("a",{attrs:{href:"#ref-ryynanen06"}},[e._v("54")]),e._v("] which estimates\nthe melody line with the corresponding MIDI note numbers. Then, they\nperformed sinusoidal resynthesis, estimating amplitudes and phases from\nthe polyphonic signal.")]),a("p",[e._v("In a similar manner, Duan et al. proposed to separate harmonic sources,\nincluding singing voices, by using harmonic structure models\n["),a("a",{attrs:{href:"#ref-duan08"}},[e._v("55")]),e._v("]. They first defined an average harmonic structure\nmodel for an instrument. Then, they learned a model for each source by\ndetecting the spectral peaks using a cross-correlation method\n["),a("a",{attrs:{href:"#ref-rodet97"}},[e._v("56")]),e._v("] and quadratic\ninterpolation ["),a("a",{attrs:{href:"#ref-smith87"}},[e._v("57")]),e._v("]. Then, they extracted the harmonic\nstructures using BIC and a clustering algorithm ["),a("a",{attrs:{href:"#ref-zhang03"}},[e._v("48")]),e._v("].\nFinally, they separated the sources by re-estimating the fundamental\nfrequencies, re-extracting the harmonics, and reconstructing the signals\nusing a phase generation method ["),a("a",{attrs:{href:"#ref-slaney94"}},[e._v("58")]),e._v("].")]),a("p",[e._v("Lagrange et al. proposed to formulate lead separation as a graph\npartition problem ["),a("a",{attrs:{href:"#ref-lagrange07"}},[e._v("59")]),e._v("], ["),a("a",{attrs:{href:"#ref-lagrange08"}},[e._v("60")]),e._v("].\nThey first identified peaks in the spectrogram and grouped the peaks\ninto clusters by using a similarity measure which accounts for\nharmonically related peaks, and the normalized cut criterion\n["),a("a",{attrs:{href:"#ref-shi00"}},[e._v("61")]),e._v("] which is used for segmenting graphs in computer\nvision. They finally selected the cluster of peaks which corresponds to\na predominant harmonic source and resynthesized it using a bank of\nsinusoidal oscillators.")]),a("p",[e._v("Ryynänen et al. proposed to separate accompaniment from polyphonic music\nusing melody transcription for karaoke\napplications ["),a("a",{attrs:{href:"#ref-ryynanen08"}},[e._v("62")]),e._v("]. They first transcribed the\nmelody into a MIDI note sequence and a fundamental frequency trajectory,\nusing the method in ["),a("a",{attrs:{href:"#ref-ryynanen082"}},[e._v("63")]),e._v("], an improved version of\nthe earlier method ["),a("a",{attrs:{href:"#ref-ryynanen06"}},[e._v("54")]),e._v("]. Then, they used sinusoidal\nmodeling to estimate, resynthesize, and remove the lead vocals from the\nmusical mixture, using the quadratic polynomial-phase model in\n["),a("a",{attrs:{href:"#ref-ding97"}},[e._v("64")]),e._v("].")]),a("h3",{attrs:{id:"comb-filtering-approaches"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#comb-filtering-approaches","aria-hidden":"true"}},[e._v("#")]),e._v(" Comb-filtering approaches")]),a("p",[e._v("Using sinusoidal synthesis to generate the lead signal suffers from a\ntypical "),a("em",[e._v("metallic")]),e._v(" sound quality, which is mostly due to discrepancies\nbetween the estimated excitation signals of the lead signal compared to\nthe ground truth. To address this issue, an alternative approach is to\nexploit harmonicity in another way, by filtering out everything from the\nmixture that is not located close to the detected harmonics.")]),a("p",[e._v("Li and Wang proposed to use a vocal/non-vocal classifier and a\npredominant pitch detection algorithm ["),a("a",{attrs:{href:"#ref-li06"}},[e._v("65")]),e._v("],\n["),a("a",{attrs:{href:"#ref-li07"}},[e._v("66")]),e._v("]. They first detected the singing voice by using a\nspectral change detector ["),a("a",{attrs:{href:"#ref-duxbury03"}},[e._v("67")]),e._v("] to partition the\nmixture into homogeneous portions, and GMMs on MFCCs to classify the\nportions as vocal or non-vocal. Then, they used the predominant pitch\ndetection algorithm in ["),a("a",{attrs:{href:"#ref-li05"}},[e._v("68")]),e._v("] to detect the pitch contours\nfrom the vocal portions, extending the multi-pitch tracking algorithm\nin ["),a("a",{attrs:{href:"#ref-wu03"}},[e._v("69")]),e._v("]. Finally, they extracted the singing voice by\ndecomposing the vocal portions into TF units and labeling them as\nsinging or accompaniment dominant, extending the speech separation\nalgorithm in ["),a("a",{attrs:{href:"#ref-hu02"}},[e._v("70")]),e._v("].")]),a("p",[e._v("Han and Raphael proposed an approach for desoloing a recording of a\nsoloist with an accompaniment given a musical score and its time\nalignment with the recording ["),a("a",{attrs:{href:"#ref-han07"}},[e._v("71")]),e._v("]. They derived a mask\n["),a("a",{attrs:{href:"#ref-roweis01"}},[e._v("72")]),e._v("] to remove the solo part after using an EM\nalgorithm to estimate its melody, that exploits the score as side\ninformation.")]),a("p",[e._v("Hsu et al. proposed an approach which also identifies and separates the\nunvoiced singing voice ["),a("a",{attrs:{href:"#ref-hsu08"}},[e._v("73")]),e._v("], ["),a("a",{attrs:{href:"#ref-hsu10"}},[e._v("74")]),e._v("].\nInstead of processing in the STFT domain, they use the perceptually\nmotivated gammatone filter-bank as in ["),a("a",{attrs:{href:"#ref-li07"}},[e._v("66")]),e._v("],\n["),a("a",{attrs:{href:"#ref-hu02"}},[e._v("70")]),e._v("]. They first detected accompaniment, unvoiced, and\nvoiced segments using an HMM and identified voice-dominant TF units in\nthe voiced frames by using the singing voice separation method in\n["),a("a",{attrs:{href:"#ref-li07"}},[e._v("66")]),e._v("], using the predominant pitch detection algorithm in\n["),a("a",{attrs:{href:"#ref-dressler062"}},[e._v("75")]),e._v("]. Unvoiced-dominant TF units were identified\nusing a GMM classifier with MFCC features learned from training data.\nFinally, filtering was achieved with spectral\nsubtraction ["),a("a",{attrs:{href:"#ref-scalart96"}},[e._v("76")]),e._v("].")]),a("p",[e._v("Raphael and Han then proposed a classifier-based approach to separate a\nsoloist from accompanying instruments using a time-aligned symbolic\nmusical score ["),a("a",{attrs:{href:"#ref-raphael08"}},[e._v("77")]),e._v("]. They built a tree-structured\nclassifier ["),a("a",{attrs:{href:"#ref-breiman84"}},[e._v("78")]),e._v("] learned from labeled training data\nto classify TF points in the STFT as belonging to solo or accompaniment.\nThey additionally constrained their classifier to estimate masks having\na connected structure.")]),a("p",[e._v("Cano et al. proposed various approaches for solo and accompaniment\nseparation. In ["),a("a",{attrs:{href:"#ref-cano09"}},[e._v("79")]),e._v("], they separated saxophone melodies\nfrom mixtures with piano and/or orchestra by using a melody line\ndetection algorithm, incorporating information about typical saxophone\nmelody lines. In ["),a("a",{attrs:{href:"#ref-grollmisch11"}},[e._v("80")]),e._v("]–["),a("a",{attrs:{href:"#ref-cano12"}},[e._v("82")]),e._v("], they\nproposed to use the pitch detection algorithm\nin ["),a("a",{attrs:{href:"#ref-dressler11"}},[e._v("83")]),e._v("]. Then, they refined the fundamental\nfrequency and the harmonics, and created a binary mask for the solo and\naccompaniment. They finally used a post-processing stage to refine the\nseparation. In ["),a("a",{attrs:{href:"#ref-cano13"}},[e._v("84")]),e._v("], they included a noise spectrum in\nthe harmonic refinement stage to also capture noise-like sounds in\nvocals. In ["),a("a",{attrs:{href:"#ref-cano14"}},[e._v("85")]),e._v("], they additionally included common\namplitude modulation characteristics in the separation scheme.")]),a("p",[e._v("Bosch et al. proposed to separate the lead instrument using a musical\nscore ["),a("a",{attrs:{href:"#ref-bosch12"}},[e._v("86")]),e._v("]. After a preliminary alignment of the score\nto the mixture, they estimated a score confidence measure to deal with\nlocal misalignments and used it to guide the predominant pitch tracking.\nFinally, they performed low-latency separation based on the method in\n["),a("a",{attrs:{href:"#ref-marxer12"}},[e._v("87")]),e._v("], by combining harmonic masks derived from the\nestimated pitch and additionally exploiting stereo information as\npresented later in Section "),a("a",{attrs:{href:"#sec:multichannel"}},[e._v("7")]),e._v(".")]),a("p",[e._v("Vaneph et al. proposed a framework for vocal isolation to help spectral\nediting ["),a("a",{attrs:{href:"#ref-vaneph16"}},[e._v("88")]),e._v("]. They first used a voice activity\ndetection process based on a deep learning technique\n["),a("a",{attrs:{href:"#ref-leglaive15"}},[e._v("89")]),e._v("]. Then, they used pitch tracking to detect the\nmelodic line of the vocal and used it to separate the vocal and\nbackground, allowing a user to provide manual annotations when\nnecessary.")]),a("h3",{attrs:{id:"shortcomings"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#shortcomings","aria-hidden":"true"}},[e._v("#")]),e._v(" Shortcomings")]),a("p",[e._v("As can be seen, explicitly assuming that the lead signal is harmonic led\nto an important body of research. While the aforementioned methods show\nexcellent performance when their assumptions are valid, their\nperformance can drop significantly in adverse, but common situations.")]),a("p",[e._v("Firstly, vocals are not always purely harmonic as they contain unvoiced\nphonemes that are not harmonic. As seen above, some methods already\nhandle this situation. However, vocals can also be whispered or\nsaturated, both of which are difficult to handle with a harmonic model.")]),a("p",[e._v("Secondly, methods based on the harmonic model depend on the quality of\nthe pitch detection method. If the pitch detector switches from\nfollowing the pitch of the lead (e.g., the voice) to another instrument,\nthe wrong sound will be isolated from the mix. Often, pitch detectors\nassume the lead signal is the "),a("em",[e._v("loudest")]),e._v(" harmonic sound in the mix.\nUnfortunately, this is not always the case. Another instrument may be\nlouder or the lead may be silent for a passage. The tendency to follow\nthe pitch of the wrong instrument can be mitigated by applying\nconstraints on the pitch range to estimate and by using a perceptually\nrelevant weighting filter before performing pitch tracking. Of course,\nthese approaches do not help when the lead signal is silent.")]),a("h2",{attrs:{id:"modeling-the-accompaniment-redundancy"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#modeling-the-accompaniment-redundancy","aria-hidden":"true"}},[e._v("#")]),e._v(" Modeling the accompaniment: redundancy")]),a("p",[e._v("In the previous section, we presented methods whose main focus was the\nmodeling of a harmonic lead melody. Most of these studies did not make\nmodeling the accompaniment a core focus. On the contrary, it was often\ndealt with as adverse noise to which the harmonic processing method\nshould be robust to.")]),a("p",[e._v("In this section, we present another line of research which concentrates\non modeling the accompaniment under the assumption it is somehow more\n"),a("em",[e._v("redundant")]),e._v(" than the lead signal. This assumption stems from the fact\nthat musical accompaniments are often highly structured, with elements\nbeing repeated many times. Such repetitions can occur at the note level,\nin terms of rhythmic structure, or even from a harmonic point of view:\ninstrumental notes are often constrained to have their pitch lie in a\nsmall set of frequencies. Therefore, modeling and removing the redundant\nelements of the signal are assumed to result in removal of the\naccompaniment.")]),a("p",[e._v("In this paper, we identify three families of methods that exploit the\nredundancy of the accompaniment for separation.")]),a("h3",{attrs:{id:"grouping-low-rank-components"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#grouping-low-rank-components","aria-hidden":"true"}},[e._v("#")]),e._v(" Grouping low-rank components")]),a("p",[a("img",{attrs:{src:"https://docs.google.com/drawings/d/e/2PACX-1vS1JrZuBXyzbaLEpt_ekUHVqENMcUWZkDjvhWvQSpN4vdUAr2asfRZzL471bpoUhbSNTN7b1nPojviG/pub?w=406&h=547",alt:""}})]),a("h4",{attrs:{id:"the-approaches-based-on-a-low-rank-assumption-non-negative-matrix-factorization-nmf-is-used-to-identify-components-from-the-mixture-that-are-subsequently-clustered-into-lead-or-accompaniment-additional-constraints-may-be-incorporated"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#the-approaches-based-on-a-low-rank-assumption-non-negative-matrix-factorization-nmf-is-used-to-identify-components-from-the-mixture-that-are-subsequently-clustered-into-lead-or-accompaniment-additional-constraints-may-be-incorporated","aria-hidden":"true"}},[e._v("#")]),e._v(" The approaches based on a "),a("em",[e._v("low-rank")]),e._v(" assumption. Non-negative matrix factorization (NMF) is used to identify "),a("em",[e._v("components")]),e._v(" from the mixture, that are subsequently clustered into lead or accompaniment. Additional constraints may be incorporated.")]),a("p",[e._v("The first set of approaches we consider is the identification of\nredundancy in the accompaniment through the assumption that its\nspectrogram may be well represented by only a few components. Techniques\nexploiting this idea then focus on algebraic methods that decompose the\nmixture spectrogram into the product of a few template spectra activated\nover time. One way to do so is via non-negative matrix factorization\n(NMF) ["),a("a",{attrs:{href:"#ref-lee99"}},[e._v("90")]),e._v("], ["),a("a",{attrs:{href:"#ref-lee01"}},[e._v("91")]),e._v("], which incorporates\nnon-negative constraints. In\nFigure "),a("a",{attrs:{href:"#fig:methods_low_rank"}},[e._v("[fig:methods_low_rank]")]),e._v(", we picture\nmethods exploiting such techniques. After factorization, we obtain\nseveral spectra, along with their activations over time. A subsequent\nstep is the clustering of these spectra (and activations) into the lead\nor the accompaniment. Separation is finally performed by deriving Wiener\nfilters to estimate the lead and the accompaniment from the mixture. For\nrelated applications of NMF in music analysis, the reader is referred to\n["),a("a",{attrs:{href:"#ref-smaragdis03"}},[e._v("92")]),e._v("]–["),a("a",{attrs:{href:"#ref-fevotte09"}},[e._v("94")]),e._v("].")]),a("p",[e._v("Vembu and Baumann proposed to use NMF (and also ICA\n["),a("a",{attrs:{href:"#ref-common94"}},[e._v("95")]),e._v("]) to separate vocals from mixtures\n["),a("a",{attrs:{href:"#ref-vembu05"}},[e._v("96")]),e._v("]. They first discriminated between vocal and\nnon-vocal sections in a mixture by using different combinations of\nfeatures, such as MFCCs ["),a("a",{attrs:{href:"#ref-david80"}},[e._v("25")]),e._v("], perceptual linear\npredictive (PLP) coefficients ["),a("a",{attrs:{href:"#ref-hermansky90"}},[e._v("97")]),e._v("], and log\nfrequency power coefficients (LFPC) ["),a("a",{attrs:{href:"#ref-nwe04"}},[e._v("98")]),e._v("], and training\ntwo classifiers, namely neural networks and support vector machines\n(SVM). They then applied redundancy reduction techniques on the TF\nrepresentation of the mixture to separate the sources\n["),a("a",{attrs:{href:"#ref-casey00"}},[e._v("99")]),e._v("], by using NMF (or ICA). The components were then\ngrouped as vocal and non-vocal by reusing a vocal/non-vocal classifier\nwith MFCC, LFPC, and PLP coefficients.")]),a("p",[e._v("Chanrungutai and Ratanamahatana proposed to use NMF with automatic\ncomponent selection["),a("a",{attrs:{href:"#ref-chanrungutai08"}},[e._v("100")]),e._v("],\n["),a("a",{attrs:{href:"#ref-chanrungutai082"}},[e._v("101")]),e._v("]. They first decomposed the mixture\nspectrogram using NMF with a fixed number of basis components. They then\nremoved the components with brief rhythmic and long-lasting continuous\nevents, assuming that they correspond to instrumental sounds. They\nfinally used the remaining components to reconstruct the singing voice,\nafter refining them using a high-pass filter.")]),a("p",[e._v("Marxer and Janer proposed an approach based on a Tikhonov\nregularization ["),a("a",{attrs:{href:"#ref-tikhonov63"}},[e._v("102")]),e._v("] as an alternative to NMF, for\nsinging voice separation ["),a("a",{attrs:{href:"#ref-marxer122"}},[e._v("103")]),e._v("]. Their method\nsacrificed the non-negativity constraints of the NMF in exchange for a\ncomputationally less expensive solution for spectrum decomposition,\nmaking it more interesting in low-latency scenarios.")]),a("p",[e._v("Yang et al. proposed a Bayesian NMF approach ["),a("a",{attrs:{href:"#ref-yang14"}},[e._v("104")]),e._v("],\n["),a("a",{attrs:{href:"#ref-chien15"}},[e._v("105")]),e._v("]. Following the approaches in\n["),a("a",{attrs:{href:"#ref-cemgil09"}},[e._v("106")]),e._v("] and ["),a("a",{attrs:{href:"#ref-schmidt09"}},[e._v("107")]),e._v("], they used a\nPoisson distribution for the likelihood function and exponential\ndistributions for the model parameters in the NMF algorithm, and derived\na variational Bayesian EM algorithm ["),a("a",{attrs:{href:"#ref-dempster77"}},[e._v("32")]),e._v("] to solve\nthe NMF problem. They also adaptively determined the number of bases\nfrom the mixture. They finally grouped the bases into singing voice and\nbackground music by using a "),a("em",[e._v("k")]),e._v("-means clustering algorithm\n["),a("a",{attrs:{href:"#ref-spiertz09"}},[e._v("108")]),e._v("] or an NMF-based clustering algorithm.")]),a("p",[e._v("In a different manner, Smaragdis and Mysore proposed a user-guided\napproach for removing sounds from mixtures by humming the target sound\nto be removed, for example a vocal track ["),a("a",{attrs:{href:"#ref-smaragdis09"}},[e._v("109")]),e._v("].\nThey modeled the mixture using probabilistic latent component analysis\n(PLCA) ["),a("a",{attrs:{href:"#ref-smaragdis07"}},[e._v("110")]),e._v("], another equivalent formulation of\nNMF. One key feature of exploiting user input was to facilitate the\ngrouping of components into vocals and accompaniment, as humming helped\nto identify some of the parameters for modeling the vocals.")]),a("p",[e._v("Nakamuray and Kameoka proposed an (L_p)-norm\nNMF ["),a("a",{attrs:{href:"#ref-nakamuray15"}},[e._v("111")]),e._v("], with (p) controlling the sparsity of\nthe error. They developed an algorithm for solving this NMF problem\nbased on the auxiliary function principle ["),a("a",{attrs:{href:"#ref-ortega70"}},[e._v("112")]),e._v("],\n["),a("a",{attrs:{href:"#ref-kameoka06"}},[e._v("113")]),e._v("]. Setting an adequate number of bases and (p)\ntaken as small enough allowed them to estimate the accompaniment as the\nlow-rank decomposition, and the singing voice as the error of the\napproximation, respectively. Note that, in this case, the singing voice\nwas not explicitly modeled as a sparse component but rather corresponded\nto the error which happened to be constrained as sparse. The next\nsubsection will actually deal with approaches that explicitly model the\nvocals as the sparse component.")]),a("h3",{attrs:{id:"low-rank-accompaniment-sparse-vocals"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#low-rank-accompaniment-sparse-vocals","aria-hidden":"true"}},[e._v("#")]),e._v(" Low-rank accompaniment, sparse vocals")]),a("p",[a("img",{attrs:{src:"https://docs.google.com/drawings/d/e/2PACX-1vSDMQhw6sU4gz4pG1sne-HLDS1qaEnfZ0fVL23yawA9EoHTQa_ZTBiJYwZVqnDVYfikANm9ZIiIU8Xh/pub?w=720",alt:""}})]),a("h4",{attrs:{id:"the-approaches-based-on-a-low-rank-accompaniment-sparse-vocals-assumption-as-opposed-to-methods-based-on-nmf-methods-based-on-robust-principal-component-analysis-rpca-assume-the-lead-signal-has-a-sparse-and-non-structured-spectrogram"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#the-approaches-based-on-a-low-rank-accompaniment-sparse-vocals-assumption-as-opposed-to-methods-based-on-nmf-methods-based-on-robust-principal-component-analysis-rpca-assume-the-lead-signal-has-a-sparse-and-non-structured-spectrogram","aria-hidden":"true"}},[e._v("#")]),e._v(" The approaches based on a "),a("em",[e._v("low-rank accompaniment, sparse vocals")]),e._v(" assumption. As opposed to methods based on NMF, methods based on robust principal component analysis (RPCA) assume the lead signal has a sparse and non-structured spectrogram.")]),a("p",[e._v("The methods presented in the previous section first compute a\ndecomposition of the mixture into many components that are sorted "),a("em",[e._v("a\nposteriori")]),e._v(" as accompaniment or lead. As can be seen, this means they\nmake a low-rank assumption for the accompaniment, but typically "),a("em",[e._v("also\nfor the vocals")]),e._v(". However, as can for instance be seen on\nFigure "),a("a",{attrs:{href:"#fig:stylized_vocals_accompaniment"}},[e._v("[fig:stylized_vocals_accompaniment]")]),e._v(",\nthe spectrogram for the vocals do exhibit much more freedom than\naccompaniment, and experience shows they are not adequately described by\na small number of spectral bases. For this reason, another track of\nresearch depicted in Figure "),a("a",{attrs:{href:"#fig:methods_rpca"}},[e._v("[fig:methods_rpca]")]),e._v("\nfocused on using a low-rank assumption on the accompaniment "),a("em",[e._v("only")]),e._v(",\nwhile assuming the vocals are "),a("em",[e._v("sparse and not structured")]),e._v(". This loose\nassumption means that only a few coefficients from their spectrogram\nshould have significant magnitude, and that they should not feature\nsignificant redundancy. Those ideas are in line with robust principal\ncomponent analysis (RPCA) ["),a("a",{attrs:{href:"#ref-candes11"}},[e._v("114")]),e._v("], which is the\nmathematical tool used by this body of methods, initiated by Huang et\nal. for singing voice separation ["),a("a",{attrs:{href:"#ref-huang12"}},[e._v("115")]),e._v("] . It decomposes\na matrix into a sparse and low-rank component.")]),a("p",[e._v("Sprechmann et al. proposed an approach based on RPCA for online singing\nvoice separation ["),a("a",{attrs:{href:"#ref-sprechmann12"}},[e._v("116")]),e._v("]. They used ideas from\nconvex optimization ["),a("a",{attrs:{href:"#ref-recht10"}},[e._v("117")]),e._v("], ["),a("a",{attrs:{href:"#ref-recht13"}},[e._v("118")]),e._v("] and\nmulti-layer neural networks ["),a("a",{attrs:{href:"#ref-gregor10"}},[e._v("119")]),e._v("]. They presented two\nextensions of RPCA and robust NMF models ["),a("a",{attrs:{href:"#ref-zhang11"}},[e._v("120")]),e._v("]. They\nthen used these extensions in a multi-layer neural network framework\nwhich, after an initial training stage, allows online source separation.")]),a("p",[e._v("Jeong and Lee proposed two extensions of the RPCA model to improve the\nestimation of vocals and accompaniment from the sparse and low-rank\ncomponents ["),a("a",{attrs:{href:"#ref-jeong14"}},[e._v("121")]),e._v("]. Their first extension included the\nSchatten (p) and (\\ell_{p}) norms as generalized nuclear norm\noptimizations ["),a("a",{attrs:{href:"#ref-nie152"}},[e._v("122")]),e._v("]. They also suggested a\npre-processing stage based on logarithmic scaling of the mixture TF\nrepresentation to enhance the RPCA.")]),a("p",[e._v("Yang also proposed an approach based on RPCA with dictionary learning\nfor recovering low-rank components ["),a("a",{attrs:{href:"#ref-yang13"}},[e._v("123")]),e._v("]. He introduced\na multiple low-rank representation following the observation that\nelements of the singing voice can also be recovered by the low-rank\ncomponent. He first incorporated online dictionary learning methods\n["),a("a",{attrs:{href:"#ref-mairal09"}},[e._v("124")]),e._v("] in his methodology to obtain prior information\nabout the structure of the sources and then incorporated them into the\nRPCA model.")]),a("p",[e._v("Chan and Yang then extended RPCA to complex and quaternionic cases with\napplication to singing voice separation ["),a("a",{attrs:{href:"#ref-chan16"}},[e._v("125")]),e._v("]. They\nextended the principal component pursuit (PCP) ["),a("a",{attrs:{href:"#ref-candes11"}},[e._v("114")]),e._v("]\nfor solving the RPCA problem by presenting complex and quaternionic\nproximity operators for the (\\ell_{1}) and trace-norm regularizations\nto account for the missing phase information.")]),a("h3",{attrs:{id:"repetitions-within-the-accompaniment"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#repetitions-within-the-accompaniment","aria-hidden":"true"}},[e._v("#")]),e._v(" Repetitions within the accompaniment")]),a("p",[e._v("While the rationale behind low-rank methods for lead-accompaniment\nseparation is to exploit the idea that the musical background should be\nredundant, adopting a low-rank model is not the only way to do it. An\nalternate way to proceed is to exploit the musical "),a("em",[e._v("structure")]),e._v(" of songs,\nto find "),a("em",[e._v("repetitions")]),e._v(" that can be utilized to perform separation. Just\nlike in RPCA-based methods, the accompaniment is then assumed to be the\nonly source for which repetitions will be found. The unique feature of\nthe methods described here is they combine music structure analysis\n["),a("a",{attrs:{href:"#ref-peeters03"}},[e._v("126")]),e._v("]–["),a("a",{attrs:{href:"#ref-paulus10"}},[e._v("128")]),e._v("] with particular ways\nto exploit the identification of repeated parts of the accompaniment.")]),a("p",[a("img",{attrs:{src:"https://docs.google.com/drawings/d/e/2PACX-1vTsnQlO0NWOqGKwG1ksYtD8oYpf2exFzCkHV6pX5COfgGCmJVNhl3E64qcgoq3dJwdapgK9eXltAUIH/pub?w=720",alt:""}})]),a("h4",{attrs:{id:"the-approaches-based-on-a-repetition-assumption-for-accompaniment-in-a-first-analysis-step-repetitions-are-identified-then-they-are-used-to-build-an-estimate-for-the-accompaniment-spectrogram-and-proceed-to-separation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#the-approaches-based-on-a-repetition-assumption-for-accompaniment-in-a-first-analysis-step-repetitions-are-identified-then-they-are-used-to-build-an-estimate-for-the-accompaniment-spectrogram-and-proceed-to-separation","aria-hidden":"true"}},[e._v("#")]),e._v(" The approaches based on a "),a("em",[e._v("repetition")]),e._v(" assumption for accompaniment. In a first analysis step, repetitions are identified. Then, they are used to build an estimate for the accompaniment spectrogram and proceed to separation.")]),a("p",[e._v("Rafii et al. proposed the REpeating Pattern Extraction Technique (REPET)\nto separate the accompaniment by assuming it is repeating\n["),a("a",{attrs:{href:"#ref-rafii11"}},[e._v("129")]),e._v("]–["),a("a",{attrs:{href:"#ref-rafii14"}},[e._v("131")]),e._v("], which is often the case\nin popular music. This approach, which is representative of this line of\nresearch, is represented on\nFigure "),a("a",{attrs:{href:"#fig:methods_repet"}},[e._v("[fig:methods_repet]")]),e._v(". First, a repeating\nperiod is extracted by a music information retrieval system, such as a\nbeat spectrum ["),a("a",{attrs:{href:"#ref-foote01"}},[e._v("132")]),e._v("] in this case. Then, this extracted\ninformation is used to estimate the spectrogram of the accompaniment\nthrough an averaging of the identified repetitions. From this, a filter\nis derived.")]),a("p",[e._v("Seetharaman et al.["),a("a",{attrs:{href:"#ref-seetharaman17"}},[e._v("133")]),e._v("] leveraged the two\ndimensional Fourier transform (2DFT) of the spectrogram to create an\nalgorithm very similar to REPET. The properties of the 2DFT let them\nseparate the periodic background from the non-periodic vocal melody by\ndeleting peaks in the 2DFT. This eliminated the need to create an\nexplicit model of the periodic audio and without the need to find the\nperiod of repetition, both of which are required in REPET.")]),a("p",[e._v("Liutkus et al. adapted the REPET approach in ["),a("a",{attrs:{href:"#ref-rafii11"}},[e._v("129")]),e._v("],\n["),a("a",{attrs:{href:"#ref-rafii13"}},[e._v("130")]),e._v("] to handle repeating structures varying along\ntime by modeling the repeating patterns only locally\n["),a("a",{attrs:{href:"#ref-rafii14"}},[e._v("131")]),e._v("], ["),a("a",{attrs:{href:"#ref-liutkus12"}},[e._v("134")]),e._v("]. They first\nidentified a repeating period for every time frame by computing a beat\nspectrogram as in ["),a("a",{attrs:{href:"#ref-foote01"}},[e._v("132")]),e._v("]. Then they estimated the\nspectrogram of the accompaniment by averaging the time frames in the\nmixture spectrogram at their local period rate, for every TF bin. From\nthis, they finally extracted the repeating structure by deriving a TF\nmask.")]),a("p",[e._v("Rafii et al. further extended the REPET approaches in\n["),a("a",{attrs:{href:"#ref-rafii11"}},[e._v("129")]),e._v("], ["),a("a",{attrs:{href:"#ref-rafii13"}},[e._v("130")]),e._v("] and\n["),a("a",{attrs:{href:"#ref-liutkus12"}},[e._v("134")]),e._v("] to handle repeating structures that are not\nperiodic. To do this, they proposed the REPET-SIM method in\n["),a("a",{attrs:{href:"#ref-rafii14"}},[e._v("131")]),e._v("], ["),a("a",{attrs:{href:"#ref-rafii12"}},[e._v("135")]),e._v("] to identify repeating\nframes for every time frame by computing a self-similarity matrix, as in\n["),a("a",{attrs:{href:"#ref-foote99"}},[e._v("136")]),e._v("]. Then, they estimated the accompaniment\nspectrogram at every TF bin by averaging the neighbors identified thanks\nto that similarity matrix. An extension for real-time processing was\npresented in ["),a("a",{attrs:{href:"#ref-rafii133"}},[e._v("137")]),e._v("] and a version exploiting user\ninteraction was proposed in ["),a("a",{attrs:{href:"#ref-rafii15"}},[e._v("138")]),e._v("]. A method close to\nREPET-SIM was also proposed by FitzGerald in\n["),a("a",{attrs:{href:"#ref-fitzgerald12"}},[e._v("139")]),e._v("].")]),a("p",[e._v("Liutkus et al. proposed the Kernel Additive modeling (KAM)\n["),a("a",{attrs:{href:"#ref-liutkus14"}},[e._v("140")]),e._v("], ["),a("a",{attrs:{href:"#ref-liutkus142"}},[e._v("141")]),e._v("] as a framework\nwhich generalizes the REPET approaches in\n["),a("a",{attrs:{href:"#ref-rafii11"}},[e._v("129")]),e._v("]–["),a("a",{attrs:{href:"#ref-rafii14"}},[e._v("131")]),e._v("],\n["),a("a",{attrs:{href:"#ref-liutkus12"}},[e._v("134")]),e._v("], ["),a("a",{attrs:{href:"#ref-rafii12"}},[e._v("135")]),e._v("]. They assumed that a\nsource at a TF location can be modeled using its values at other\nlocations through a specified kernel which can account for features such\nas periodicity, self-similarity, stability over time or frequency, etc.\nThis notably enabled modeling of the accompaniment using more than one\nrepeating pattern. Liutkus et al. also proposed a light version using a\nfast compression algorithm to make the approach more scalable\n["),a("a",{attrs:{href:"#ref-liutkus15"}},[e._v("142")]),e._v("]. The approach was also used for interference\nreduction in music recordings ["),a("a",{attrs:{href:"#ref-pratzlich15"}},[e._v("143")]),e._v("],\n["),a("a",{attrs:{href:"#ref-fanoyela17"}},[e._v("144")]),e._v("].")]),a("p",[e._v("With the same idea of exploiting intra-song redundancies for singing\nvoice separation, but through a very different methodology, Moussallam\net al. assumed in ["),a("a",{attrs:{href:"#ref-moussallam12"}},[e._v("145")]),e._v("] that all the sources can\nbe decomposed sparsely in the same dictionary and used a matching\npursuit greedy algorithm ["),a("a",{attrs:{href:"#ref-mallat93"}},[e._v("146")]),e._v("] to solve the problem.\nThey integrated the separation process in the algorithm by modifying the\natom selection criterion and adding a decision to assign a chosen atom\nto the repeated source or to the lead signal.")]),a("p",[e._v("Deif et al. proposed to use multiple median filters to separate vocals\nfrom music recordings ["),a("a",{attrs:{href:"#ref-deif152"}},[e._v("147")]),e._v("]. They augmented the\napproach in ["),a("a",{attrs:{href:"#ref-fitzgerald102"}},[e._v("148")]),e._v("] with diagonal median filters\nto improve the separation of the vocal component. They also investigated\ndifferent filter lengths to further improve the separation.")]),a("p",[e._v("Lee et al. also proposed to use the KAM approach\n["),a("a",{attrs:{href:"#ref-lee152"}},[e._v("149")]),e._v("]–["),a("a",{attrs:{href:"#ref-kim16"}},[e._v("152")]),e._v("]. They applied the\n(\\beta)-order minimum mean square error (MMSE) estimation\n["),a("a",{attrs:{href:"#ref-plourde08"}},[e._v("153")]),e._v("] to the back-fitting algorithm in KAM to\nimprove the separation. They adaptively calculated a perceptually\nweighting factor (\\alpha) and the singular value decomposition\n(SVD)-based factorized spectral amplitude exponent (\\beta) for each\nkernel component.")]),a("h3",{attrs:{id:"shortcomings-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#shortcomings-2","aria-hidden":"true"}},[e._v("#")]),e._v(" Shortcomings")]),a("p",[e._v("While methods focusing on harmonic models for the lead often fall short\nin their expressive power for the accompaniment, the methods we reviewed\nin this section are often observed to suffer exactly from the converse\nweakness, namely they do not provide an adequate model for the lead\nsignal. Hence, the separated vocals often will feature interference from\nunpredictable parts from the accompaniment, such as some percussion or\neffects which occur infrequently.")]),a("p",[e._v("Furthermore, even if the musical accompaniment will exhibit more\nredundancy, the vocals part will also be redundant to some extent, which\nis poorly handled by these methods. When the lead signal is not vocals\nbut played by some lead instrument, its redundancy is even more\npronounced, because the notes it plays lie in a reduced set of\nfundamental frequencies. Consequently, such methods would include the\nredundant parts of the lead within the accompaniment estimate, for\nexample, a steady humming by a vocalist.")]),a("h2",{attrs:{id:"joint-models-for-lead-and-accompaniment"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#joint-models-for-lead-and-accompaniment","aria-hidden":"true"}},[e._v("#")]),e._v(" Joint models for lead and accompaniment")]),a("p",[e._v("In the previous sections, we reviewed two important bodies of\nliterature, focused on modeling either the lead or the accompaniment\nparts of music recordings, respectively. While each approach showed its\nown advantages, it also featured its own drawbacks. For this reason,\nsome researchers devised methods combining ideas for modeling both the\nlead and the accompaniment sources, and thus benefiting from both\napproaches. We now review this line of research.")]),a("h3",{attrs:{id:"using-music-structure-analysis-to-drive-learning"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#using-music-structure-analysis-to-drive-learning","aria-hidden":"true"}},[e._v("#")]),e._v(" Using music structure analysis to drive learning")]),a("p",[e._v("The first idea we find in the literature is to augment methods for\naccompaniment modeling with the prior identification of sections where\nthe vocals are present or absent. In the case of the low rank models\ndiscussed in Sections "),a("a",{attrs:{href:"#ssec:NMF"}},[e._v("4.1")]),e._v(" and "),a("a",{attrs:{href:"#ssec:RPCA"}},[e._v("4.2")]),e._v(", such a\nstrategy indeed dramatically improves performance.")]),a("p",[e._v("Raj et al. proposed an approach in ["),a("a",{attrs:{href:"#ref-raj07"}},[e._v("154")]),e._v("] that is based\non the PLCA formulation of NMF ["),a("a",{attrs:{href:"#ref-smaragdis06"}},[e._v("155")]),e._v("], and extends\ntheir prior work ["),a("a",{attrs:{href:"#ref-raj05"}},[e._v("156")]),e._v("]. The parameters for the frequency\ndistribution of the background music are estimated from the background\nmusic-only segments, and the rest of the parameters from the singing\nvoice+background music segments, assuming a priori identified vocal\nregions.")]),a("p",[e._v("Han and Chen also proposed a similar approach for melody extraction\nbased on PLCA ["),a("a",{attrs:{href:"#ref-han11"}},[e._v("157")]),e._v("], which includes a further estimate\nof the melody from the vocals signal by an autocorrelation technique\nsimilar to ["),a("a",{attrs:{href:"#ref-boersma93"}},[e._v("158")]),e._v("].")]),a("p",[e._v("Gómez et al. proposed to separate the singing voice from the guitar\naccompaniment in flamenco music to help with melody transcription\n["),a("a",{attrs:{href:"#ref-gomez12"}},[e._v("159")]),e._v("]. They first manually segmented the mixture into\nvocal and non-vocal regions. They then learned percussive and harmonic\nbases from the non-vocal regions by using an unsupervised NMF\npercussive/harmonic separation approach ["),a("a",{attrs:{href:"#ref-virtanen07"}},[e._v("93")]),e._v("],\n["),a("a",{attrs:{href:"#ref-ono08"}},[e._v("160")]),e._v("]. The vocal spectrogram was estimated by keeping\nthe learned percussive and harmonic bases fixed.")]),a("p",[e._v("Papadopoulos and Ellis proposed a signal-adaptive formulation of RPCA\nwhich incorporates music content information to guide the recovery of\nthe sparse and low-rank components ["),a("a",{attrs:{href:"#ref-papadopoulos14"}},[e._v("161")]),e._v("]. Prior\nmusical knowledge, such as predominant melody, is used to regularize the\nselection of active coefficients during the optimization procedure.")]),a("p",[e._v("In a similar manner, Chan et al. proposed to use RPCA with vocal\nactivity information ["),a("a",{attrs:{href:"#ref-chan15"}},[e._v("162")]),e._v("]. They modified the RPCA\nalgorithm to constraint parts of the input spectrogram to be non-sparse\nto account for the non-vocal parts of the singing voice.")]),a("p",[e._v("A related method was proposed by Jeong and Lee in\n["),a("a",{attrs:{href:"#ref-jeong17"}},[e._v("163")]),e._v("], using RPCA with a weighted (l_1)-norm. They\nreplaced the uniform weighting between the low-rank and sparse\ncomponents in the RPCA algorithm by an adaptive weighting based on the\nvariance ratio between the singing voice and the accompaniment. One key\nelement of the method is to incorporate vocal activation information in\nthe weighting.")]),a("h3",{attrs:{id:"factorization-with-a-known-melody"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#factorization-with-a-known-melody","aria-hidden":"true"}},[e._v("#")]),e._v(" Factorization with a known melody")]),a("p",[e._v("While using only the knowledge of vocal activity as described above\nalready yields an increase of performance over methods operating\nblindly, many authors went further to also incorporate the fact that\nvocals often have a strong melody line. Some redundant model is then\nassumed for the accompaniment, while also enforcing a harmonic model for\nthe vocals.")]),a("p",[a("img",{attrs:{src:"https://docs.google.com/drawings/d/e/2PACX-1vSnyEXv0_Zwpc25L8mcLTDhNK84jaTrOWu2L8kM4W75Whmw6xSz3KR-XTZ3wErGk9CeFgf35HHy0z5G/pub?w=720",alt:""}})]),a("h4",{attrs:{id:"factorization-informed-with-the-melody-first-melody-extraction-is-performed-on-the-mixture-then-this-information-is-used-to-drive-the-estimation-of-the-accompaniment-tf-bins-pertaining-to-the-lead-should-not-be-taken-into-account-for-estimating-the-accompaniment-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#factorization-informed-with-the-melody-first-melody-extraction-is-performed-on-the-mixture-then-this-information-is-used-to-drive-the-estimation-of-the-accompaniment-tf-bins-pertaining-to-the-lead-should-not-be-taken-into-account-for-estimating-the-accompaniment-model","aria-hidden":"true"}},[e._v("#")]),e._v(" Factorization informed with the melody. First, melody extraction is performed on the mixture. Then, this information is used to drive the estimation of the accompaniment: TF bins pertaining to the lead should not be taken into account for estimating the accompaniment model.")]),a("p",[e._v("An early method to achieve this is depicted in\nFigure "),a("a",{attrs:{href:"#fig:NMF_known_melody"}},[e._v("[fig:NMF_known_melody]")]),e._v(" and was\nproposed by Virtanen et al. in ["),a("a",{attrs:{href:"#ref-virtanen08"}},[e._v("164")]),e._v("]. They\nestimated the pitch of the vocals in the mixture by using a melody\ntranscription algorithm ["),a("a",{attrs:{href:"#ref-ryynanen082"}},[e._v("63")]),e._v("] and derived a binary\nTF mask to identify where vocals are not present. They then applied NMF\non the remaining non-vocal segments to learn a model for the background.")]),a("p",[e._v("Wang and Ou also proposed an approach which combines melody extraction\nand NMF-based soft masking ["),a("a",{attrs:{href:"#ref-wang11"}},[e._v("165")]),e._v("]. They identified\naccompaniment, unvoiced, and voiced segments in the mixture using an HMM\nmodel with MFCCs and GMMs. They then estimated the pitch of the vocals\nfrom the voiced segments using the method in ["),a("a",{attrs:{href:"#ref-klapuri06"}},[e._v("166")]),e._v("]\nand an HMM with the Viterbi algorithm as in ["),a("a",{attrs:{href:"#ref-hsu09"}},[e._v("167")]),e._v("]. They\nfinally applied a soft mask to separate voice and accompaniment.")]),a("p",[e._v("Rafii et al. investigated the combination of an approach for modeling\nthe background and an approach for modeling the melody\n["),a("a",{attrs:{href:"#ref-rafii142"}},[e._v("168")]),e._v("]. They modeled the background by deriving a\nrhythmic mask using the REPET-SIM algorithm ["),a("a",{attrs:{href:"#ref-rafii12"}},[e._v("135")]),e._v("] and\nthe melody by deriving a harmonic mask using a pitch-based algorithm\n["),a("a",{attrs:{href:"#ref-duan10"}},[e._v("169")]),e._v("]. They proposed a parallel and a sequential\ncombination of those algorithms.")]),a("p",[e._v("Venkataramani et al. proposed an approach combining sinusoidal modeling\nand matrix decomposition, which incorporates prior knowledge about\nsinger and phoneme identity ["),a("a",{attrs:{href:"#ref-venkataramani14"}},[e._v("170")]),e._v("]. They\napplied a predominant pitch algorithm on annotated sung regions\n["),a("a",{attrs:{href:"#ref-rao10"}},[e._v("171")]),e._v("] and performed harmonic sinusoidal modeling\n["),a("a",{attrs:{href:"#ref-rao11"}},[e._v("172")]),e._v("]. Then, they estimated the spectral envelope of the\nvocal component from the spectral envelope of the mixture using a\nphoneme dictionary. After that, a spectral envelope dictionary\nrepresenting sung vowels from song segments of a given singer was\nlearned using an extension of NMF ["),a("a",{attrs:{href:"#ref-kim112"}},[e._v("173")]),e._v("],\n["),a("a",{attrs:{href:"#ref-zhou14"}},[e._v("174")]),e._v("]. They finally estimated a soft mask using the\nsinger-vowel dictionary to refine and extract the vocal component.")]),a("p",[e._v("Ikemiya et al. proposed to combine RPCA with pitch\nestimation["),a("a",{attrs:{href:"#ref-ikemiya15"}},[e._v("175")]),e._v("], ["),a("a",{attrs:{href:"#ref-ikemiya16"}},[e._v("176")]),e._v("]. They\nderived a mask using RPCA ["),a("a",{attrs:{href:"#ref-huang12"}},[e._v("115")]),e._v("] to separate the\nmixture spectrogram into singing voice and accompaniment components.\nThey then estimated the fundamental frequency contour from the singing\nvoice component based on ["),a("a",{attrs:{href:"#ref-hermes88"}},[e._v("177")]),e._v("] and derived a harmonic\nmask. They integrated the two masks and resynthesized the singing voice\nand accompaniment signals. Dobashi et al. then proposed to use that\nsinging voice separation approach in a music performance assistance\nsystem ["),a("a",{attrs:{href:"#ref-dobashi15"}},[e._v("178")]),e._v("].")]),a("p",[e._v("Hu and Liu proposed to combine approaches based on matrix decomposition\nand pitch information for singer identification["),a("a",{attrs:{href:"#ref-hu15"}},[e._v("179")]),e._v("].\nThey used non-negative matrix partial co-factorization\n["),a("a",{attrs:{href:"#ref-kim112"}},[e._v("173")]),e._v("], ["),a("a",{attrs:{href:"#ref-yoo10"}},[e._v("180")]),e._v("] which integrates prior\nknowledge about the singing voice and the accompaniment, to separate the\nmixture into singing voice and accompaniment portions. They then\nidentified the singing pitch from the singing voice portions using\n["),a("a",{attrs:{href:"#ref-boersma01"}},[e._v("181")]),e._v("] and derived a harmonic mask as in\n["),a("a",{attrs:{href:"#ref-li09"}},[e._v("182")]),e._v("], and finally reconstructed the singing voice using\na missing feature method ["),a("a",{attrs:{href:"#ref-raj04"}},[e._v("183")]),e._v("]. They also proposed to\nadd temporal and sparsity criteria to their algorithm\n["),a("a",{attrs:{href:"#ref-hu16"}},[e._v("184")]),e._v("].")]),a("p",[e._v("That methodology was also adopted by Zhang et al. in\n["),a("a",{attrs:{href:"#ref-zhang15"}},[e._v("185")]),e._v("], that followed the framework of the pitch-based\napproach in ["),a("a",{attrs:{href:"#ref-li07"}},[e._v("66")]),e._v("], by performing singing voice detection\nusing an HMM classifier, singing pitch detection using the algorithm in\n["),a("a",{attrs:{href:"#ref-decheveigne02"}},[e._v("186")]),e._v("], and singing voice separation using a\nbinary mask. Additionally, they augmented that approach by analyzing the\nlatent components of the TF matrix using NMF in order to refine the\nsinging voice and accompaniment.")]),a("p",[e._v("Zhu et al. ["),a("a",{attrs:{href:"#ref-zhu15"}},[e._v("187")]),e._v("] proposed an approach which is also\nrepresentative of this body of literature, with the pitch detection\nalgorithm being the one in ["),a("a",{attrs:{href:"#ref-boersma01"}},[e._v("181")]),e._v("] and binary TF masks\nused for separation after NMF.")]),a("h3",{attrs:{id:"joint-factorization-and-melody-estimation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#joint-factorization-and-melody-estimation","aria-hidden":"true"}},[e._v("#")]),e._v(" Joint factorization and melody estimation")]),a("p",[e._v("The methods presented above put together the ideas of modeling the lead\n(typically the vocals) as featuring a melodic harmonic line and the\naccompaniment as redundant. As such, they already exhibit significant\nimprovement over approaches only applying one of these ideas as\npresented in Sections "),a("a",{attrs:{href:"#sec:lead-harmonicity"}},[e._v("3")]),e._v("\nand "),a("a",{attrs:{href:"#sec:accompaniment-redundancy"}},[e._v("4")]),e._v(", respectively. However, these\nmethods above are still restricted in the sense that the analysis\nperformed on each side cannot help improve the other one. In other\nwords, the estimation of the models for the lead and the accompaniment\nare done sequentially. Another idea is to proceed "),a("em",[e._v("jointly")]),e._v(".")]),a("p",[a("img",{attrs:{src:"https://docs.google.com/drawings/d/e/2PACX-1vQ-ORJenf_FMWUl1CkGsNR5Vq9Gf8YapW_tkQwuAWTYGVVG6wzCablPGA_M_464l-Gig6Y6mr6nbFNX/pub?w=720",alt:""}})]),a("h4",{attrs:{id:"joint-estimation-of-the-lead-and-accompaniment-the-former-one-as-a-source-filter-model-and-the-latter-one-as-an-nmf-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#joint-estimation-of-the-lead-and-accompaniment-the-former-one-as-a-source-filter-model-and-the-latter-one-as-an-nmf-model","aria-hidden":"true"}},[e._v("#")]),e._v(" Joint estimation of the lead and accompaniment, the former one as a source-filter model and the latter one as an NMF model.")]),a("p",[e._v("A seminal work in this respect was done by Durrieu et al. using a\nsource-filter and NMF model\n["),a("a",{attrs:{href:"#ref-durrieu08"}},[e._v("188")]),e._v("]–["),a("a",{attrs:{href:"#ref-durrieu10"}},[e._v("190")]),e._v("], depicted in\nFigure "),a("a",{attrs:{href:"#fig:methods_sourcefilter"}},[e._v("[fig:methods_sourcefilter]")]),e._v(". Its\ncore idea is to decompose the mixture spectrogram as the sum of two\nterms. The first term accounts for the lead and is inspired by the\nsource-filter model described in Section "),a("a",{attrs:{href:"#sec:concepts"}},[e._v("2")]),e._v(": it is the\nelement-wise product of an "),a("em",[e._v("excitation")]),e._v(" spectrogram with a "),a("em",[e._v("filter")]),e._v("\nspectrogram. The former one can be understood as harmonic combs\nactivated by the melodic line, while the latter one modulates the\nenvelope and is assumed low-rank because few phonemes are used. The\nsecond term accounts for the accompaniment and is modeled with a\nstandard NMF. In ["),a("a",{attrs:{href:"#ref-durrieu08"}},[e._v("188")]),e._v("]–["),a("a",{attrs:{href:"#ref-durrieu10"}},[e._v("190")]),e._v("],\nthey modeled the lead by using a GMM-based model\n["),a("a",{attrs:{href:"#ref-ozerov07"}},[e._v("191")]),e._v("] and a glottal source model\n["),a("a",{attrs:{href:"#ref-klatt90"}},[e._v("192")]),e._v("], and the accompaniment by using an instantaneous\nmixture model ["),a("a",{attrs:{href:"#ref-benaroya032"}},[e._v("193")]),e._v("] leading to an NMF problem\n["),a("a",{attrs:{href:"#ref-fevotte09"}},[e._v("94")]),e._v("]. They jointly estimated the parameters of their\nmodels by maximum likelihood estimation using an iterative algorithm\ninspired by ["),a("a",{attrs:{href:"#ref-dhillon05"}},[e._v("194")]),e._v("] with multiplicative update rules\ndeveloped in ["),a("a",{attrs:{href:"#ref-lee01"}},[e._v("91")]),e._v("]. They also extracted the melody by\nusing an algorithm comparable to the Viterbi algorithm, before\nre-estimating the parameters and finally performing source separation\nusing Wiener filters ["),a("a",{attrs:{href:"#ref-benaroya06"}},[e._v("195")]),e._v("]. In\n["),a("a",{attrs:{href:"#ref-durrieu12"}},[e._v("196")]),e._v("], they proposed to adapt their model for\nuser-guided source separation.")]),a("p",[e._v("The joint modeling of the lead and accompaniment parts of a music signal\nwas also considered by Fuentes et al. in ["),a("a",{attrs:{href:"#ref-fuentes2012"}},[e._v("197")]),e._v("],\nthat introduced the idea of using a log-frequency TF representation\ncalled the constant-Q transform (CQT)\n["),a("a",{attrs:{href:"#ref-brown91"}},[e._v("198")]),e._v("]–["),a("a",{attrs:{href:"#ref-schorkhuber10"}},[e._v("200")]),e._v("]. The advantage\nof such a representation is that a change in pitch corresponds to a\nsimple translation in the TF plane, instead of a scaling as in the STFT.\nThis idea was used along the creation of a user interface to guide the\ndecomposition, in line with what was done in ["),a("a",{attrs:{href:"#ref-durrieu12"}},[e._v("196")]),e._v("].")]),a("p",[e._v("Joder and Schuller used the source-filter NMF model in\n["),a("a",{attrs:{href:"#ref-durrieu11"}},[e._v("201")]),e._v("], additionally exploiting MIDI scores\n["),a("a",{attrs:{href:"#ref-joder12"}},[e._v("202")]),e._v("]. They synchronized the MIDI scores to the audio\nusing the alignment algorithm in ["),a("a",{attrs:{href:"#ref-joder11"}},[e._v("203")]),e._v("]. They proposed\nto exploit the score information through two types of constraints\napplied in the model. In a first approach, they only made use of the\ninformation regarding whether the leading voice is present or not in\neach frame. In a second approach, they took advantage of both time and\npitch information on the aligned score.")]),a("p",[e._v("Zhao et al. proposed a score-informed leading voice separation system\nwith a weighting scheme ["),a("a",{attrs:{href:"#ref-zhao14"}},[e._v("204")]),e._v("]. They extended the system\nin ["),a("a",{attrs:{href:"#ref-joder12"}},[e._v("202")]),e._v("], which is based on the source-filter NMF\nmodel in ["),a("a",{attrs:{href:"#ref-durrieu11"}},[e._v("201")]),e._v("], by using a Laplacian or a\nGaussian-based mask on the NMF activation matrix to enhance the\nlikelihood of the score-informed pitch candidates.")]),a("p",[e._v("Jointly estimating accompaniment and lead allowed for some research in\ncorrectly estimating the unvoiced parts of the lead, which is the main\nissue with purely harmonic models, as highlighted in\nSection "),a("a",{attrs:{href:"#ssec:shortcomings_harmonics"}},[e._v("3.3")]),e._v(". In\n["),a("a",{attrs:{href:"#ref-durrieu11"}},[e._v("201")]),e._v("], ["),a("a",{attrs:{href:"#ref-durrieu092"}},[e._v("205")]),e._v("], Durrieu et al.\nextended their model to account for the unvoiced parts by adding white\nnoise components to the voice model.")]),a("p",[e._v("In the same direction, Janer and Marxer proposed to separate unvoiced\nfricative consonants using a semi-supervised NMF\n["),a("a",{attrs:{href:"#ref-janer13"}},[e._v("206")]),e._v("]. They extended the source-filter NMF model in\n["),a("a",{attrs:{href:"#ref-durrieu11"}},[e._v("201")]),e._v("] using a low-latency method with timbre\nclassification to estimate the predominant pitch\n["),a("a",{attrs:{href:"#ref-marxer12"}},[e._v("87")]),e._v("]. They approximated the fricative consonants as\nan additive wideband component, training a model of NMF bases. They also\nused the transient quality to differentiate between fricatives and\ndrums, after extracting transient time points using the method in\n["),a("a",{attrs:{href:"#ref-janer12"}},[e._v("207")]),e._v("].")]),a("p",[e._v("Similarly, Marxer and Janer then proposed to separately model the\nsinging voice breathiness ["),a("a",{attrs:{href:"#ref-marxer13"}},[e._v("208")]),e._v("]. They estimated the\nbreathiness component by approximating the voice spectrum as a filtered\ncomposition of a glottal excitation and a wideband component. They\nmodeled the magnitude of the voice spectrum using the model in\n["),a("a",{attrs:{href:"#ref-degottex11"}},[e._v("209")]),e._v("] and the envelope of the voice excitation\nusing the model in ["),a("a",{attrs:{href:"#ref-klatt90"}},[e._v("192")]),e._v("]. They estimated the pitch\nusing the method in ["),a("a",{attrs:{href:"#ref-marxer12"}},[e._v("87")]),e._v("]. This was all integrated\ninto the source-filter NMF model.")]),a("p",[e._v("The body of research initiated by Durrieu et al. in\n["),a("a",{attrs:{href:"#ref-durrieu08"}},[e._v("188")]),e._v("] consists of using algebraic models more\nsophisticated than one simple matrix product, but rather inspired by\nmusicological knowledge. Ozerov et al. formalized this idea through a\ngeneral framework and showed its application for singing voice\nseparation ["),a("a",{attrs:{href:"#ref-ozerov102"}},[e._v("210")]),e._v("]–["),a("a",{attrs:{href:"#ref-salaun14"}},[e._v("212")]),e._v("].")]),a("p",[e._v("Finally, Hennequin and Rigaud augmented their model to account for\nlong-term reverberation, with application to singing voice separation\n["),a("a",{attrs:{href:"#ref-hennequin16"}},[e._v("213")]),e._v("]. They extended the model in\n["),a("a",{attrs:{href:"#ref-singh10"}},[e._v("214")]),e._v("] which allows extraction of the reverberation of\na specific source with its dry signal. They combined this model with the\nsource-filter NMF model in ["),a("a",{attrs:{href:"#ref-durrieu09"}},[e._v("189")]),e._v("].")]),a("h3",{attrs:{id:"different-constraints-for-different-sources"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#different-constraints-for-different-sources","aria-hidden":"true"}},[e._v("#")]),e._v(" Different constraints for different sources")]),a("p",[e._v("Algebraic methods that decompose the mixture spectrogram as the sum of\nthe lead and accompaniment spectrograms are based on the minimization of\na "),a("em",[e._v("cost")]),e._v(" or "),a("em",[e._v("loss function")]),e._v(" which measures the error between the\napproximation and the observation. While the methods presented above for\nlead and accompaniment separation did propose more sophisticated models\nwith parameters explicitly pertaining to the lead or the accompaniment,\nanother option that is also popular in the dedicated literature is to\nmodify the cost function of an optimization algorithm for an existing\nalgorithm (e.g., RPCA), so that one part of the resulting components\nwould preferentially account for one source or another.")]),a("p",[e._v("This approach can be exemplified by the harmonic-percussive source\nseparation method (HPSS), presented in ["),a("a",{attrs:{href:"#ref-ono08"}},[e._v("160")]),e._v("],\n["),a("a",{attrs:{href:"#ref-ono082"}},[e._v("215")]),e._v("], ["),a("a",{attrs:{href:"#ref-fitzgerald10"}},[e._v("216")]),e._v("]. It consists in\nfiltering a mixture spectrogram so that horizontal lines go in a\nso-called "),a("em",[e._v("harmonic")]),e._v(" source, while its vertical lines go into a\n"),a("em",[e._v("percussive")]),e._v(" source. Separation is then done with TF masking. Of course,\nsuch a method is not adequate for lead and accompaniment separation "),a("em",[e._v("per\nse")]),e._v(", because all the harmonic content of the accompaniment is classified\nas harmonic. However, it shows that "),a("em",[e._v("nonparametric")]),e._v(" approaches are also\nan option, provided the cost function itself is well chosen for each\nsource.")]),a("p",[e._v("This idea was followed by Yang in ["),a("a",{attrs:{href:"#ref-yang12"}},[e._v("217")]),e._v("] who proposed an\napproach based on RPCA with the incorporation of harmonicity priors and\na back-end drum removal procedure to improve the decomposition. He added\na regularization term in the algorithm to account for harmonic sounds in\nthe low-rank component and used an NMF-based model trained for drum\nseparation ["),a("a",{attrs:{href:"#ref-ozerov12"}},[e._v("211")]),e._v("] to eliminate percussive sounds in\nthe sparse component.")]),a("p",[e._v("Jeong and Lee proposed to separate a vocal signal from a music signal\n["),a("a",{attrs:{href:"#ref-jeong142"}},[e._v("218")]),e._v("], extending the HPSS approach in\n["),a("a",{attrs:{href:"#ref-ono08"}},[e._v("160")]),e._v("], ["),a("a",{attrs:{href:"#ref-ono082"}},[e._v("215")]),e._v("]. Assuming that the\nspectrogram of the signal can be represented as the sum of harmonic,\npercussive, and vocal components, they derived an objective function\nwhich enforces the temporal and spectral continuity of the harmonic and\npercussive components, respectively, similarly to ["),a("a",{attrs:{href:"#ref-ono08"}},[e._v("160")]),e._v("],\nbut also the sparsity of the vocal component. Assuming non-negativity of\nthe components, they then derived iterative update rules to minimize the\nobjective function. Ochiai et al. extended this work in\n["),a("a",{attrs:{href:"#ref-ochiai15"}},[e._v("219")]),e._v("], notably by imposing harmonic constraints for\nthe lead.")]),a("p",[e._v("Watanabe et al. extended RPCA for singing voice separation\n["),a("a",{attrs:{href:"#ref-watanabe16"}},[e._v("220")]),e._v("]. They added a harmonicity constraint in the\nobjective function to account for harmonic structures, such as in vocal\nsignals, and regularization terms to enforce the non-negativity of the\nsolution. They used the generalized forward-backward splitting algorithm\n["),a("a",{attrs:{href:"#ref-raguet13"}},[e._v("221")]),e._v("] to solve the optimization problem. They also\napplied post-processing to remove the low frequencies in the vocal\nspectrogram and built a TF mask to remove time frames with low energy.")]),a("p",[e._v("Going beyond smoothness and harmonicity, Hayashi et al. proposed an NMF\nwith a constraint to help separate periodic components, such as a\nrepeating accompaniment ["),a("a",{attrs:{href:"#ref-hayashi16"}},[e._v("222")]),e._v("]. They defined a\nperiodicity constraint which they incorporated in the objective function\nof the NMF algorithm to enforce the periodicity of the bases.")]),a("h3",{attrs:{id:"cascaded-and-iterated-methods"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cascaded-and-iterated-methods","aria-hidden":"true"}},[e._v("#")]),e._v(" Cascaded and iterated methods")]),a("p",[e._v("In their effort to propose separation methods for the lead and\naccompaniment in music, some authors discovered that very different\nmethods often have complementary strengths. This motivated the\n"),a("em",[e._v("combination")]),e._v(" of methods. In practice, there are several ways to follow\nthis line of research.")]),a("p",[e._v("One potential route to achieve better separation is to "),a("em",[e._v("cascade")]),e._v(" several\nmethods. This is what FitzGerald and Gainza proposed in\n["),a("a",{attrs:{href:"#ref-fitzgerald10"}},[e._v("216")]),e._v("] with multiple median filters\n["),a("a",{attrs:{href:"#ref-fitzgerald102"}},[e._v("148")]),e._v("]. They used a median-filter based HPSS\napproach at different frequency resolutions to separate a mixture into\nharmonic, percussive, and vocal components. They also investigated the\nuse of STFT or CQT as the TF representation and proposed a\npost-processing step to improve the separation with tensor factorization\ntechniques ["),a("a",{attrs:{href:"#ref-fitzgerald09"}},[e._v("223")]),e._v("] and non-negative partial\nco-factorization ["),a("a",{attrs:{href:"#ref-yoo10"}},[e._v("180")]),e._v("].")]),a("p",[e._v("The two-stage HPSS system proposed by Tachibana et al. in\n["),a("a",{attrs:{href:"#ref-tachibana14"}},[e._v("224")]),e._v("] proceeds the same way. It is an extension of\nthe melody extraction approach in ["),a("a",{attrs:{href:"#ref-tachibana10"}},[e._v("225")]),e._v("] and was\napplied for karaoke in ["),a("a",{attrs:{href:"#ref-tachibana16"}},[e._v("226")]),e._v("]. It consists in using\nthe optimization-based HPSS algorithm from ["),a("a",{attrs:{href:"#ref-ono08"}},[e._v("160")]),e._v("],\n["),a("a",{attrs:{href:"#ref-ono082"}},[e._v("215")]),e._v("], ["),a("a",{attrs:{href:"#ref-ono10"}},[e._v("227")]),e._v("],\n["),a("a",{attrs:{href:"#ref-tachibana12"}},[e._v("228")]),e._v("] at different frequency resolutions to\nseparate the mixture into harmonic, percussive, and vocal components.")]),a("p",[a("img",{attrs:{src:"https://docs.google.com/drawings/d/e/2PACX-1vSd3BGs_oCGRJlqPsH6O1ZDcbPPtc9ttABvNXvN8M5E2BggOcmToEVnVTmQPvAU4RMhCbm7MDerswoU/pub?w=392&h=499",alt:""}})]),a("h4",{attrs:{id:"cascading-source-separation-methods-the-results-from-method-a-is-improved-by-applying-methods-b-and-c-on-its-output-which-are-specialized-in-reducing-interferences-from-undesired-sources-in-each-signal"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cascading-source-separation-methods-the-results-from-method-a-is-improved-by-applying-methods-b-and-c-on-its-output-which-are-specialized-in-reducing-interferences-from-undesired-sources-in-each-signal","aria-hidden":"true"}},[e._v("#")]),e._v(" Cascading source separation methods. The results from method A is improved by applying methods B and C on its output, which are specialized in reducing interferences from undesired sources in each signal.")]),a("p",[e._v("HPSS was not the only separation module considered as the building block\nof combined lead and accompaniment separation approaches. Deif et al.\nalso proposed a multi-stage NMF-based algorithm ["),a("a",{attrs:{href:"#ref-deif15"}},[e._v("229")]),e._v("],\nbased on the approach in ["),a("a",{attrs:{href:"#ref-zhu13"}},[e._v("230")]),e._v("]. They used a local\nspectral discontinuity measure to refine the non-pitched components\nobtained from the factorization of the long window spectrogram and a\nlocal temporal discontinuity measure to refine the non-percussive\ncomponents obtained from factorization of the short window spectrogram.")]),a("p",[e._v("Finally, this cascading concept was considered again by Driedger and\nMüller in ["),a("a",{attrs:{href:"#ref-driedger15"}},[e._v("231")]),e._v("], that introduces a processing\npipeline for the outputs of different methods ["),a("a",{attrs:{href:"#ref-huang12"}},[e._v("115")]),e._v("],\n["),a("a",{attrs:{href:"#ref-virtanen08"}},[e._v("164")]),e._v("], ["),a("a",{attrs:{href:"#ref-driedger14"}},[e._v("232")]),e._v("],\n["),a("a",{attrs:{href:"#ref-talmon11"}},[e._v("233")]),e._v("] to obtain an improved separation quality. Their\ncore idea is depicted in\nFigure "),a("a",{attrs:{href:"#fig:methods_cascading"}},[e._v("[fig:methods_cascading]")]),e._v(" and\ncombines the output of different methods in a specific order to improve\nseparation.")]),a("p",[e._v("Another approach for improving the quality of separation when using\nseveral separation procedures is not to restrict the number of such\niterations from one method to another, but rather to iterate them many\ntimes until satisfactory results are obtained. This is what is proposed\nin Hsu et al. in ["),a("a",{attrs:{href:"#ref-hsu12"}},[e._v("234")]),e._v("], extending the algorithm in\n["),a("a",{attrs:{href:"#ref-hu10"}},[e._v("235")]),e._v("]. They first estimated the pitch range of the\nsinging voice by using the HPSS method in ["),a("a",{attrs:{href:"#ref-ono08"}},[e._v("160")]),e._v("],\n["),a("a",{attrs:{href:"#ref-tachibana10"}},[e._v("225")]),e._v("]. They separated the voice given the\nestimated pitch using a binary mask obtained by training a multilayer\nperceptron ["),a("a",{attrs:{href:"#ref-rumelhart86"}},[e._v("236")]),e._v("] and re-estimated the pitch given\nthe separated voice. Voice separation and pitch estimation are then\niterated until convergence.")]),a("p",[e._v("As another iterative method, Zhu et al. proposed a multi-stage NMF\n["),a("a",{attrs:{href:"#ref-zhu13"}},[e._v("230")]),e._v("], using harmonic and percussive separation at\ndifferent frequency resolutions similar to ["),a("a",{attrs:{href:"#ref-tachibana10"}},[e._v("225")]),e._v("]\nand ["),a("a",{attrs:{href:"#ref-fitzgerald10"}},[e._v("216")]),e._v("]. The main originality of their\ncontribution was to iterate the refinements instead of applying it only\nonce.")]),a("p",[e._v("An issue with such iterated methods lies in how to decide whether\nconvergence is obtained, and it is not clear whether the quality of the\nseparated signals will necessarily improve. For this reason, Bryan and\nMysore proposed a user-guided approach based on PLCA, which can be\napplied for the separation of the vocals\n["),a("a",{attrs:{href:"#ref-bryan13"}},[e._v("237")]),e._v("]–["),a("a",{attrs:{href:"#ref-bryan133"}},[e._v("239")]),e._v("]. They allowed a user\nto make annotations on the spectrogram of a mixture, incorporated the\nfeedback as constraints in a PLCA model ["),a("a",{attrs:{href:"#ref-smaragdis07"}},[e._v("110")]),e._v("],\n["),a("a",{attrs:{href:"#ref-raj05"}},[e._v("156")]),e._v("], and used a posterior regularization technique\n["),a("a",{attrs:{href:"#ref-ganchev10"}},[e._v("240")]),e._v("] to refine the estimates, repeating the process\nuntil the user is satisfied with the results. This is similar to the way\nOzerov et al. proposed to take user input into account in\n["),a("a",{attrs:{href:"#ref-ozerov13"}},[e._v("241")]),e._v("].")]),a("p",[a("img",{attrs:{src:"https://docs.google.com/drawings/d/e/2PACX-1vT4NTF1Q8ctDDQGm_bui5NLHZczmdH8XGB9K8Wq9R9E93k9Ev-INRXKmDIVifcz8X4JZoGdcuij6U-F/pub?w=540",alt:""}})]),a("h4",{attrs:{id:"fusion-of-separation-methods-the-output-of-many-separation-methods-is-fed-into-a-fusion-system-that-combines-them-to-produce-a-single-estimate"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fusion-of-separation-methods-the-output-of-many-separation-methods-is-fed-into-a-fusion-system-that-combines-them-to-produce-a-single-estimate","aria-hidden":"true"}},[e._v("#")]),e._v(" Fusion of separation methods. The output of many separation methods is fed into a fusion system that combines them to produce a single estimate.")]),a("p",[e._v("A principled way to aggregate the result of many source separation\nsystems to obtain one single estimate that is consistently better than\nall of them was presented by Jaureguiberry et al. in their "),a("em",[e._v("fusion\nframework")]),e._v(", depicted in\nFigure "),a("a",{attrs:{href:"#fig:methods_fusion"}},[e._v("[fig:methods_fusion]")]),e._v(". It takes\nadvantage of multiple existing approaches, and demonstrated its\napplication to singing voice separation\n["),a("a",{attrs:{href:"#ref-jaureguiberry13"}},[e._v("242")]),e._v("]–["),a("a",{attrs:{href:"#ref-jaureguiberry16"}},[e._v("244")]),e._v("]. They\ninvestigated fusion methods based on non-linear optimization, Bayesian\nmodel averaging ["),a("a",{attrs:{href:"#ref-hoeting99"}},[e._v("245")]),e._v("], and deep neural networks\n(DNN).")]),a("p",[e._v("As another attempt to design an efficient fusion method, McVicar et al.\nproposed in ["),a("a",{attrs:{href:"#ref-mcvicar16"}},[e._v("246")]),e._v("] to combine the outputs of RPCA\n["),a("a",{attrs:{href:"#ref-huang12"}},[e._v("115")]),e._v("], HPSS ["),a("a",{attrs:{href:"#ref-fitzgerald10"}},[e._v("216")]),e._v("], Gabor\nfiltered spectrograms ["),a("a",{attrs:{href:"#ref-jain90"}},[e._v("247")]),e._v("], REPET\n["),a("a",{attrs:{href:"#ref-rafii13"}},[e._v("130")]),e._v("] and an approach based on deep learning\n["),a("a",{attrs:{href:"#ref-huang14"}},[e._v("248")]),e._v("]. To do this, they used different classification\ntechniques to build the aggregated TF mask, such as a logistic\nregression model or a conditional random field (CRF) trained using the\nmethod in ["),a("a",{attrs:{href:"#ref-lacoste-julien13"}},[e._v("249")]),e._v("] with time and/or frequency\ndependencies.")]),a("p",[e._v("Manilow et al. trained a neural network to predict quality of source\nseparation for three source separation algorithms, each leveraging a\ndifferent cue - repetition, spatialization, and harmonicity/pitch\nproximity ["),a("a",{attrs:{href:"#ref-manilow17"}},[e._v("250")]),e._v("]. The method estimates separation\nquality of the lead vocals for each algorithm, using only the original\naudio mixture and separated source output. These estimates were used to\nguide switching between algorithms along time.")]),a("h3",{attrs:{id:"source-dependent-representations"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#source-dependent-representations","aria-hidden":"true"}},[e._v("#")]),e._v(" Source-dependent representations")]),a("p",[e._v("In the previous section, we stated that some authors considered\niterating separation at different frequency resolutions, i.e., using\ndifferent TF representations ["),a("a",{attrs:{href:"#ref-fitzgerald10"}},[e._v("216")]),e._v("],\n["),a("a",{attrs:{href:"#ref-tachibana14"}},[e._v("224")]),e._v("], ["),a("a",{attrs:{href:"#ref-deif15"}},[e._v("229")]),e._v("]. This can be seen as\na combination of different methods. However, this can also be seen from\nanother perspective as based on picking specific "),a("em",[e._v("representations")]),e._v(".")]),a("p",[e._v("Wolf et al. proposed an approach using rigid motion segmentation, with\napplication to singing voice separation ["),a("a",{attrs:{href:"#ref-wolf14"}},[e._v("251")]),e._v("],\n["),a("a",{attrs:{href:"#ref-wolf16"}},[e._v("252")]),e._v("]. They introduced harmonic template models with\namplitude and pitch modulations defined by a velocity vector. They\napplied a wavelet transform ["),a("a",{attrs:{href:"#ref-anden14"}},[e._v("253")]),e._v("] on the harmonic\ntemplate models to build an audio image where the amplitude and pitch\ndynamics can be separated through the velocity vector. They then derived\na velocity equation, similar to the optical flow velocity equation used\nin images ["),a("a",{attrs:{href:"#ref-bernard01"}},[e._v("254")]),e._v("], to segment velocity components.\nFinally, they identified the harmonic templates which model different\nsources in the mixture and separated them by approximating the velocity\nfield over the corresponding harmonic template models.")]),a("p",[e._v("Yen et al. proposed an approach using spectro-temporal modulation\nfeatures ["),a("a",{attrs:{href:"#ref-yen14"}},[e._v("255")]),e._v("], ["),a("a",{attrs:{href:"#ref-yen15"}},[e._v("256")]),e._v("]. They decomposed a\nmixture using a two-stage auditory model which consists of a cochlear\nmodule ["),a("a",{attrs:{href:"#ref-chi05"}},[e._v("257")]),e._v("] and cortical module ["),a("a",{attrs:{href:"#ref-chi99"}},[e._v("258")]),e._v("].\nThey then extracted spectro-temporal modulation features from the TF\nunits and clustered the TF units into harmonic, percussive, and vocal\ncomponents using the EM algorithm and resynthesized the estimated\nsignals.")]),a("p",[e._v("Chan and Yang proposed an approach using an informed group sparse\nrepresentation ["),a("a",{attrs:{href:"#ref-chan17"}},[e._v("259")]),e._v("]. They introduced a representation\nbuilt using a learned dictionary based on a chord sequence which\nexhibits group sparsity ["),a("a",{attrs:{href:"#ref-yuan06"}},[e._v("260")]),e._v("] and which can incorporate\nmelody annotations. They derived a formulation of the problem in a\nmanner similar to RPCA and solved it using the alternating direction\nmethod of multipliers ["),a("a",{attrs:{href:"#ref-ma16"}},[e._v("261")]),e._v("]. They also showed a relation\nbetween their representation and the low-rank representation in\n["),a("a",{attrs:{href:"#ref-yang13"}},[e._v("123")]),e._v("], ["),a("a",{attrs:{href:"#ref-liu13"}},[e._v("262")]),e._v("].")]),a("h3",{attrs:{id:"shortcomings-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#shortcomings-3","aria-hidden":"true"}},[e._v("#")]),e._v(" Shortcomings")]),a("p",[e._v("The large body of literature we reviewed in the preceding sections is\nconcentrated on choosing adequate models for the lead and accompaniment\nparts of music signals in order to devise effective signal processing\nmethods to achieve separation. From a higher perspective, their common\nfeature is to guide the separation process in a "),a("em",[e._v("model-based way")]),e._v(":\nfirst, the scientist has some idea regarding characteristics of the lead\nsignal and/or the accompaniment, and then an algorithm is designed to\nexploit this knowledge for separation.")]),a("p",[e._v("Model-based methods for lead and accompaniment separation are faced with\na common risk that their core assumptions will be violated for the\nsignal under study. For instance, the lead to be separated may not be\nharmonic but saturated vocals or the accompaniment may not be repetitive\nor redundant, but rather always changing. In such cases, model-based\nmethods are prone to large errors and poor performance.")]),a("h2",{attrs:{id:"data-driven-approaches"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#data-driven-approaches","aria-hidden":"true"}},[e._v("#")]),e._v(" Data-driven approaches")]),a("p",[e._v("A way to address the potential caveats of model-based separation\nbehaving badly in case of violated assumptions is to avoid making\nassumptions altogether, but rather to let the model be learned from a\nlarge and representative database of examples. This line of research\nleads to "),a("em",[e._v("data-driven")]),e._v(" methods, for which researchers are concerned\nabout directly estimating a mapping between the mixture and either the\nTF mask for separating the sources, or their spectrograms to be used for\ndesigning a filter.")]),a("p",[e._v("As may be foreseen, this strategy based on machine learning comes with\nseveral challenges of its own. First, it requires considerable amounts\nof data. Second, it typically requires a high-capacity learner (many\ntunable parameters) that can be prone to over-fitting the training data\nand therefore not working well on the audio it faces when deployed.")]),a("h3",{attrs:{id:"algebraic-approaches"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#algebraic-approaches","aria-hidden":"true"}},[e._v("#")]),e._v(" Algebraic approaches")]),a("p",[e._v("A natural way to exploit a training database was to learn some parts of\nthe model to guide the estimation process into better solutions. Work on\nthis topic may be traced back to the suggestion of Ozerov et al. in\n["),a("a",{attrs:{href:"#ref-ozerov05"}},[e._v("276")]),e._v("] to learn spectral template models based on a\ndatabase of isolated sources, and then to adapt this dictionary of\ntemplates on the mixture using the method in ["),a("a",{attrs:{href:"#ref-tsai04"}},[e._v("277")]),e._v("].")]),a("p",[e._v("The exploitation of training data was formalized by Smaragdis et al. in\n["),a("a",{attrs:{href:"#ref-smaragdis07"}},[e._v("110")]),e._v("] in the context of source separation within\nthe supervised and semi-supervised PLCA framework. The core idea of this\nprobabilistic formulation, equivalent to NMF, is to learn some spectral\nbases from the training set which are then kept fixed at separation\ntime.")]),a("p",[e._v("In the same line, Ozerov et al. proposed an approach using Bayesian\nmodels ["),a("a",{attrs:{href:"#ref-ozerov07"}},[e._v("191")]),e._v("]. They first segmented a song into vocal\nand non-vocal parts using GMMs with MFCCs. Then, they adapted a general\nmusic model on the non-vocal parts of a particular song by using the\nmaximum a posteriori (MAP) adaptation approach in\n["),a("a",{attrs:{href:"#ref-gauvain94"}},[e._v("278")]),e._v("]")]),a("p",[e._v("Ozerov et al. later proposed a framework for source separation which\ngeneralizes several approaches given prior information about the problem\nand showed its application for singing voice separation\n["),a("a",{attrs:{href:"#ref-ozerov102"}},[e._v("210")]),e._v("]–["),a("a",{attrs:{href:"#ref-salaun14"}},[e._v("212")]),e._v("]. They chose the local\nGaussian model ["),a("a",{attrs:{href:"#ref-vincent10"}},[e._v("279")]),e._v("] as the core of the framework\nand allowed the prior knowledge about each source and its mixing\ncharacteristics using user-specified constraints. Estimation was\nperformed through a generalized EM algorithm ["),a("a",{attrs:{href:"#ref-dempster77"}},[e._v("32")]),e._v("].")]),a("p",[e._v("Rafii et al. proposed in ["),a("a",{attrs:{href:"#ref-rafii132"}},[e._v("280")]),e._v("] to address the main\ndrawback of the repetition-based methods described in\nSection "),a("a",{attrs:{href:"#ssec:methods_repet"}},[e._v("4.3")]),e._v(", which is the weakness of the model\nfor vocals. For this purpose, they combined the REPET-SIM model\n["),a("a",{attrs:{href:"#ref-rafii12"}},[e._v("135")]),e._v("] for the accompaniment with a NMF-based model for\nsinging voice learned from a voice dataset.")]),a("p",[e._v("As yet another example of using training data for NMF,\nBoulanger-Lewandowski et al. proposed in\n["),a("a",{attrs:{href:"#ref-boulanger-lewandowski14"}},[e._v("281")]),e._v("] to exploit long-term temporal\ndependencies in NMF, embodied using recurrent neural networks (RNN)\n["),a("a",{attrs:{href:"#ref-rumelhart86"}},[e._v("236")]),e._v("]. They incorporated RNN regularization into\nthe NMF framework to temporally constrain the activity matrix during the\ndecomposition, which can be seen as a generalization of the non-negative\nHMM in ["),a("a",{attrs:{href:"#ref-mysore10"}},[e._v("282")]),e._v("]. Furthermore, they used supervised and\nsemi-supervised NMF algorithms on isolated sources to train the models,\nas in ["),a("a",{attrs:{href:"#ref-smaragdis07"}},[e._v("110")]),e._v("].")]),a("h3",{attrs:{id:"deep-neural-networks"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#deep-neural-networks","aria-hidden":"true"}},[e._v("#")]),e._v(" Deep neural networks")]),a("p",[a("img",{attrs:{src:"https://docs.google.com/drawings/d/e/2PACX-1vRXc_l4uNUTSOoAif8r4O-AKfAVBBMUSPVG_VMu79LjcZLb4xKLgFTVoSqVodvGetEvdeakfb4Nul-3/pub?w=720",alt:""}})]),a("h5",{attrs:{id:"general-architecture-for-methods-exploiting-deep-learning-the-network-inputs-the-mixture-and-outputs-either-the-sources-spectrograms-or-a-tf-mask-methods-usually-differ-in-their-choice-for-a-network-architecture-and-the-way-it-is-learned-using-the-training-data"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#general-architecture-for-methods-exploiting-deep-learning-the-network-inputs-the-mixture-and-outputs-either-the-sources-spectrograms-or-a-tf-mask-methods-usually-differ-in-their-choice-for-a-network-architecture-and-the-way-it-is-learned-using-the-training-data","aria-hidden":"true"}},[e._v("#")]),e._v(" General architecture for methods exploiting deep learning. The network inputs the mixture and outputs either the sources spectrograms or a TF mask. Methods usually differ in their choice for a network architecture and the way it is learned using the training data.")]),a("p",[e._v("Taking advantage of the recent availability of sufficiently large\ndatabases of isolated vocals along with their accompaniment, several\nresearchers investigated the use of machine learning methods to directly\nestimate a mapping between the mixture and the sources. Although\nend-to-end systems inputting and outputting the waveforms have already\nbeen proposed in the speech community ["),a("a",{attrs:{href:"#ref-qian17"}},[e._v("283")]),e._v("], they are\nnot yet available for music source separation. This may be due to the\nrelative small size of music separation databases, at most 10 h today.\nInstead, most systems feature pre and post-processing steps that consist\nin computing classical TF representations and building TF masks,\nrespectively. Although such end-to-end systems will inevitably be\nproposed in the near future, the common structure of deep learning\nmethods for lead and accompaniment separation usually corresponds for\nnow to the one depicted in\nFigure "),a("a",{attrs:{href:"#fig:methods_dnn"}},[e._v("[fig:methods_dnn]")]),e._v(". From a general\nperspective, we may say that most current methods mainly differ in the\nstructure picked for the network, as well as in the way it is learned.")]),a("p",[e._v("Providing a thorough introduction to deep neural networks is out of the\nscope of this paper. For our purpose, it suffices to mention that they\nconsist of a cascade of several possibly non-linear transformations of\nthe input, which are learned during a training stage. They were shown to\neffectively learn representations and mappings, provided enough data is\navailable for estimating their parameters\n["),a("a",{attrs:{href:"#ref-deng14"}},[e._v("284")]),e._v("]–["),a("a",{attrs:{href:"#ref-goodfellow16"}},[e._v("286")]),e._v("]. Different\narchitectures for neural networks may be combined/cascaded together, and\nmany architectures were proposed in the past, such as feedforward\nfully-connected neural networks (FNN), convolutional neural networks\n(CNN), or RNN and variants such as the long short-term memory (LSTM) and\nthe gated-recurrent units (GRU). Training of such functions is achieved\nby stochastic gradient descent ["),a("a",{attrs:{href:"#ref-robbins51"}},[e._v("287")]),e._v("] and associated\nalgorithms, such as backpropagation ["),a("a",{attrs:{href:"#ref-rumelhart862"}},[e._v("288")]),e._v("] or\nbackpropagation through time ["),a("a",{attrs:{href:"#ref-rumelhart86"}},[e._v("236")]),e._v("] for the case of\nRNNs.")]),a("p",[e._v("To the best of our knowledge, Huang et al. were the first to propose\ndeep neural networks, RNNs here ["),a("a",{attrs:{href:"#ref-hermans13"}},[e._v("289")]),e._v("],\n["),a("a",{attrs:{href:"#ref-pascanu14"}},[e._v("290")]),e._v("], for singing voice separation in\n["),a("a",{attrs:{href:"#ref-huang14"}},[e._v("248")]),e._v("], ["),a("a",{attrs:{href:"#ref-huang15"}},[e._v("291")]),e._v("]. They adapted their\nframework from ["),a("a",{attrs:{href:"#ref-huang142"}},[e._v("292")]),e._v("] to model all sources\nsimultaneously through masking. Input and target functions were the\nmixture magnitude and a joint representation of the individual sources.\nThe objective was to estimate jointly either singing voice and\naccompaniment music, or speech and background noise from the\ncorresponding mixtures.")]),a("p",[e._v("Modeling the temporal structures of both the lead and the accompaniment\nis a considerable challenge, even when using DNN methods. As an\nalternative to the RNN approach proposed by Huang et al. in\n["),a("a",{attrs:{href:"#ref-huang14"}},[e._v("248")]),e._v("], Uhlich et al. proposed the usage of FNNs\n["),a("a",{attrs:{href:"#ref-uhlich15"}},[e._v("293")]),e._v("] whose input consists of "),a("em",[e._v("supervectors")]),e._v(" of a few\nconsecutive frames from the mixture spectrogram. Later in\n["),a("a",{attrs:{href:"#ref-uhlich17"}},[e._v("294")]),e._v("], the same authors considered the use of\nbi-directional LSTMs for the same task.")]),a("p",[e._v("In an effort to make the resulting system less computationally demanding\nat separation time but still incorporating dynamic modeling of audio,\nSimpson et al. proposed in ["),a("a",{attrs:{href:"#ref-simpson15"}},[e._v("295")]),e._v("] to predict binary\nTF masks using deep CNNs, which typically utilize fewer parameters than\nthe FNNs. Similarly, Schlueter proposed a method trained to detect\nsinging voice using CNNs ["),a("a",{attrs:{href:"#ref-schlueter16"}},[e._v("296")]),e._v("]. In that case, the\ntrained network was used to compute "),a("em",[e._v("saliency maps")]),e._v(" from which TF masks\ncan be computed for singing voice separation. Chandna et al. also\nconsidered CNNs for lead separation in ["),a("a",{attrs:{href:"#ref-chandna17"}},[e._v("297")]),e._v("], with a\nparticular focus on low-latency.")]),a("p",[e._v("The classical FNN, LSTM and CNN structures above served as baseline\nstructures over which some others tried to improve. As a first example,\nMimilakis et al. proposed to use a hybrid structure of FNNs with skip\nconnections to separate the lead instrument for purposes of remixing\njazz recordings ["),a("a",{attrs:{href:"#ref-mimilakis16"}},[e._v("298")]),e._v("]. Such skip connections allow\nto propagate the input spectrogram to intermediate representations\nwithin the network, and mask it similarly to the operation of TF masks.\nAs advocated, this enforces the networks to approximate a TF masking\nprocess. Extensions to temporal data for singing voice separation were\npresented in ["),a("a",{attrs:{href:"#ref-mimilakis17"}},[e._v("299")]),e._v("], ["),a("a",{attrs:{href:"#ref-mimilakis172"}},[e._v("300")]),e._v("].\nSimilarly, Jansson et al. proposed to propagate the spectral information\ncomputed by convolutional layers to intermediate representations\n["),a("a",{attrs:{href:"#ref-jansson17"}},[e._v("301")]),e._v("]. This propagation aggregates intermediate\noutputs to proceeding layer(s). The output of the last layer is\nresponsible for masking the input mixture spectrogram. In the same vein,\nTakahashi et al. proposed to use skip connections via element-wise\naddition through representations computed by\nCNNs ["),a("a",{attrs:{href:"#ref-takahashi17"}},[e._v("302")]),e._v("].")]),a("p",[e._v("Apart from the structure of the network, the way it is trained,\ncomprising how the targets are computed, has a tremendous impact on\nperformance. As we saw, most methods operate on defining TF masks or\nestimating magnitude spectrograms. However, other methods were proposed\nbased on deep clustering ["),a("a",{attrs:{href:"#ref-hershey16"}},[e._v("303")]),e._v("],\n["),a("a",{attrs:{href:"#ref-isik16"}},[e._v("304")]),e._v("], where TF mask estimation is seen as a clustering\nproblem. Luo et al. investigated both approaches in\n["),a("a",{attrs:{href:"#ref-luo17"}},[e._v("305")]),e._v("] by proposing deep bidirectional LSTM networks\ncapable of outputting both TF masks or features to use as in deep\nclustering. Kim and Smaragdis proposed in ["),a("a",{attrs:{href:"#ref-kim15"}},[e._v("306")]),e._v("] another\nway to learn the model, in a denoising auto-encoding fashion\n["),a("a",{attrs:{href:"#ref-vincentp10"}},[e._v("307")]),e._v("], again utilizing short segments of the\nmixture spectrogram as an input to the network, as in\n["),a("a",{attrs:{href:"#ref-uhlich15"}},[e._v("293")]),e._v("].")]),a("p",[e._v("As the best network structure may vary from one track to another, some\nauthors considered a fusion of methods, in a manner similar to the\nmethod ["),a("a",{attrs:{href:"#ref-jaureguiberry13"}},[e._v("242")]),e._v("] presented above. Grais et.\nal ["),a("a",{attrs:{href:"#ref-grais16"}},[e._v("308")]),e._v("], ["),a("a",{attrs:{href:"#ref-grais162"}},[e._v("309")]),e._v("] proposed to\naggregate the results from an ensemble of feedforward DNNs to predict TF\nmasks for separation. An improvement was presented in\n["),a("a",{attrs:{href:"#ref-grais17"}},[e._v("310")]),e._v("], ["),a("a",{attrs:{href:"#ref-grais172"}},[e._v("311")]),e._v("] where the inputs to\nthe fusion network were separated signals, instead of TF masks, aiming\nat enhancing the reconstruction of the separated sources.")]),a("p",[e._v("As can be seen the use of deep learning methods for the design of lead\nand accompaniment separation has already stimulated a lot of research,\nalthough it is still in its infancy. Interestingly, we also note that\nusing audio and music specific knowledge appears to be fundamental in\ndesigning effective systems. As an example of this, the contribution\nfrom Nie et al. in ["),a("a",{attrs:{href:"#ref-nie15"}},[e._v("312")]),e._v("] was to include the construction\nof the TF mask as an extra non-linearity included in a recurrent\nnetwork. This is an exemplar of where signal processing elements, such\nas filtering through masking, are incorporated as a building block of\nthe machine learning method.")]),a("p",[e._v("The network structure is not the only thing that can benefit from audio\nknowledge for better separation. The design of appropriate features is\nanother. While we saw that supervectors of spectrogram patches offered\nthe ability to effectively model time-context information in\nFNNs ["),a("a",{attrs:{href:"#ref-uhlich15"}},[e._v("293")]),e._v("], Sebastian and\nMurthy ["),a("a",{attrs:{href:"#ref-sebastian16"}},[e._v("313")]),e._v("] proposed the use of the modified\ngroup delay feature representation ["),a("a",{attrs:{href:"#ref-yegnanarayana91"}},[e._v("314")]),e._v("] in\ntheir deep RNN architecture. They applied their approach for both\nsinging voice and vocal-violin separation.")]),a("p",[e._v("Finally, as with other methods, DNN-based separation techniques can also\nbe combined with others to yield improved performance. As an example,\nFan et al. proposed to use DNNs to separate the singing voice and to\nalso exploit vocal pitch estimation ["),a("a",{attrs:{href:"#ref-fan16"}},[e._v("315")]),e._v("]. They first\nextracted the singing voice using feedforward DNNs with sigmoid\nactivation functions. They then estimated the vocal pitch from the\nextracted singing voice using dynamic programming.")]),a("h3",{attrs:{id:"shortcomings-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#shortcomings-4","aria-hidden":"true"}},[e._v("#")]),e._v(" Shortcomings")]),a("p",[e._v("Data-driven methods are nowadays the topic of important research\nefforts, particularly those based on DNNs. This is notably due to their\nimpressive performance in terms of separation quality, as can, for\ninstance, be noticed below in Section "),a("a",{attrs:{href:"#sec:evaluation"}},[e._v("8")]),e._v(". However,\nthey also come with some limitations.")]),a("p",[e._v("First, we highlighted that lead and accompaniment separation in music\nhas the very specific problem of scarce data. Since it is very hard to\ngather large amounts of training data for that application, it is hard\nto fully exploit learning methods that require large training sets. This\nraises very specific challenges in terms of machine learning.")]),a("p",[e._v("Second, the lack of interpretability of model parameters is often\nmentioned as a significant shortcoming when it comes to applications.\nIndeed, music engineering systems are characterized by a strong\nimportance of human-computer interactions, because they are used in an\nartistic context that may require specific needs or results. As of\ntoday, it is unclear how to provide user interaction for controlling the\nmillions of parameters of DNN-based systems.")]),a("h2",{attrs:{id:"including-multichannel-information"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#including-multichannel-information","aria-hidden":"true"}},[e._v("#")]),e._v(" Including multichannel information")]),a("p",[e._v("In describing the above methods, we have not discussed the fact that\nmusic signals are typically stereophonic. On the contrary, the bulk of\nmethods we discussed focused on designing good spectrogram models for\nthe purpose of filtering mixtures that may be "),a("em",[e._v("monophonic")]),e._v(". Such a\nstrategy is called "),a("em",[e._v("single-channel")]),e._v(" source separation and is usually\npresented as more challenging than multichannel source separation.\nIndeed, only TF structure may then be used to discriminate the\naccompaniment from the lead. In stereo recordings, one further so-called\n"),a("em",[e._v("spatial")]),e._v(" dimension is introduced, which is sometimes referred to as\n"),a("em",[e._v("pan")]),e._v(", that corresponds to the perceived "),a("em",[e._v("position")]),e._v(" of a source in the\nstereo field. Devising methods to exploit this spatial diversity for\nsource separation has also been the topic of an important body of\nresearch that we review now.")]),a("h3",{attrs:{id:"extracting-the-lead-based-on-panning"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#extracting-the-lead-based-on-panning","aria-hidden":"true"}},[e._v("#")]),e._v(" Extracting the lead based on panning")]),a("p",[e._v("In the case of popular music signals, a fact of paramount practical\nimportance is that the lead signal — such as vocals — is very often\nmixed "),a("em",[e._v("in the center")]),e._v(", which means that its energy is approximately the\nsame in left and right channels. On the contrary, other instruments are\noften mixed at positions to the left or right of the stereo field.")]),a("p",[a("img",{attrs:{src:"https://docs.google.com/drawings/d/e/2PACX-1vT6DxnIs8i-BCZblC-rKuOOvI7ZRnlykXKiBznkuYrPbMANQohbPaF9vsM73J3Oobew7z3211WpaeS5/pub?w=720",alt:""}})]),a("h5",{attrs:{id:"separation-of-the-lead-based-on-panning-information-a-stereo-cue-called-panning-allows-to-design-a-tf-mask"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#separation-of-the-lead-based-on-panning-information-a-stereo-cue-called-panning-allows-to-design-a-tf-mask","aria-hidden":"true"}},[e._v("#")]),e._v(" Separation of the lead based on panning information. A stereo cue called panning allows to design a TF mask.")]),a("p",[e._v("The general structure of methods extracting the lead based on stereo\ncues is displayed on\nFigure "),a("a",{attrs:{href:"#fig:methods_panning"}},[e._v("[fig:methods_panning]")]),e._v(", introduced by\nAvendano, who proposed to separate sources in stereo mixtures by using a\npanning index ["),a("a",{attrs:{href:"#ref-avendano03"}},[e._v("316")]),e._v("]. He derived a two-dimensional\nmap by comparing left and right channels in the TF domain to identify\nthe different sources based on their panning position\n["),a("a",{attrs:{href:"#ref-avendano02"}},[e._v("317")]),e._v("]. The same methodology was considered by Barry\net al. in ["),a("a",{attrs:{href:"#ref-barry04"}},[e._v("318")]),e._v("] in his Azimuth Discrimination and\nResynthesis (ADRess) approach, with panning indexes computed with\ndifferences instead of ratios.")]),a("p",[e._v("Vinyes et al. also proposed to unmix commercially produced music\nrecordings thanks to stereo cues ["),a("a",{attrs:{href:"#ref-vinyes06"}},[e._v("319")]),e._v("]. They designed\nan interface similar to ["),a("a",{attrs:{href:"#ref-barry04"}},[e._v("318")]),e._v("] where a user can set\nsome parameters to generate different TF filters in real time. They\nshowed applications for extracting various instruments, including\nvocals.")]),a("p",[e._v("Cobos and López proposed to separate sources in stereo mixtures by using\nTF masking and multilevel thresholding ["),a("a",{attrs:{href:"#ref-cobos082"}},[e._v("320")]),e._v("]. They\nbased their approach on the Degenerate Unmixing Estimation Technique\n(DUET) ["),a("a",{attrs:{href:"#ref-yilmaz04"}},[e._v("321")]),e._v("]. They first derived histograms by\nmeasuring the amplitude relationship between TF points in left and right\nchannels. Then, they obtained several thresholds using the multilevel\nextension of Otsu’s method ["),a("a",{attrs:{href:"#ref-otsu79"}},[e._v("322")]),e._v("]. Finally, TF points\nwere assigned to their related sources to produce TF masks.")]),a("p",[e._v("Sofianos et al. proposed to separate the singing voice from a stereo\nmixture using ICA ["),a("a",{attrs:{href:"#ref-sofianos10"}},[e._v("323")]),e._v("]–["),a("a",{attrs:{href:"#ref-sofianos12"}},[e._v("325")]),e._v("].\nThey assumed that most commercial songs have the vocals panned to the\ncenter and that they dominate the other sources in amplitude. In\n["),a("a",{attrs:{href:"#ref-sofianos10"}},[e._v("323")]),e._v("], they proposed to combine a modified version\nof ADRess with ICA to filter out the other instruments. In\n["),a("a",{attrs:{href:"#ref-sofianos102"}},[e._v("324")]),e._v("], they proposed a modified version without\nADRess.")]),a("p",[e._v("Kim et al. proposed to separate centered singing voice in stereo music\nby exploiting binaural cues, such as inter-channel level and\ninter-channel phase difference ["),a("a",{attrs:{href:"#ref-kim11"}},[e._v("326")]),e._v("]. To this end, they\nbuild the pan-based TF mask through an EM algorithm, exploiting a GMM\nmodel on these cues.")]),a("h3",{attrs:{id:"augmenting-models-with-stereo"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#augmenting-models-with-stereo","aria-hidden":"true"}},[e._v("#")]),e._v(" Augmenting models with stereo")]),a("p",[e._v("As with using only a harmonic model for the lead signal, using stereo\ncues in isolation is not always sufficient for good separation, as there\ncan often be multiple sources at the same spatial location. Combining\nstereo cues with other methods improves performance in these cases.")]),a("p",[e._v("Cobos and López proposed to extract singing voice by combining panning\ninformation and pitch tracking ["),a("a",{attrs:{href:"#ref-cobos08"}},[e._v("327")]),e._v("]. They first\nobtained an estimate for the lead thanks to a pan-based method such as\n["),a("a",{attrs:{href:"#ref-avendano03"}},[e._v("316")]),e._v("], and refined the singing voice by using a TF\nbinary mask based on comb-filtering method as in\nSection "),a("a",{attrs:{href:"#ssec:harmonicity-combfiltering"}},[e._v("3.2")]),e._v(". The same combination\nwas proposed by Marxer et al. in ["),a("a",{attrs:{href:"#ref-marxer12"}},[e._v("87")]),e._v("] in a\nlow-latency context, with different methods used for the binaural cues\nand pitch tracking blocks.")]),a("p",[e._v("FitzGerald proposed to combine approaches based on repetition and\npanning to extract stereo vocals ["),a("a",{attrs:{href:"#ref-fitzgerald13"}},[e._v("328")]),e._v("]. He first\nused his nearest neighbors median filtering algorithm\n["),a("a",{attrs:{href:"#ref-fitzgerald12"}},[e._v("139")]),e._v("] to separate vocals and accompaniment from a\nstereo mixture. He then used the ADRess algorithm\n["),a("a",{attrs:{href:"#ref-barry04"}},[e._v("318")]),e._v("] and a high-pass filter to refine the vocals and\nimprove the accompaniment. In a somewhat different manner, FitzGerald\nand Jaiswal also proposed to combine approaches based on repetition and\npanning to improve stereo accompaniment recovery\n["),a("a",{attrs:{href:"#ref-fitzgerald132"}},[e._v("329")]),e._v("]. They presented an audio inpainting scheme\n["),a("a",{attrs:{href:"#ref-alder12"}},[e._v("330")]),e._v("] based on the nearest neighbors and median\nfiltering algorithm ["),a("a",{attrs:{href:"#ref-fitzgerald12"}},[e._v("139")]),e._v("] to recover TF regions\nof the accompaniment assigned to the vocals after using a source\nseparation algorithm based on panning information.")]),a("p",[e._v("In a more theoretically grounded manner, several methods based on a\nprobabilistic model were generalized to the multichannel case. For\ninstance, Durrieu et al. extended their source-filter model in\n["),a("a",{attrs:{href:"#ref-durrieu11"}},[e._v("201")]),e._v("], ["),a("a",{attrs:{href:"#ref-durrieu092"}},[e._v("205")]),e._v("] to handle stereo\nsignals, by incorporating the panning coefficients as model parameters\nto be estimated.")]),a("p",[e._v("Ozerov and Févotte proposed a multichannel NMF framework with\napplication to source separation, including vocals and music\n["),a("a",{attrs:{href:"#ref-ozerov09"}},[e._v("331")]),e._v("], ["),a("a",{attrs:{href:"#ref-ozerov10"}},[e._v("332")]),e._v("]. They adopted a\nstatistical model where each source is represented as a sum of Gaussian\ncomponents ["),a("a",{attrs:{href:"#ref-benaroya032"}},[e._v("193")]),e._v("], and where maximum likelihood\nestimation of the parameters is equivalent to NMF with the Itakura-Saito\ndivergence ["),a("a",{attrs:{href:"#ref-fevotte09"}},[e._v("94")]),e._v("]. They proposed two methods for\nestimating the parameters of their model, one that maximized the\nlikelihood of the multichannel data using EM, and one that maximized the\nsum of the likelihoods of all channels using a multiplicative update\nalgorithm inspired by NMF ["),a("a",{attrs:{href:"#ref-lee99"}},[e._v("90")]),e._v("].")]),a("p",[e._v("Ozerov et al. then proposed a multichannel non-negative tensor\nfactorization (NTF) model with application to user-guided source\nseparation ["),a("a",{attrs:{href:"#ref-ozerov11"}},[e._v("333")]),e._v("]. They modeled the sources jointly by\na 3-valence tensor (time/frequency/source) as in\n["),a("a",{attrs:{href:"#ref-liutkus10"}},[e._v("334")]),e._v("] which extends the multichannel NMF model in\n["),a("a",{attrs:{href:"#ref-ozerov10"}},[e._v("332")]),e._v("]. They used a generalized EM algorithm based on\nmultiplicative updates ["),a("a",{attrs:{href:"#ref-fevotte10"}},[e._v("335")]),e._v("] to minimize the\nobjective function. They incorporated information about the temporal\nsegmentation of the tracks and the number of components per track.\nOzerov et al. later proposed weighted variants of NMF and NTF with\napplication to user-guided source separation, including separation of\nvocals and music ["),a("a",{attrs:{href:"#ref-ozerov13"}},[e._v("241")]),e._v("], ["),a("a",{attrs:{href:"#ref-ozerov14"}},[e._v("336")]),e._v("].")]),a("p",[e._v("Sawada et al. also proposed multichannel extensions of NMF, tested for\nseparating stereo mixtures of multiple sources, including vocals and\naccompaniment ["),a("a",{attrs:{href:"#ref-sawada11"}},[e._v("337")]),e._v("]–["),a("a",{attrs:{href:"#ref-sawada13"}},[e._v("339")]),e._v("]. They\nfirst defined multichannel extensions of the cost function, namely,\nEuclidean distance and Itakura-Saito divergence, and derived\nmultiplicative update rules accordingly. They then proposed two\ntechniques for clustering the bases, one built into the NMF model and\none performing sequential pair-wise merges.")]),a("p",[e._v("Finally, multichannel information was also used with DNN models. Nugraha\net al. addressed the problem of multichannel source separation for\nspeech enhancement ["),a("a",{attrs:{href:"#ref-sivasankaran15"}},[e._v("340")]),e._v("],\n["),a("a",{attrs:{href:"#ref-nugraha162"}},[e._v("341")]),e._v("] and music separation\n["),a("a",{attrs:{href:"#ref-nugraha15"}},[e._v("342")]),e._v("], ["),a("a",{attrs:{href:"#ref-nugraha16"}},[e._v("343")]),e._v("]. In this framework,\nDNNs are still used for the spectrograms, while more classical EM\nalgorithms ["),a("a",{attrs:{href:"#ref-duong10"}},[e._v("344")]),e._v("], ["),a("a",{attrs:{href:"#ref-ozerov112"}},[e._v("345")]),e._v("] are used\nfor estimating the spatial parameters.")]),a("h3",{attrs:{id:"shortcomings-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#shortcomings-5","aria-hidden":"true"}},[e._v("#")]),e._v(" Shortcomings")]),a("p",[e._v("When compared to simply processing the different channels independently,\nincorporating spatial information in the separation method often comes\nat the cost of additional computational complexity. The resulting\nmethods are indeed usually more demanding in terms of computing power,\nbecause they involve the design of beamforming filters and inversion of\ncovariance matrices. While this is not really an issue for stereophonic\nmusic, this may become prohibiting in configurations with higher numbers\nof channels")]),a("h2",{attrs:{id:"references"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#references","aria-hidden":"true"}},[e._v("#")]),e._v(" References")]),a("div",{staticClass:"references",attrs:{id:"refs"}},[a("div",{attrs:{id:"ref-kalakota00"}},[a("p",[e._v("[1] R. Kalakota and M. Robinson, "),a("em",[e._v("E-business 2.0: Roadmap for\nsuccess")]),e._v(". Addison-Wesley Professional, 2000.")])]),a("div",{attrs:{id:"ref-lam01"}},[a("p",[e._v("[2] C. K. Lam and B. C. Tan, “The Internet is changing the music\nindustry,” "),a("em",[e._v("Communications of the ACM")]),e._v(", vol. 44, no. 8, pp. 62–68, 2001.")])]),a("div",{attrs:{id:"ref-common10"}},[a("p",[e._v("[3] P. Common and C. Jutten, "),a("em",[e._v("Handbook of blind source separation")]),e._v(".\nAcademic Press, 2010.")])]),a("div",{attrs:{id:"ref-naik14"}},[a("p",[e._v("[4] G. R. Naik and W. Wang, "),a("em",[e._v("Blind source separation")]),e._v(". Springer-Verlag\nBerlin Heidelberg, 2014.")])]),a("div",{attrs:{id:"ref-hyvarinen99"}},[a("p",[e._v("[5] A. Hyvärinen, “Fast and robust fixed-point algorithm for\nindependent component analysis,” "),a("em",[e._v("IEEE Transactions on Neural Networks")]),e._v(",\nvol. 10, no. 3, pp. 626–634, May 1999.")])]),a("div",{attrs:{id:"ref-hyvarinen00"}},[a("p",[e._v("[6] A. Hyvärinen and E. Oja, “Independent component analysis:\nAlgorithms and applications,” "),a("em",[e._v("Neural Networks")]),e._v(", vol. 13, nos. 4-5, pp.\n411–430, Jun. 2000.")])]),a("div",{attrs:{id:"ref-makino07"}},[a("p",[e._v("[7] S. Makino, T.-W. Lee, and H. Sawada, "),a("em",[e._v("Blind speech separation")]),e._v(".\nSpringer Netherlands, 2007.")])]),a("div",{attrs:{id:"ref-vincent18"}},[a("p",[e._v("[8] E. Vincent, T. Virtanen, and S. Gannot, "),a("em",[e._v("Audio source separation\nand speech enhancement")]),e._v(". Wiley, 2018.")])]),a("div",{attrs:{id:"ref-loizou13"}},[a("p",[e._v("[9] P. C. Loizou, "),a("em",[e._v("Speech enhancement: Theory and practice")]),e._v(". CRC\nPress, 1990.")])]),a("div",{attrs:{id:"ref-liutkus13"}},[a("p",[e._v("[10] A. Liutkus, J.-L. Durrieu, L. Daudet, and G. Richard, “An\noverview of informed audio source separation,” in "),a("em",[e._v("14th international\nworkshop on image analysis for multimedia interactive services")]),e._v(", 2013.")])]),a("div",{attrs:{id:"ref-vincent14"}},[a("p",[e._v("[11] E. Vincent, N. Bertin, R. Gribonval, and F. Bimbot, “From blind\nto guided audio source separation: How models and side information can\nimprove the separation of sound,” "),a("em",[e._v("IEEE Signal Processing Magazine")]),e._v(",\nvol. 31, no. 3, pp. 107–115, May 2014.")])]),a("div",{attrs:{id:"ref-zolzer11"}},[a("p",[e._v("[12] U. Zölzer, "),a("em",[e._v("DAFX - digital audio effects")]),e._v(". Wiley, 2011.")])]),a("div",{attrs:{id:"ref-muller2015"}},[a("p",[e._v("[13] M. Müller, "),a("em",[e._v("Fundamentals of music processing: Audio, analysis,\nalgorithms, applications")]),e._v(". Springer, 2015.")])]),a("div",{attrs:{id:"ref-jaynes2003probability"}},[a("p",[e._v("[14] E. T. Jaynes, "),a("em",[e._v("Probability theory: The logic of science")]),e._v(".\nCambridge university press, 2003.")])]),a("div",{attrs:{id:"ref-cappe2005"}},[a("p",[e._v("[15] O. Cappé, E. Moulines, and T. Ryden, "),a("em",[e._v("Inference in hidden markov\nmodels (springer series in statistics)")]),e._v(". Secaucus, NJ, USA:\nSpringer-Verlag New York, Inc., 2005.")])]),a("div",{attrs:{id:"ref-mcaulay86"}},[a("p",[e._v("[16] R. J. McAulay and T. F. Quatieri, “Speech analysis/synthesis\nbased on a sinusoidal representation,” "),a("em",[e._v("IEEE Transactions on Audio,\nSpeech, and Language Processing")]),e._v(", vol. 34, no. 4, pp. 744–754, Aug.\n1986.")])]),a("div",{attrs:{id:"ref-rickard02"}},[a("p",[e._v("[17] S. Rickard and O. Yilmaz, “On the approximate w-disjoint\northogonality of speech,” in "),a("em",[e._v("IEEE international conference on\nacoustics, speech, and signal processing")]),e._v(", 2002.")])]),a("div",{attrs:{id:"ref-boll1979"}},[a("p",[e._v("[18] S. Boll, “Suppression of acoustic noise in speech using spectral\nsubtraction,” "),a("em",[e._v("IEEE Transactions on acoustics, speech, and signal\nprocessing")]),e._v(", vol. 27, no. 2, pp. 113–120, 1979.")])]),a("div",{attrs:{id:"ref-wiener1975"}},[a("p",[e._v("[19] N. Wiener, “Extrapolation, interpolation, and smoothing of\nstationary time series,” 1975.")])]),a("div",{attrs:{id:"ref-liutkus15c"}},[a("p",[e._v("[20] A. Liutkus and R. Badeau, “Generalized Wiener filtering with\nfractional power spectrograms,” in "),a("em",[e._v("IEEE international conference on\nacoustics, speech and signal processing")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-fant70"}},[a("p",[e._v("[21] G. Fant, "),a("em",[e._v("Acoustic theory of speech production")]),e._v(". Walter de\nGruyter, 1970.")])]),a("div",{attrs:{id:"ref-bogert1963"}},[a("p",[e._v("[22] B. P. Bogert, M. J. R. Healy, and J. W. Tukey, “The quefrency\nalanysis of time series for echoes: Cepstrum pseudo-autocovariance,\ncross-cepstrum, and saphe cracking,” "),a("em",[e._v("Proceedings of a symposium on time\nseries analysis")]),e._v(", pp. 209–243, 1963.")])]),a("div",{attrs:{id:"ref-noll64"}},[a("p",[e._v("[23] A. M. Noll, “Short-time spectrum and ‘cepstrum’ techniques for\nvocal-pitch detection,” "),a("em",[e._v("Journal of the Acoustical Society of America")]),e._v(",\nvol. 36, no. 2, pp. 296–302, 1964.")])]),a("div",{attrs:{id:"ref-noll67"}},[a("p",[e._v("[24] A. M. Noll, “Cepstrum pitch determination,” "),a("em",[e._v("Journal of the\nAcoustical Society of America")]),e._v(", vol. 41, no. 2, pp. 293–309, 1967.")])]),a("div",{attrs:{id:"ref-david80"}},[a("p",[e._v("[25] S. B. Davis and P. Mermelstein, “Comparison of parametric\nrepresentations for monosyllabic word recognition in continuously spoken\nsentences,” "),a("em",[e._v("IEEE Transactions on Audio, Speech, and Language\nProcessing")]),e._v(", vol. 28, no. 4, pp. 357–366, Aug. 1980.")])]),a("div",{attrs:{id:"ref-oppenheim69"}},[a("p",[e._v("[26] A. V. Oppenheim, “Speech analysis-synthesis system based on\nhomomorphic filtering,” "),a("em",[e._v("Journal of the Acoustical Society of America")]),e._v(",\nvol. 45, no. 2, pp. 458–465, 1969.")])]),a("div",{attrs:{id:"ref-durrett2010probability"}},[a("p",[e._v("[27] R. Durrett, "),a("em",[e._v("Probability: Theory and examples")]),e._v(". Cambridge\nuniversity press, 2010.")])]),a("div",{attrs:{id:"ref-schwarz78"}},[a("p",[e._v("[28] G. Schwarz, “Estimating the dimension of a model,” "),a("em",[e._v("Annals of\nStatistics")]),e._v(", vol. 6, no. 2, pp. 461–464, Mar. 1978.")])]),a("div",{attrs:{id:"ref-rabiner89"}},[a("p",[e._v("[29] L. R. Rabiner, “A tutorial on hidden Markov models and selected\napplications in speech recognition,” "),a("em",[e._v("Proceedings of the IEEE")]),e._v(", vol. 77,\nno. 2, pp. 257–286, Feb. 1989.")])]),a("div",{attrs:{id:"ref-viterbi2006"}},[a("p",[e._v("[30] A. J. Viterbi, “A personal history of the Viterbi algorithm,”\n"),a("em",[e._v("IEEE Signal Processing Magazine")]),e._v(", vol. 23, no. 4, pp. 120–142, 2006.")])]),a("div",{attrs:{id:"ref-bishop96"}},[a("p",[e._v("[31] C. Bishop, "),a("em",[e._v("Neural networks for pattern recognition")]),e._v(". Clarendon\nPress, 1996.")])]),a("div",{attrs:{id:"ref-dempster77"}},[a("p",[e._v("[32] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood\nfrom incomplete data via the EM algorithm,” "),a("em",[e._v("Journal of the Royal\nStatistical Society")]),e._v(", vol. 39, no. 1, pp. 1–38, 1977.")])]),a("div",{attrs:{id:"ref-salamon14"}},[a("p",[e._v("[33] J. Salamon, E. Gómez, D. Ellis, and G. Richard, “Melody\nextraction from polyphonic music signals: Approaches, applications and\nchallenges,” "),a("em",[e._v("IEEE Signal Processing Magazine")]),e._v(", vol. 31, 2014.")])]),a("div",{attrs:{id:"ref-miller73"}},[a("p",[e._v("[34] N. J. Miller, “Removal of noise from a voice signal by\nsynthesis,” Utah University, 1973.")])]),a("div",{attrs:{id:"ref-oppenheim68"}},[a("p",[e._v("[35] A. V. Oppenheim and R. W. Schafer, “Homomorphic analysis of\nspeech,” "),a("em",[e._v("IEEE Transactions on Audio and Electroacoustics")]),e._v(", vol. 16, no.\n2, pp. 221–226, Jun. 1968.")])]),a("div",{attrs:{id:"ref-maher89"}},[a("p",[e._v("[36] R. C. Maher, “An approach for the separation of voices in\ncomposite musical signals,” PhD thesis, University of Illinois at\nUrbana-Champaign, 1989.")])]),a("div",{attrs:{id:"ref-wang94"}},[a("p",[e._v("[37] A. L. Wang, “Instantaneous and frequency-warped techniques for\nauditory source separation,” PhD thesis, Stanford University, 1994.")])]),a("div",{attrs:{id:"ref-wang95"}},[a("p",[e._v("[38] A. L. Wang, “Instantaneous and frequency-warped techniques for\nsource separation and signal parametrization,” in "),a("em",[e._v("IEEE workshop on\napplications of signal processing to audio and acoustics")]),e._v(", 1995.")])]),a("div",{attrs:{id:"ref-meron98"}},[a("p",[e._v("[39] Y. Meron and K. Hirose, “Separation of singing and piano sounds,”\nin "),a("em",[e._v("5th international conference on spoken language processing")]),e._v(", 1998.")])]),a("div",{attrs:{id:"ref-quatieri92"}},[a("p",[e._v("[40] T. F. Quatieri, “Shape invariant time-scale and pitch\nmodification of speech,” "),a("em",[e._v("IEEE Transactions on Signal Processing")]),e._v(", vol.\n40, no. 3, pp. 497–510, Mar. 1992.")])]),a("div",{attrs:{id:"ref-ben-shalom04"}},[a("p",[e._v("[41] A. Ben-Shalom and S. Dubnov, “Optimal filtering of an instrument\nsound in a mixed recording given approximate pitch prior,” in\n"),a("em",[e._v("International computer music conference")]),e._v(", 2004.")])]),a("div",{attrs:{id:"ref-shalev-shwartz02"}},[a("p",[e._v("[42] S. Shalev-Shwartz, S. Dubnov, N. Friedman, and Y. Singer, “Robust\ntemporal and spectral modeling for query by melody,” in "),a("em",[e._v("25th annual\ninternational acm sigir conference on research and development in\ninformation retrieval")]),e._v(", 2002.")])]),a("div",{attrs:{id:"ref-serra97"}},[a("p",[e._v("[43] X. Serra, “Musical sound modeling with sinusoids plus noise,” in\n"),a("em",[e._v("Musical signal processing")]),e._v(", Swets & Zeitlinger, 1997, pp. 91–122.")])]),a("div",{attrs:{id:"ref-vanveen97"}},[a("p",[e._v("[44] B. V. Veen and K. M. Buckley, “Beamforming techniques for spatial\nfiltering,” in "),a("em",[e._v("The digital signal processing handbook")]),e._v(", CRC Press,\n1997, pp. 1–22.")])]),a("div",{attrs:{id:"ref-zhang05"}},[a("p",[e._v("[45] Y.-G. Zhang and C.-S. Zhang, “Separation of voice and music by\nharmonic structure stability analysis,” in "),a("em",[e._v("IEEE international\nconference on multimedia and expo")]),e._v(", 2005.")])]),a("div",{attrs:{id:"ref-zhang06"}},[a("p",[e._v("[46] Y.-G. Zhang and C.-S. Zhang, “Separation of music signals by\nharmonic structure modeling,” in "),a("em",[e._v("Advances in neural information\nprocessing systems 18")]),e._v(", MIT Press, 2006, pp. 1617–1624.")])]),a("div",{attrs:{id:"ref-terhardt79"}},[a("p",[e._v("[47] E. Terhardt, “Calculating virtual pitch,” "),a("em",[e._v("Hearing Research")]),e._v(",\nvol. 1, no. 2, pp. 155–182, Mar. 1979.")])]),a("div",{attrs:{id:"ref-zhang03"}},[a("p",[e._v("[48] Y.-G. Zhang, C.-S. Zhang, and S. Wang, “Clustering in knowledge\nembedded space,” in "),a("em",[e._v("Machine learning: ECML 2003")]),e._v(", Springer Berlin\nHeidelberg, 2003, pp. 480–491.")])]),a("div",{attrs:{id:"ref-fujihara05"}},[a("p",[e._v("[49] H. Fujihara, T. Kitahara, M. Goto, K. Komatani, T. Ogata, and H.\nG. Okuno, “Singer identification based on accompaniment sound reduction\nand reliable frame selection,” in "),a("em",[e._v("6th international conference on music\ninformation retrieval")]),e._v(", 2005.")])]),a("div",{attrs:{id:"ref-fujihara10"}},[a("p",[e._v("[50] H. Fujihara, M. Goto, T. Kitahara, and H. G. Okuno, “A modeling\nof singing voice robust to accompaniment sounds and its application to\nsinger identification and vocal-timbre-similarity-based music\ninformation retrieval,” "),a("em",[e._v("IEEE Transactions on Audio, Speech, and\nLanguage Processing")]),e._v(", vol. 18, no. 3, pp. 638–648, Mar. 2010.")])]),a("div",{attrs:{id:"ref-goto04"}},[a("p",[e._v("[51] M. Goto, “A real-time music-scene-description system:\nPredominant-F0 estimation for detecting melody and bass lines in\nreal-world audio signals,” "),a("em",[e._v("Speech Communication")]),e._v(", vol. 43, no. 4, pp.\n311–329, Sep. 2004.")])]),a("div",{attrs:{id:"ref-moorer05"}},[a("p",[e._v("[52] J. A. Moorer, “Signal processing aspects of computer music: A\nsurvey,” "),a("em",[e._v("Proceedings of the IEEE")]),e._v(", vol. 65, no. 8, pp. 1108–1137, Aug.\n2005.")])]),a("div",{attrs:{id:"ref-mesaros07"}},[a("p",[e._v("[53] A. Mesaros, T. Virtanen, and A. Klapuri, “Singer identification\nin polyphonic music using vocal separation and pattern recognition\nmethods,” in "),a("em",[e._v("7th international conference on music information\nretrieval")]),e._v(", 2007.")])]),a("div",{attrs:{id:"ref-ryynanen06"}},[a("p",[e._v("[54] M. Ryynänen and A. Klapuri, “Transcription of the singing melody\nin polyphonic music,” in "),a("em",[e._v("7th international conference on music\ninformation retrieval")]),e._v(", 2006.")])]),a("div",{attrs:{id:"ref-duan08"}},[a("p",[e._v("[55] Z. Duan, Y.-F. Zhang, C.-S. Zhang, and Z. Shi, “Unsupervised\nsingle-channel music source separation by average harmonic structure\nmodeling,” "),a("em",[e._v("IEEE Transactions on Audio, Speech, and Language\nProcessing")]),e._v(", vol. 16, no. 4, pp. 766–778, May 2008.")])]),a("div",{attrs:{id:"ref-rodet97"}},[a("p",[e._v("[56] X. Rodet, “Musical sound signal analysis/synthesis:\nSinusoidal+Residual and elementary waveform models,” in "),a("em",[e._v("IEEE\ntime-frequency and time-scale workshop")]),e._v(", 1997.")])]),a("div",{attrs:{id:"ref-smith87"}},[a("p",[e._v("[57] J. O. Smith and X. Serra, “PARSHL: An analysis/synthesis program\nfor non-harmonic sounds based on a sinusoidal representation,” in\n"),a("em",[e._v("International computer music conference")]),e._v(", 1987.")])]),a("div",{attrs:{id:"ref-slaney94"}},[a("p",[e._v("[58] M. Slaney, D. Naar, and R. F. Lyon, “Auditory model inversion for\nsound separation,” in "),a("em",[e._v("IEEE international conference on acoustics,\nspeech and signal processing")]),e._v(", 1994.")])]),a("div",{attrs:{id:"ref-lagrange07"}},[a("p",[e._v("[59] M. Lagrange and G. Tzanetakis, “Sound source tracking and\nformation using normalized cuts,” in "),a("em",[e._v("IEEE international conference on\nacoustics, speech and signal processing")]),e._v(", 2007.")])]),a("div",{attrs:{id:"ref-lagrange08"}},[a("p",[e._v("[60] M. Lagrange, L. G. Martins, J. Murdoch, and G. Tzanetakis,\n“Normalized cuts for predominant melodic source separation,” "),a("em",[e._v("IEEE\nTransactions on Audio, Speech, and Language Processing")]),e._v(", vol. 16, no. 2,\npp. 278–290, Feb. 2008.")])]),a("div",{attrs:{id:"ref-shi00"}},[a("p",[e._v("[61] J. Shi and J. Malik, “Normalized cuts and image segmentation,”\n"),a("em",[e._v("IEEE Transactions on Pattern Analysis and Machine Intelligence")]),e._v(", vol.\n22, no. 8, pp. 888–905, Aug. 2000.")])]),a("div",{attrs:{id:"ref-ryynanen08"}},[a("p",[e._v("[62] M. Ryynänen, T. Virtanen, J. Paulus, and A. Klapuri,\n“Accompaniment separation and karaoke application based on automatic\nmelody transcription,” in "),a("em",[e._v("IEEE international conference on multimedia\nand expo")]),e._v(", 2008.")])]),a("div",{attrs:{id:"ref-ryynanen082"}},[a("p",[e._v("[63] M. Ryynänen and A. Klapuri, “Automatic transcription of melody,\nbass line, and chords in polyphonic music,” "),a("em",[e._v("Computer Music Journal")]),e._v(",\nvol. 32, no. 3, pp. 72–86, Sep. 2008.")])]),a("div",{attrs:{id:"ref-ding97"}},[a("p",[e._v("[64] Y. Ding and X. Qian, “Processing of musical tones using a\ncombined quadratic polynomial-phase sinusoid and residual (QUASAR)\nsignal model,” "),a("em",[e._v("Journal of the Audio Engineering Society")]),e._v(", vol. 45, no.\n7/8, pp. 571–584, Jul. 1997.")])]),a("div",{attrs:{id:"ref-li06"}},[a("p",[e._v("[65] Y. Li and D. Wang, “Singing voice separation from monaural\nrecordings,” in "),a("em",[e._v("7th international conference on music information\nretrieval")]),e._v(", 2006.")])]),a("div",{attrs:{id:"ref-li07"}},[a("p",[e._v("[66] Y. Li and D. Wang, “Separation of singing voice from music\naccompaniment for monaural recordings,” "),a("em",[e._v("IEEE Transactions on Audio,\nSpeech, and Language Processing")]),e._v(", vol. 15, no. 4, pp. 1475–1487, May\n2007.")])]),a("div",{attrs:{id:"ref-duxbury03"}},[a("p",[e._v("[67] C. Duxbury, J. P. Bello, M. Davies, and M. Sandler, “Complex\ndomain onset detection for musical signals,” in "),a("em",[e._v("6th international\nconference on digital audio effects")]),e._v(", 2003.")])]),a("div",{attrs:{id:"ref-li05"}},[a("p",[e._v("[68] Y. Li and D. Wang, “Detecting pitch of singing voice in\npolyphonic audio,” in "),a("em",[e._v("IEEE international conference on acoustics,\nspeech and signal processing")]),e._v(", 2005.")])]),a("div",{attrs:{id:"ref-wu03"}},[a("p",[e._v("[69] M. Wu, D. Wang, and G. J. Brown, “A multipitch tracking algorithm\nfor noisy speech,” "),a("em",[e._v("IEEE Transactions on Audio, Speech, and Language\nProcessing")]),e._v(", vol. 11, no. 3, pp. 229–241, May 2003.")])]),a("div",{attrs:{id:"ref-hu02"}},[a("p",[e._v("[70] G. Hu and D. Wang, “Monaural speech segregation based on pitch\ntracking and amplitude modulation,” "),a("em",[e._v("IEEE Transactions on Neural\nNetworks")]),e._v(", vol. 15, no. 5, pp. 1135–1150, Sep. 2002.")])]),a("div",{attrs:{id:"ref-han07"}},[a("p",[e._v("[71] Y. Han and C. Raphael, “Desoloing monaural audio using mixture\nmodels,” in "),a("em",[e._v("7th international conference on music information\nretrieval")]),e._v(", 2007.")])]),a("div",{attrs:{id:"ref-roweis01"}},[a("p",[e._v("[72] S. T. Roweis, “One microphone source separation,” in "),a("em",[e._v("Advances in\nneural information processing systems 13")]),e._v(", MIT Press, 2001, pp. 793–799.")])]),a("div",{attrs:{id:"ref-hsu08"}},[a("p",[e._v("[73] C.-L. Hsu, J.-S. R. Jang, and T.-L. Tsai, “Separation of singing\nvoice from music accompaniment with unvoiced sounds reconstruction for\nmonaural recordings,” in "),a("em",[e._v("AES 125th convention")]),e._v(", 2008.")])]),a("div",{attrs:{id:"ref-hsu10"}},[a("p",[e._v("[74] C.-L. Hsu and J.-S. R. Jang, “On the improvement of singing voice\nseparation for monaural recordings using the MIR-1K dataset,” "),a("em",[e._v("IEEE\nTransactions on Audio, Speech, and Language Processing")]),e._v(", vol. 18, no. 2,\npp. 310–319, Feb. 2010.")])]),a("div",{attrs:{id:"ref-dressler062"}},[a("p",[e._v("[75] K. Dressler, “Sinusoidal extraction using an efficient\nimplementation of a multi-resolution FFT,” in "),a("em",[e._v("9th international\nconference on digital audio effects")]),e._v(", 2006.")])]),a("div",{attrs:{id:"ref-scalart96"}},[a("p",[e._v("[76] P. Scalart and J. V. Filho, “Speech enhancement based on a priori\nsignal to noise estimation,” in "),a("em",[e._v("IEEE international conference on\nacoustics, speech and signal processing")]),e._v(", 1996.")])]),a("div",{attrs:{id:"ref-raphael08"}},[a("p",[e._v("[77] C. Raphael and Y. Han, “A classifier-based approach to\nscore-guided music audio source separation,” "),a("em",[e._v("Computer Music Journal")]),e._v(",\nvol. 32, no. 1, pp. 51–59, 2008.")])]),a("div",{attrs:{id:"ref-breiman84"}},[a("p",[e._v("[78] L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen,\n"),a("em",[e._v("Classification and regression trees")]),e._v(". Chapman; Hall/CRC, 1984.")])]),a("div",{attrs:{id:"ref-cano09"}},[a("p",[e._v("[79] E. Cano and C. Cheng, “Melody line detection and source\nseparation in classical saxophone recordings,” in "),a("em",[e._v("12th international\nconference on digital audio effects")]),e._v(", 2009.")])]),a("div",{attrs:{id:"ref-grollmisch11"}},[a("p",[e._v("[80] S. Grollmisch, E. Cano, and C. Dittmar, “Songs2See: Learn to play\nby playing,” in "),a("em",[e._v("AES 41st conference: Audio for games")]),e._v(", 2011, pp. P2–3.")])]),a("div",{attrs:{id:"ref-dittmar12"}},[a("p",[e._v("[81] C. Dittmar, E. Cano, J. Abeßer, and S. Grollmisch, “Music\ninformation retrieval meets music education,” in "),a("em",[e._v("Multimodal music\nprocessing")]),e._v(", Dagstuhl Publishing, 2012, pp. 95–120.")])]),a("div",{attrs:{id:"ref-cano12"}},[a("p",[e._v("[82] E. Cano, C. Dittmar, and G. Schuller, “Efficient implementation\nof a system for solo and accompaniment separation in polyphonic music,”\nin "),a("em",[e._v("20th european signal processing conference")]),e._v(", 2012.")])]),a("div",{attrs:{id:"ref-dressler11"}},[a("p",[e._v("[83] K. Dressler, “Pitch estimation by the pair-wise evaluation of\nspectral peaks,” in "),a("em",[e._v("42nd aes conference on semantic audio")]),e._v(", 2011.")])]),a("div",{attrs:{id:"ref-cano13"}},[a("p",[e._v("[84] E. Cano, C. Dittmar, and G. Schuller, “Re-thinking sound\nseparation: Prior information and additivity constraints in separation\nalgorithms,” in "),a("em",[e._v("16th international conference on digital audio\neffects")]),e._v(", 2013.")])]),a("div",{attrs:{id:"ref-cano14"}},[a("p",[e._v("[85] E. Cano, G. Schuller, and C. Dittmar, “Pitch-informed solo and\naccompaniment separation towards its use in music education\napplications,” "),a("em",[e._v("EURASIP Journal on Advances in Signal Processing")]),e._v(", vol.\n2014, no. 23, Sep. 2014.")])]),a("div",{attrs:{id:"ref-bosch12"}},[a("p",[e._v("[86] J. J. Bosch, K. Kondo, R. Marxer, and J. Janer, “Score-informed\nand timbre independent lead instrument separation in real-world\nscenarios,” in "),a("em",[e._v("20th european signal processing conference")]),e._v(", 2012.")])]),a("div",{attrs:{id:"ref-marxer12"}},[a("p",[e._v("[87] R. Marxer, J. Janer, and J. Bonada, “Low-latency instrument\nseparation in polyphonic audio using timbre models,” in "),a("em",[e._v("10th\ninternational conference on latent variable analysis and signal\nseparation")]),e._v(", 2012.")])]),a("div",{attrs:{id:"ref-vaneph16"}},[a("p",[e._v("[88] A. Vaneph, E. McNeil, and F. Rigaud, “An automated source\nseparation technology and its practical applications,” in "),a("em",[e._v("140th aes\nconvention")]),e._v(", 2016.")])]),a("div",{attrs:{id:"ref-leglaive15"}},[a("p",[e._v("[89] S. Leglaive, R. Hennequin, and R. Badeau, “Singing voice\ndetection with deep recurrent neural networks,” in "),a("em",[e._v("IEEE international\nconference on acoustics, speech and signal processing")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-lee99"}},[a("p",[e._v("[90] D. D. Lee and H. S. Seung, “Learning the parts of objects by\nnon-negative matrix factorization,” "),a("em",[e._v("Nature")]),e._v(", vol. 401, pp. 788–791,\nOct. 1999.")])]),a("div",{attrs:{id:"ref-lee01"}},[a("p",[e._v("[91] D. D. Lee and H. S. Seung, “Algorithms for non-negative matrix\nfactorization,” in "),a("em",[e._v("Advances in neural information processing systems\n13")]),e._v(", MIT Press, 2001, pp. 556–562.")])]),a("div",{attrs:{id:"ref-smaragdis03"}},[a("p",[e._v("[92] P. Smaragdis and J. C. Brown, “Non-negative matrix factorization\nfor polyphonic music transcription,” in "),a("em",[e._v("IEEE workshop on applications\nof signal processing to audio and acoustics")]),e._v(", 2003.")])]),a("div",{attrs:{id:"ref-virtanen07"}},[a("p",[e._v("[93] T. Virtanen, “Monaural sound source separation by nonnegative\nmatrix factorization with temporal continuity and sparseness criteria,”\n"),a("em",[e._v("IEEE Transactions on Audio, Speech, and Language Processing")]),e._v(", vol. 15,\nno. 3, pp. 1066–1074, Mar. 2007.")])]),a("div",{attrs:{id:"ref-fevotte09"}},[a("p",[e._v("[94] C. Févotte, “Nonnegative matrix factorization with the\nItakura-Saito divergence: With application to music analysis,” "),a("em",[e._v("Neural\nComputation")]),e._v(", vol. 21, no. 3, pp. 793–830, Mar. 2009.")])]),a("div",{attrs:{id:"ref-common94"}},[a("p",[e._v("[95] P. Common, “Independent component analysis, a new concept?”\n"),a("em",[e._v("Signal Processing")]),e._v(", vol. 36, no. 3, pp. 287–314, Apr. 1994.")])]),a("div",{attrs:{id:"ref-vembu05"}},[a("p",[e._v("[96] S. Vembu and S. Baumann, “Separation of vocals from polyphonic\naudio recordings,” in "),a("em",[e._v("6th international conference on music information\nretrieval")]),e._v(", 2005.")])]),a("div",{attrs:{id:"ref-hermansky90"}},[a("p",[e._v("[97] H. Hermansky, “Perceptual linear predictive (PLP) analysis of\nspeech,” "),a("em",[e._v("Journal of the Acoustical Society of America")]),e._v(", vol. 87, no. 4,\npp. 1738–1752, Apr. 1990.")])]),a("div",{attrs:{id:"ref-nwe04"}},[a("p",[e._v("[98] T. L. Nwe and Y. Wang, “Automatic detection of vocal segments in\npopular songs,” in "),a("em",[e._v("5th international conference for music information\nretrieval")]),e._v(", 2004.")])]),a("div",{attrs:{id:"ref-casey00"}},[a("p",[e._v("[99] M. A. Casey and A. Westner, “Separation of mixed audio sources by\nindependent subspace analysis,” in "),a("em",[e._v("International computer music\nconference")]),e._v(", 2000.")])]),a("div",{attrs:{id:"ref-chanrungutai08"}},[a("p",[e._v("[100] A. Chanrungutai and C. A. Ratanamahatana, “Singing voice\nseparation for mono-channel music using non-negative matrix\nfactorization,” in "),a("em",[e._v("International conference on advanced technologies\nfor communications")]),e._v(", 2008.")])]),a("div",{attrs:{id:"ref-chanrungutai082"}},[a("p",[e._v("[101] A. Chanrungutai and C. A. Ratanamahatana, “Singing voice\nseparation in mono-channel music,” in "),a("em",[e._v("International symposium on\ncommunications and information technologies")]),e._v(", 2008.")])]),a("div",{attrs:{id:"ref-tikhonov63"}},[a("p",[e._v("[102] A. N. Tikhonov, “Solution of incorrectly formulated problems and\nthe regularization method,” "),a("em",[e._v("Soviet Mathematics")]),e._v(", vol. 4, pp. 1035–1038,\n1963.")])]),a("div",{attrs:{id:"ref-marxer122"}},[a("p",[e._v("[103] R. Marxer and J. Janer, “A Tikhonov regularization method for\nspectrum decomposition in low latency audio source separation,” in "),a("em",[e._v("IEEE\ninternational conference on acoustics, speech and signal processing")]),e._v(",\n2012.")])]),a("div",{attrs:{id:"ref-yang14"}},[a("p",[e._v("[104] P.-K. Yang, C.-C. Hsu, and J.-T. Chien, “Bayesian singing-voice\nseparation,” in "),a("em",[e._v("15th international society for music information\nretrieval conference")]),e._v(", 2014.")])]),a("div",{attrs:{id:"ref-chien15"}},[a("p",[e._v("[105] J.-T. Chien and P.-K. Yang, “Bayesian factorization and learning\nfor monaural source separation,” "),a("em",[e._v("IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing")]),e._v(", vol. 24, no. 1, pp. 185–195, Jan.\n2015.")])]),a("div",{attrs:{id:"ref-cemgil09"}},[a("p",[e._v("[106] A. T. Cemgil, “Bayesian inference for nonnegative matrix\nfactorisation models,” "),a("em",[e._v("Computational Intelligence and Neuroscience")]),e._v(",\nvol. 2009, no. 4, pp. 1–17, Jan. 2009.")])]),a("div",{attrs:{id:"ref-schmidt09"}},[a("p",[e._v("[107] M. N. Schmidt, O. Winther, and L. K. Hansen, “Bayesian\nnon-negative matrix factorization,” in "),a("em",[e._v("8th international conference on\nindependent component analysis and signal separation")]),e._v(", 2009.")])]),a("div",{attrs:{id:"ref-spiertz09"}},[a("p",[e._v("[108] M. Spiertz and V. Gnann, “Source-filter based clustering for\nmonaural blind source separation,” in "),a("em",[e._v("12th international conference on\ndigital audio effects")]),e._v(", 2009.")])]),a("div",{attrs:{id:"ref-smaragdis09"}},[a("p",[e._v("[109] P. Smaragdis and G. J. Mysore, “Separation by ‘humming’:\nUser-guided sound extraction from monophonic mixtures,” in "),a("em",[e._v("IEEE\nworkshop on applications of signal processing to audio and acoustics")]),e._v(",\n2009.")])]),a("div",{attrs:{id:"ref-smaragdis07"}},[a("p",[e._v("[110] P. Smaragdis, B. Raj, and M. Shashanka, “Supervised and\nsemi-supervised separation of sounds from single-channel mixtures,” in\n"),a("em",[e._v("7th international conference on independent component analysis and\nsignal separation")]),e._v(", 2007.")])]),a("div",{attrs:{id:"ref-nakamuray15"}},[a("p",[e._v("[111] T. Nakamuray and H. Kameoka, “(L_p)-norm non-negative matrix\nfactorization and its application to singing voice enhancement,” in\n"),a("em",[e._v("IEEE international conference on acoustics, speech and signal\nprocessing")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-ortega70"}},[a("p",[e._v("[112] J. M. Ortega and W. C. Rheinboldt, "),a("em",[e._v("Iterative solution of\nnonlinear equations in several variables")]),e._v(". Academic Press, 1970.")])]),a("div",{attrs:{id:"ref-kameoka06"}},[a("p",[e._v("[113] H. Kameoka, M. Goto, and S. Sagayama, “Selective amplifier of\nperiodic and non-periodic components in concurrent audio signals with\nspectral control envelopes,” Information Processing Society of Japan,\n2006.")])]),a("div",{attrs:{id:"ref-candes11"}},[a("p",[e._v("[114] E. J. Candès, X. Li, Y. Ma, and J. Wright, “Robust principal\ncomponent analysis?” "),a("em",[e._v("Journal of the ACM")]),e._v(", vol. 58, no. 3, pp. 1–37, May\n2011.")])]),a("div",{attrs:{id:"ref-huang12"}},[a("p",[e._v("[115] P.-S. Huang, S. D. Chen, P. Smaragdis, and M. Hasegawa-Johnson,\n“Singing-voice separation from monaural recordings using robust\nprincipal component analysis,” in "),a("em",[e._v("IEEE international conference on\nacoustics, speech and signal processing")]),e._v(", 2012.")])]),a("div",{attrs:{id:"ref-sprechmann12"}},[a("p",[e._v("[116] P. Sprechmann, A. Bronstein, and G. Sapiro, “Real-time online\nsinging voice separation from monaural recordings using robust low-rank\nmodeling,” in "),a("em",[e._v("13th international society for music information\nretrieval conference")]),e._v(", 2012.")])]),a("div",{attrs:{id:"ref-recht10"}},[a("p",[e._v("[117] B. Recht, M. Fazel, and P. A. Parrilo, “Guaranteed minimum-rank\nsolutions of linear matrix equations via nuclear norm minimization,”\n"),a("em",[e._v("SIAM Review")]),e._v(", vol. 52, no. 3, pp. 471–501, Aug. 2010.")])]),a("div",{attrs:{id:"ref-recht13"}},[a("p",[e._v("[118] B. Recht and C. Ré, “Parallel stochastic gradient algorithms for\nlarge-scale matrix completion,” "),a("em",[e._v("Mathematical Programming Computation")]),e._v(",\nvol. 5, no. 2, pp. 201–226, Jun. 2013.")])]),a("div",{attrs:{id:"ref-gregor10"}},[a("p",[e._v("[119] K. Gregor and Y. LeCun, “Learning fast approximations of sparse\ncoding,” in "),a("em",[e._v("27th international conference on machine learning")]),e._v(", 2010.")])]),a("div",{attrs:{id:"ref-zhang11"}},[a("p",[e._v("[120] L. Zhang, Z. Chen, M. Zheng, and X. He, “Robust non-negative\nmatrix factorization,” "),a("em",[e._v("Frontiers of Electrical Electronic Engineering\nChina")]),e._v(", vol. 6, no. 2, pp. 192–200, Jun. 2011.")])]),a("div",{attrs:{id:"ref-jeong14"}},[a("p",[e._v("[121] I.-Y. Jeong and K. Lee, “Vocal separation using extended robust\nprincipal component analysis with Schatten (P)/(L_p)-norm and scale\ncompression,” in "),a("em",[e._v("International workshop on machine learning for signal\nprocessing")]),e._v(", 2014.")])]),a("div",{attrs:{id:"ref-nie152"}},[a("p",[e._v("[122] F. Nie, H. Wang, and H. Huang, “Joint Schatten (p)-norm and\n(l_p)-norm robust matrix completion for missing value recovery,”\n"),a("em",[e._v("Knowledge and Information Systems")]),e._v(", vol. 42, no. 3, pp. 525–544, Mar.\n2015.")])]),a("div",{attrs:{id:"ref-yang13"}},[a("p",[e._v("[123] Y.-H. Yang, “Low-rank representation of both singing voice and\nmusic accompaniment via learned dictionaries,” in "),a("em",[e._v("14th international\nsociety for music information retrieval conference")]),e._v(", 2013.")])]),a("div",{attrs:{id:"ref-mairal09"}},[a("p",[e._v("[124] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online dictionary\nlearning for sparse coding,” in "),a("em",[e._v("26th annual international conference on\nmachine learning")]),e._v(", 2009.")])]),a("div",{attrs:{id:"ref-chan16"}},[a("p",[e._v("[125] T.-S. T. Chan and Y.-H. Yang, “Complex and quaternionic\nprincipal component pursuit and its application to audio separation,”\n"),a("em",[e._v("IEEE Signal Processing Letters")]),e._v(", vol. 23, no. 2, pp. 287–291, Feb.\n2016.")])]),a("div",{attrs:{id:"ref-peeters03"}},[a("p",[e._v('[126] G. Peeters, “Deriving musical structures from signal analysis\nfor music audio summary generation: "Sequence" and "state" approach,” in\n'),a("em",[e._v("International symposium on computer music multidisciplinary research")]),e._v(",\n2003.")])]),a("div",{attrs:{id:"ref-dannenberg08"}},[a("p",[e._v("[127] R. B. Dannenberg and M. Goto, “Music structure analysis from\nacoustic signals,” in "),a("em",[e._v("Handbook of signal processing in acoustics")]),e._v(",\nSpringer New York, 2008, pp. 305–331.")])]),a("div",{attrs:{id:"ref-paulus10"}},[a("p",[e._v("[128] J. Paulus, M. Müller, and A. Klapuri, “Audio-based music\nstructure analysis,” in "),a("em",[e._v("11th international society for music\ninformation retrieval conference")]),e._v(", 2010.")])]),a("div",{attrs:{id:"ref-rafii11"}},[a("p",[e._v("[129] Z. Rafii and B. Pardo, “A simple music/voice separation system\nbased on the extraction of the repeating musical structure,” in "),a("em",[e._v("IEEE\ninternational conference on acoustics, speech and signal processing")]),e._v(",\n2011.")])]),a("div",{attrs:{id:"ref-rafii13"}},[a("p",[e._v("[130] Z. Rafii and B. Pardo, “REpeating Pattern Extraction Technique\n(REPET): A simple method for music/voice separation,” "),a("em",[e._v("IEEE Transactions\non Audio, Speech, and Language Processing")]),e._v(", vol. 21, no. 1, pp. 73–84,\nJan. 2013.")])]),a("div",{attrs:{id:"ref-rafii14"}},[a("p",[e._v("[131] Z. Rafii, A. Liutkus, and B. Pardo, “REPET for\nbackground/foreground separation in audio,” in "),a("em",[e._v("Blind source\nseparation")]),e._v(", Springer Berlin Heidelberg, 2014, pp. 395–411.")])]),a("div",{attrs:{id:"ref-foote01"}},[a("p",[e._v("[132] J. Foote and S. Uchihashi, “The beat spectrum: A new approach to\nrhythm analysis,” in "),a("em",[e._v("IEEE international conference on multimedia and\nexpo")]),e._v(", 2001.")])]),a("div",{attrs:{id:"ref-seetharaman17"}},[a("p",[e._v("[133] P. Seetharaman, F. Pishdadian, and B. Pardo, “Music/voice\nseparation using the 2D Fourier transform,” in "),a("em",[e._v("IEEE workshop on\napplications of signal processing to audio and acoustics")]),e._v(", 2017.")])]),a("div",{attrs:{id:"ref-liutkus12"}},[a("p",[e._v("[134] A. Liutkus, Z. Rafii, R. Badeau, B. Pardo, and G. Richard,\n“Adaptive filtering for music/voice separation exploiting the\nrepeating musical structure,” in "),a("em",[e._v("IEEE international conference on\nacoustics, speech and signal processing")]),e._v(", 2012.")])]),a("div",{attrs:{id:"ref-rafii12"}},[a("p",[e._v("[135] Z. Rafii and B. Pardo, “Music/voice separation using the\nsimilarity matrix,” in "),a("em",[e._v("13th international society for music information\nretrieval conference")]),e._v(", 2012.")])]),a("div",{attrs:{id:"ref-foote99"}},[a("p",[e._v("[136] J. Foote, “Visualizing music and audio using self-similarity,”\nin "),a("em",[e._v("7th acm international conference on multimedia")]),e._v(", 1999.")])]),a("div",{attrs:{id:"ref-rafii133"}},[a("p",[e._v("[137] Z. Rafii and B. Pardo, “Online REPET-SIM for real-time speech\nenhancement,” in "),a("em",[e._v("IEEE international conference on acoustics, speech and\nsignal processing")]),e._v(", 2013.")])]),a("div",{attrs:{id:"ref-rafii15"}},[a("p",[e._v("[138] Z. Rafii, A. Liutkus, and B. Pardo, “A simple user interface\nsystem for recovering patterns repeating in time and frequency in\nmixtures of sounds,” in "),a("em",[e._v("IEEE international conference on acoustics,\nspeech and signal processing")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-fitzgerald12"}},[a("p",[e._v("[139] D. FitzGerald, “Vocal separation using nearest neighbours and\nmedian filtering,” in "),a("em",[e._v("23rd iet irish signals and systems conference")]),e._v(",\n2012.")])]),a("div",{attrs:{id:"ref-liutkus14"}},[a("p",[e._v("[140] A. Liutkus, Z. Rafii, B. Pardo, D. FitzGerald, and L. Daudet,\n“Kernel spectrogram models for source separation,” in "),a("em",[e._v("4th joint\nworkshop on hands-free speech communication microphone arrays")]),e._v(", 2014.")])]),a("div",{attrs:{id:"ref-liutkus142"}},[a("p",[e._v("[141] A. Liutkus, D. FitzGerald, Z. Rafii, B. Pardo, and L. Daudet,\n“Kernel additive models for source separation,” "),a("em",[e._v("IEEE Transactions on\nSignal Processing")]),e._v(", vol. 62, no. 16, pp. 4298–4310, Aug. 2014.")])]),a("div",{attrs:{id:"ref-liutkus15"}},[a("p",[e._v("[142] A. Liutkus, D. FitzGerald, and Z. Rafii, “Scalable audio\nseparation with light kernel additive modelling,” in "),a("em",[e._v("IEEE international\nconference on acoustics, speech and signal processing")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-pratzlich15"}},[a("p",[e._v("[143] T. Prätzlich, R. Bittner, A. Liutkus, and M. Müller, “Kernel\nadditive modeling for interference reduction in multi-channel music\nrecordings,” in "),a("em",[e._v("IEEE international conference on acoustics, speech and\nsignal processing")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-fanoyela17"}},[a("p",[e._v("[144] D. F. Yela, S. Ewert, D. FitzGerald, and M. Sandler,\n“Interference reduction in music recordings combining kernel additive\nmodelling and non-negative matrix factorization,” in "),a("em",[e._v("IEEE international\nconference on acoustics, speech and signal processing")]),e._v(", 2017.")])]),a("div",{attrs:{id:"ref-moussallam12"}},[a("p",[e._v("[145] M. Moussallam, G. Richard, and L. Daudet, “Audio source\nseparation informed by redundancy with greedy multiscale\ndecompositions,” in "),a("em",[e._v("20th european signal processing conference")]),e._v(",\n2012.")])]),a("div",{attrs:{id:"ref-mallat93"}},[a("p",[e._v("[146] S. G. Mallat and Z. Zhang, “Matching pursuits with\ntime-frequency dictionaries,” "),a("em",[e._v("IEEE Transactions on Signal Processing")]),e._v(",\nvol. 41, no. 12, pp. 3397–3415, Dec. 1993.")])]),a("div",{attrs:{id:"ref-deif152"}},[a("p",[e._v("[147] H. Deif, D. FitzGerald, W. Wang, and L. Gan, “Separation of\nvocals from monaural music recordings using diagonal median filters and\npractical time-frequency parameters,” in "),a("em",[e._v("IEEE international symposium\non signal processing and information technology")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-fitzgerald102"}},[a("p",[e._v("[148] D. FitzGerald and M. Gainza, “Single channel vocal separation\nusing median filtering and factorisation techniques,” "),a("em",[e._v("ISAST\nTransactions on Electronic and Signal Processing")]),e._v(", vol. 4, no. 1, pp.\n62–73, Jan. 2010.")])]),a("div",{attrs:{id:"ref-lee152"}},[a("p",[e._v("[149] J.-Y. Lee and H.-G. Kim, “Music and voice separation using\nlog-spectral amplitude estimator based on kernel spectrogram models\nbackfitting,” "),a("em",[e._v("Journal of the Acoustical Society of Korea")]),e._v(", vol. 34, no.\n3, pp. 227–233, 2015.")])]),a("div",{attrs:{id:"ref-lee15"}},[a("p",[e._v("[150] J.-Y. Lee, H.-S. Cho, and H.-G. Kim, “Vocal separation from\nmonaural music using adaptive auditory filtering based on kernel\nback-fitting,” in "),a("em",[e._v("Interspeech")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-cho15"}},[a("p",[e._v("[151] H.-S. Cho, J.-Y. Lee, and H.-G. Kim, “Singing voice separation\nfrom monaural music based on kernel back-fitting using beta-order\nspectral amplitude estimation,” in "),a("em",[e._v("16th international society for music\ninformation retrieval conference")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-kim16"}},[a("p",[e._v("[152] H.-G. Kim and J. Y. Kim, “Music/voice separation based on kernel\nback-fitting using weighted (\\beta)-order MMSE estimation,” "),a("em",[e._v("ETRI\nJournal")]),e._v(", vol. 38, no. 3, pp. 510–517, Jun. 2016.")])]),a("div",{attrs:{id:"ref-plourde08"}},[a("p",[e._v("[153] E. Plourde and B. Champagne, “Auditory-based spectral amplitude\nestimators for speech enhancement,” "),a("em",[e._v("IEEE Transactions on Audio, Speech,\nand Language Processing")]),e._v(", vol. 16, no. 8, pp. 1614–1623, Nov. 2008.")])]),a("div",{attrs:{id:"ref-raj07"}},[a("p",[e._v("[154] B. Raj, P. Smaragdis, M. Shashanka, and R. Singh, “Separating a\nforeground singer from background music,” in "),a("em",[e._v("International symposium on\nfrontiers of research on speech and music")]),e._v(", 2007.")])]),a("div",{attrs:{id:"ref-smaragdis06"}},[a("p",[e._v("[155] P. Smaragdis and B. Raj, “Shift-invariant probabilistic latent\ncomponent analysis,” MERL, 2006.")])]),a("div",{attrs:{id:"ref-raj05"}},[a("p",[e._v("[156] B. Raj and P. Smaragdis, “Latent variable decomposition of\nspectrograms for single channel speaker separation,” in "),a("em",[e._v("IEEE workshop\non applications of signal processing to audio and acoustics")]),e._v(", 2005.")])]),a("div",{attrs:{id:"ref-han11"}},[a("p",[e._v("[157] J. Han and C.-W. Chen, “Improving melody extraction using\nprobabilistic latent component analysis,” in "),a("em",[e._v("IEEE international\nconference on acoustics, speech and signal processing")]),e._v(", 2011.")])]),a("div",{attrs:{id:"ref-boersma93"}},[a("p",[e._v("[158] P. Boersma, “Accurate short-term analysis of the fundamental\nfrequency and the harmonics-to-noise ratio of a sampled sound,” in "),a("em",[e._v("IFA\nproceedings 17")]),e._v(", 1993.")])]),a("div",{attrs:{id:"ref-gomez12"}},[a("p",[e._v("[159] E. Gómez, F. J. C. Quesada, J. Salamon, J. Bonada, P. V. Candea,\nand P. C. Molero, “Predominant fundamental frequency estimation vs\nsinging voice separation for the automatic transcription of accompanied\nflamenco singing,” in "),a("em",[e._v("13th international society for music information\nretrieval conference")]),e._v(", 2012.")])]),a("div",{attrs:{id:"ref-ono08"}},[a("p",[e._v("[160] N. Ono, K. Miyamoto, J. L. Roux, H. Kameoka, and S. Sagayama,\n“Separation of a monaural audio signal into harmonic/percussive\ncomponents by complementary diffusion on spectrogram,” in "),a("em",[e._v("16th european\nsignal processing conference")]),e._v(", 2008.")])]),a("div",{attrs:{id:"ref-papadopoulos14"}},[a("p",[e._v("[161] H. Papadopoulos and D. P. Ellis, “Music-content-adaptive robust\nprincipal component analysis for a semantically consistent separation of\nforeground and background in music audio signals,” in "),a("em",[e._v("17th\ninternational conference on digital audio effects")]),e._v(", 2014.")])]),a("div",{attrs:{id:"ref-chan15"}},[a("p",[e._v("[162] T.-S. Chan "),a("em",[e._v("et al.")]),e._v(", “Vocal activity informed singing voice\nseparation with the iKala dataset,” in "),a("em",[e._v("IEEE international conference on\nacoustics, speech and signal processing")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-jeong17"}},[a("p",[e._v("[163] I.-Y. Jeong and K. Lee, “Singing voice separation using RPCA\nwith weighted (l_1)-norm,” in "),a("em",[e._v("13th international conference on latent\nvariable analysis and signal separation")]),e._v(", 2017.")])]),a("div",{attrs:{id:"ref-virtanen08"}},[a("p",[e._v("[164] T. Virtanen, A. Mesaros, and M. Ryynänen, “Combining pitch-based\ninference and non-negative spectrogram factorization in separating\nvocals from polyphonic music,” in "),a("em",[e._v("ISCA tutorial and research workshop\non statistical and perceptual audition")]),e._v(", 2008.")])]),a("div",{attrs:{id:"ref-wang11"}},[a("p",[e._v("[165] Y. Wang and Z. Ou, “Combining HMM-based melody extraction and\nNMF-based soft masking for separating voice and accompaniment from\nmonaural audio,” in "),a("em",[e._v("IEEE international conference on acoustics, speech\nand signal processing")]),e._v(", 2011.")])]),a("div",{attrs:{id:"ref-klapuri06"}},[a("p",[e._v("[166] A. Klapuri, “Multiple fundamental frequency estimation by\nsumming harmonic amplitudes,” in "),a("em",[e._v("7th international conference on music\ninformation retrieval")]),e._v(", 2006.")])]),a("div",{attrs:{id:"ref-hsu09"}},[a("p",[e._v("[167] C.-L. Hsu, L.-Y. Chen, J.-S. R. Jang, and H.-J. Li, “Singing\npitch extraction from monaural polyphonic songs by contextual audio\nmodeling and singing harmonic enhancement,” in "),a("em",[e._v("10th international\nsociety for music information retrieval conference")]),e._v(", 2009.")])]),a("div",{attrs:{id:"ref-rafii142"}},[a("p",[e._v("[168] Z. Rafii, Z. Duan, and B. Pardo, “Combining rhythm-based and\npitch-based methods for background and melody separation,” "),a("em",[e._v("IEEE/ACM\nTransactions on Audio, Speech, and Language Processing")]),e._v(", vol. 22, no.\n12, pp. 1884–1893, Sep. 2014.")])]),a("div",{attrs:{id:"ref-duan10"}},[a("p",[e._v("[169] Z. Duan and B. Pardo, “Multiple fundamental frequency estimation\nby modeling spectral peaks and non-peak regions,” "),a("em",[e._v("IEEE Transactions on\nAudio, Speech, and Language Processing")]),e._v(", vol. 18, no. 8, pp. 2121–2133,\nNov. 2010.")])]),a("div",{attrs:{id:"ref-venkataramani14"}},[a("p",[e._v("[170] S. Venkataramani, N. Nayak, P. Rao, and R. Velmurugan, “Vocal\nseparation using singer-vowel priors obtained from polyphonic audio,” in\n"),a("em",[e._v("15th international society for music information retrieval conference")]),e._v(",\n2014.")])]),a("div",{attrs:{id:"ref-rao10"}},[a("p",[e._v("[171] V. Rao and P. Rao, “Vocal melody extraction in the presence of\npitched accompaniment in polyphonic music,” "),a("em",[e._v("IEEE Transactions on Audio,\nSpeech, and Language Processing")]),e._v(", vol. 18, no. 8, pp. 2145–2154, Nov.\n2010.")])]),a("div",{attrs:{id:"ref-rao11"}},[a("p",[e._v("[172] V. Rao, C. Gupta, and P. Rao, “Context-aware features for\nsinging voice detection in polyphonic music,” in "),a("em",[e._v("International workshop\non adaptive multimedia retrieval")]),e._v(", 2011.")])]),a("div",{attrs:{id:"ref-kim112"}},[a("p",[e._v("[173] M. Kim, J. Yoo, K. Kang, and S. Choi, “Nonnegative matrix\npartial co-factorization for spectral and temporal drum source\nseparation,” "),a("em",[e._v("IEEE Journal of Selected Topics in Signal Processing")]),e._v(",\nvol. 5, no. 6, pp. 1192–1204, Oct. 2011.")])]),a("div",{attrs:{id:"ref-zhou14"}},[a("p",[e._v("[174] L. Zhang, Z. Chen, M. Zheng, and X. He, “Nonnegative matrix and\ntensor factorizations: An algorithmic perspective,” "),a("em",[e._v("IEEE Signal\nProcessing Magazine")]),e._v(", vol. 31, no. 3, pp. 54–65, May 2014.")])]),a("div",{attrs:{id:"ref-ikemiya15"}},[a("p",[e._v("[175] Y. Ikemiya, K. Yoshii, and K. Itoyama, “Singing voice analysis\nand editing based on mutually dependent F0 estimation and source\nseparation,” in "),a("em",[e._v("IEEE international conference on acoustics, speech and\nsignal processing")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-ikemiya16"}},[a("p",[e._v("[176] Y. Ikemiya, K. Itoyama, and K. Yoshii, “Singing voice separation\nand vocal F0 estimation based on mutual combination of robust principal\ncomponent analysis and subharmonic summation,” "),a("em",[e._v("IEEE/ACM Transactions on\nAudio, Speech, and Language Processing")]),e._v(", vol. 24, no. 11, pp. 2084–2095,\nNov. 2016.")])]),a("div",{attrs:{id:"ref-hermes88"}},[a("p",[e._v("[177] D. J. Hermes, “Measurement of pitch by subharmonic summation,”\n"),a("em",[e._v("Journal of the Acoustical Society of America")]),e._v(", vol. 83, no. 1, pp.\n257–264, Jan. 1988.")])]),a("div",{attrs:{id:"ref-dobashi15"}},[a("p",[e._v("[178] A. Dobashi, Y. Ikemiya, K. Itoyama, and K. Yoshii, “A music\nperformance assistance system based on vocal, harmonic, and percussive\nsource separation and content visualization for music audio signals,” in\n"),a("em",[e._v("12th sound and music computing conference")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-hu15"}},[a("p",[e._v("[179] Y. Hu and G. Liu, “Separation of singing voice using nonnegative\nmatrix partial co-factorization for singer identification,” "),a("em",[e._v("IEEE\nTransactions on Audio, Speech, and Language Processing")]),e._v(", vol. 23, no. 4,\npp. 643–653, Apr. 2015.")])]),a("div",{attrs:{id:"ref-yoo10"}},[a("p",[e._v("[180] J. Yoo, M. Kim, K. Kang, and S. Choi, “Nonnegative matrix\npartial co-factorization for drum source separation,” in "),a("em",[e._v("IEEE\ninternational conference on acoustics, speech and signal processing")]),e._v(",\n2010.")])]),a("div",{attrs:{id:"ref-boersma01"}},[a("p",[e._v("[181] P. Boersma, “PRAAT, a system for doing phonetics by computer,”\n"),a("em",[e._v("Glot International")]),e._v(", vol. 5, no. 9/10, pp. 341–347, Dec. 2001.")])]),a("div",{attrs:{id:"ref-li09"}},[a("p",[e._v("[182] Y. Li, J. Woodruff, and D. Wang, “Monaural musical sound\nseparation based on pitch and common amplitude modulation,” "),a("em",[e._v("IEEE\nTransactions on Audio, Speech, and Language Processing")]),e._v(", vol. 17, no. 7,\npp. 1361–1371, Sep. 2009.")])]),a("div",{attrs:{id:"ref-raj04"}},[a("p",[e._v("[183] B. Raj, M. L. Seltzer, and R. M. Stern, “Reconstruction of\nmissing features for robust speech recognition,” "),a("em",[e._v("Speech Communication")]),e._v(",\nvol. 43, no. 4, pp. 275–296, Sep. 2004.")])]),a("div",{attrs:{id:"ref-hu16"}},[a("p",[e._v("[184] Y. Hu and G. Liu, “Monaural singing voice separation by\nnon-negative matrix partial co-factorization with temporal continuity\nand sparsity criteria,” in "),a("em",[e._v("12th international conference on intelligent\ncomputing")]),e._v(", 2016.")])]),a("div",{attrs:{id:"ref-zhang15"}},[a("p",[e._v("[185] X. Zhang, W. Li, and B. Zhu, “Latent time-frequency component\nanalysis: A novel pitch-based approach for singing voice separation,” in\n"),a("em",[e._v("IEEE international conference on acoustics, speech and signal\nprocessing")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-decheveigne02"}},[a("p",[e._v("[186] A. de Cheveigné and H. Kawahara, “YIN, a fundamental frequency\nestimator for speech and music,” "),a("em",[e._v("Journal of the Acoustical Society of\nAmerica")]),e._v(", vol. 111, no. 4, pp. 1917–1930, Apr. 2002.")])]),a("div",{attrs:{id:"ref-zhu15"}},[a("p",[e._v("[187] B. Zhu, W. Li, and L. Li, “Towards solving the bottleneck of\npitch-based singing voice separation,” in "),a("em",[e._v("23rd acm international\nconference on multimedia")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-durrieu08"}},[a("p",[e._v("[188] J.-L. Durrieu, G. Richard, and B. David, “Singer melody\nextraction in polyphonic signals using source separation methods,” in\n"),a("em",[e._v("IEEE international conference on acoustics, speech and signal\nprocessing")]),e._v(", 2008.")])]),a("div",{attrs:{id:"ref-durrieu09"}},[a("p",[e._v("[189] J.-L. Durrieu, G. Richard, and B. David, “An iterative approach\nto monaural musical mixture de-soloing,” in "),a("em",[e._v("IEEE international\nconference on acoustics, speech and signal processing")]),e._v(", 2009.")])]),a("div",{attrs:{id:"ref-durrieu10"}},[a("p",[e._v("[190] J.-L. Durrieu, G. Richard, B. David, and C. Févotte,\n“Source/filter model for unsupervised main melody extraction from\npolyphonic audio signals,” "),a("em",[e._v("IEEE Transactions on Audio, Speech, and\nLanguage Processing")]),e._v(", vol. 18, no. 3, pp. 564–575, Mar. 2010.")])]),a("div",{attrs:{id:"ref-ozerov07"}},[a("p",[e._v("[191] A. Ozerov, P. Philippe, F. Bimbot, and R. Gribonval, “Adaptation\nof Bayesian models for single-channel source separation and its\napplication to voice/music separation in popular songs,” "),a("em",[e._v("IEEE\nTransactions on Audio, Speech, and Language Processing")]),e._v(", vol. 15, no. 5,\npp. 1564–1578, Jul. 2007.")])]),a("div",{attrs:{id:"ref-klatt90"}},[a("p",[e._v("[192] D. H. Klatt and L. C. Klatt, “Analysis, synthesis, and\nperception of voice quality variations among female and male talkers,”\n"),a("em",[e._v("Journal of the Acoustical Society of America")]),e._v(", vol. 87, no. 2, pp.\n820–857, Feb. 1990.")])]),a("div",{attrs:{id:"ref-benaroya032"}},[a("p",[e._v("[193] L. Benaroya, L. Mcdonagh, F. Bimbot, and R. Gribonval, “Non\nnegative sparse representation for Wiener based source separation with a\nsingle sensor,” in "),a("em",[e._v("IEEE international conference on acoustics, speech\nand signal processing")]),e._v(", 2003.")])]),a("div",{attrs:{id:"ref-dhillon05"}},[a("p",[e._v("[194] I. S. Dhillon and S. Sra, “Generalized nonnegative matrix\napproximations with Bregman divergences,” in "),a("em",[e._v("Advances in neural\ninformation processing systems 18")]),e._v(", MIT Press, 2005, pp. 283–290.")])]),a("div",{attrs:{id:"ref-benaroya06"}},[a("p",[e._v("[195] L. Benaroya, F. Bimbot, and R. Gribonval, “Audio source\nseparation with a single sensor,” "),a("em",[e._v("IEEE Transactions on Audio, Speech,\nand Language Processing")]),e._v(", vol. 14, no. 1, pp. 191–199, Jan. 2006.")])]),a("div",{attrs:{id:"ref-durrieu12"}},[a("p",[e._v("[196] J.-L. Durrieu and J.-P. Thiran, “Musical audio source separation\nbased on user-selected F0 track,” in "),a("em",[e._v("10th international conference on\nlatent variable analysis and signal separation")]),e._v(", 2012.")])]),a("div",{attrs:{id:"ref-fuentes2012"}},[a("p",[e._v("[197] B. Fuentes, R. Badeau, and G. Richard, “Blind harmonic adaptive\ndecomposition applied to supervised source separation,” in "),a("em",[e._v("Signal\nprocessing conference (eusipco), 2012 proceedings of the 20th european")]),e._v(",\n2012, pp. 2654–2658.")])]),a("div",{attrs:{id:"ref-brown91"}},[a("p",[e._v("[198] J. C. Brown, “Calculation of a constant Q spectral transform,”\n"),a("em",[e._v("Journal of the Acoustical Society of America")]),e._v(", vol. 89, no. 1, pp.\n425–434, Jan. 1991.")])]),a("div",{attrs:{id:"ref-brown92"}},[a("p",[e._v("[199] J. C. Brown and M. S. Puckette, “An efficient algorithm for the\ncalculation of a constant Q transform,” "),a("em",[e._v("Journal of the Acoustical\nSociety of America")]),e._v(", vol. 92, no. 5, pp. 2698–2701, Nov. 1992.")])]),a("div",{attrs:{id:"ref-schorkhuber10"}},[a("p",[e._v("[200] C. Schörkhuber and A. Klapuri, “Constant-Q transform toolbox,”\nin "),a("em",[e._v("7th sound and music computing conference")]),e._v(", 2010.")])]),a("div",{attrs:{id:"ref-durrieu11"}},[a("p",[e._v("[201] J.-L. Durrieu, B. David, and G. Richard, “A musically motivated\nmid-level representation for pitch estimation and musical audio source\nseparation,” "),a("em",[e._v("IEEE Journal of Selected Topics in Signal Processing")]),e._v(",\nvol. 5, no. 6, pp. 1180–1191, Oct. 2011.")])]),a("div",{attrs:{id:"ref-joder12"}},[a("p",[e._v("[202] C. Joder and B. Schuller, “Score-informed leading voice\nseparation from monaural audio,” in "),a("em",[e._v("13th international society for\nmusic information retrieval conference")]),e._v(", 2012.")])]),a("div",{attrs:{id:"ref-joder11"}},[a("p",[e._v("[203] C. Joder, S. Essid, and G. Richard, “A conditional random field\nframework for robust and scalable audio-to-score matching,” "),a("em",[e._v("IEEE\nTransactions on Audio, Speech, and Language Processing")]),e._v(", vol. 19, no. 8,\npp. 2385–2397, Nov. 2011.")])]),a("div",{attrs:{id:"ref-zhao14"}},[a("p",[e._v("[204] R. Zhao, S. Lee, D.-Y. Huang, and M. Dong, “Soft constrained\nleading voice separation with music score guidance,” in "),a("em",[e._v("9th\ninternational symposium on chinese spoken language")]),e._v(", 2014.")])]),a("div",{attrs:{id:"ref-durrieu092"}},[a("p",[e._v("[205] J.-L. Durrieu, A. Ozerov, C. Févotte, G. Richard, and B. David,\n“Main instrument separation from stereophonic audio signals using a\nsource/filter model,” in "),a("em",[e._v("17th european signal processing conference")]),e._v(",\n2009.")])]),a("div",{attrs:{id:"ref-janer13"}},[a("p",[e._v("[206] J. Janer and R. Marxer, “Separation of unvoiced fricatives in\nsinging voice mixtures with semi-supervised NMF,” in "),a("em",[e._v("16th international\nconference on digital audio effects")]),e._v(", 2013.")])]),a("div",{attrs:{id:"ref-janer12"}},[a("p",[e._v("[207] J. Janer, R. Marxer, and K. Arimoto, “Combining a harmonic-based\nNMF decomposition with transient analysis for instantaneous percussion\nseparation,” in "),a("em",[e._v("IEEE international conference on acoustics, speech and\nsignal processing")]),e._v(", 2012.")])]),a("div",{attrs:{id:"ref-marxer13"}},[a("p",[e._v("[208] R. Marxer and J. Janer, “Modelling and separation of singing\nvoice breathiness in polyphonic mixtures,” in "),a("em",[e._v("16th international\nconference on digital audio effects")]),e._v(", 2013.")])]),a("div",{attrs:{id:"ref-degottex11"}},[a("p",[e._v("[209] G. Degottex, A. Roebel, and X. Rodet, “Pitch transposition and\nbreathiness modification using a glottal source model and its adapted\nvocal-tract filter,” in "),a("em",[e._v("IEEE international conference on acoustics,\nspeech and signal processing")]),e._v(", 2011.")])]),a("div",{attrs:{id:"ref-ozerov102"}},[a("p",[e._v("[210] A. Ozerov, E. Vincent, and F. Bimbot, “A general modular\nframework for audio source separation,” in "),a("em",[e._v("9th international conference\non latent variable analysis and signal separation")]),e._v(", 2010.")])]),a("div",{attrs:{id:"ref-ozerov12"}},[a("p",[e._v("[211] A. Ozerov, E. Vincent, and F. Bimbot, “A general flexible\nframework for the handling of prior information in audio source\nseparation,” "),a("em",[e._v("IEEE Transactions on Audio, Speech, and Language\nProcessing")]),e._v(", vol. 20, no. 4, pp. 1118–1133, May 2012.")])]),a("div",{attrs:{id:"ref-salaun14"}},[a("p",[e._v("[212] Y. Salaün "),a("em",[e._v("et al.")]),e._v(", “The flexible audio source separation\ntoolbox version 2.0,” in "),a("em",[e._v("IEEE international conference on acoustics,\nspeech and signal processing")]),e._v(", 2014.")])]),a("div",{attrs:{id:"ref-hennequin16"}},[a("p",[e._v("[213] R. Hennequin and F. Rigaud, “Long-term reverberation modeling\nfor under-determined audio source separation with application to vocal\nmelody extraction,” in "),a("em",[e._v("17th international society for music information\nretrieval conference")]),e._v(", 2016.")])]),a("div",{attrs:{id:"ref-singh10"}},[a("p",[e._v("[214] R. Singh, B. Raj, and P. Smaragdis, “Latent-variable\ndecomposition based dereverberation of monaural and multi-channel\nsignals,” in "),a("em",[e._v("IEEE international conference on acoustics, speech and\nsignal processing")]),e._v(", 2010.")])]),a("div",{attrs:{id:"ref-ono082"}},[a("p",[e._v("[215] N. Ono, K. Miyamoto, H. Kameoka, and S. Sagayama, “A real-time\nequalizer of harmonic and percussive components in music signals,” in\n"),a("em",[e._v("9th international conference on music information retrieval")]),e._v(", 2008.")])]),a("div",{attrs:{id:"ref-fitzgerald10"}},[a("p",[e._v("[216] D. FitzGerald, “Harmonic/percussive separation using median\nfiltering,” in "),a("em",[e._v("13th international conference on digital audio effects")]),e._v(",\n2010.")])]),a("div",{attrs:{id:"ref-yang12"}},[a("p",[e._v("[217] Y.-H. Yang, “On sparse and low-rank matrix decomposition for\nsinging voice separation,” in "),a("em",[e._v("20th acm international conference on\nmultimedia")]),e._v(", 2012.")])]),a("div",{attrs:{id:"ref-jeong142"}},[a("p",[e._v("[218] I.-Y. Jeong and K. Lee, “Vocal separation from monaural music\nusing temporal/spectral continuity and sparsity constraints,” "),a("em",[e._v("IEEE\nSignal Processing Letters")]),e._v(", vol. 21, no. 10, pp. 1197–1200, Jun. 2014.")])]),a("div",{attrs:{id:"ref-ochiai15"}},[a("p",[e._v("[219] E. Ochiai, T. Fujisawa, and M. Ikehara, “Vocal separation by\nconstrained non-negative matrix factorization,” in "),a("em",[e._v("Asia-pacific signal\nand information processing association annual summit and conference")]),e._v(",\n2015.")])]),a("div",{attrs:{id:"ref-watanabe16"}},[a("p",[e._v("[220] T. Watanabe, T. Fujisawa, and M. Ikehara, “Vocal separation\nusing improved robust principal component analysis and post-processing,”\nin "),a("em",[e._v("IEEE 59th international midwest symposium on circuits and systems")]),e._v(",\n2016.")])]),a("div",{attrs:{id:"ref-raguet13"}},[a("p",[e._v("[221] H. Raguet, J. Fadili, and and Gabriel Peyré, “A generalized\nforward-backward splitting,” "),a("em",[e._v("SIAM Journal on Imaging Sciences")]),e._v(", vol. 6,\nno. 3, pp. 1199–1226, Jul. 2013.")])]),a("div",{attrs:{id:"ref-hayashi16"}},[a("p",[e._v("[222] A. Hayashi, H. Kameoka, T. Matsubayashi, and H. Sawada,\n“Non-negative periodic component analysis for music source\nseparation,” in "),a("em",[e._v("Asia-pacific signal and information processing\nassociation annual summit and conference")]),e._v(", 2016.")])]),a("div",{attrs:{id:"ref-fitzgerald09"}},[a("p",[e._v("[223] D. FitzGerald, M. Cranitch, and E. Coyle, “Using tensor\nfactorisation models to separate drums from polyphonic music,” in "),a("em",[e._v("12th\ninternational conference on digital audio effects")]),e._v(", 2009.")])]),a("div",{attrs:{id:"ref-tachibana14"}},[a("p",[e._v("[224] H. Tachibana, N. Ono, and S. Sagayama, “Singing voice\nenhancement in monaural music signals based on two-stage\nharmonic/percussive sound separation on multiple resolution\nspectrograms,” "),a("em",[e._v("IEEE/ACM Transactions on Audio, Speech and Language\nProcessing")]),e._v(", vol. 22, no. 1, pp. 228–237, Jan. 2014.")])]),a("div",{attrs:{id:"ref-tachibana10"}},[a("p",[e._v("[225] H. Tachibana, T. Ono, N. Ono, and S. Sagayama, “Melody line\nestimation in homophonic music audio signals based on\ntemporal-variability of melodic source,” in "),a("em",[e._v("IEEE international\nconference on acoustics, speech and signal processing")]),e._v(", 2010.")])]),a("div",{attrs:{id:"ref-tachibana16"}},[a("p",[e._v("[226] H. Tachibana, N. Ono, and S. Sagayama, “A real-time\naudio-to-audio karaoke generation system for monaural recordings based\non singing voice suppression and key conversion techniques,” "),a("em",[e._v("Journal of\nInformation Processing")]),e._v(", vol. 24, no. 3, pp. 470–482, May 2016.")])]),a("div",{attrs:{id:"ref-ono10"}},[a("p",[e._v("[227] N. Ono "),a("em",[e._v("et al.")]),e._v(", “Harmonic and percussive sound separation and\nits application to MIR-related tasks,” in "),a("em",[e._v("Advances in music information\nretrieval")]),e._v(", Springer Berlin Heidelberg, 2010, pp. 213–236.")])]),a("div",{attrs:{id:"ref-tachibana12"}},[a("p",[e._v("[228] H. Tachibana, H. Kameoka, N. Ono, and S. Sagayama, “Comparative\nevaluations of multiple harmonic/percussive sound separation techniques\nbased on anisotropic smoothness of spectrogram,” in "),a("em",[e._v("IEEE international\nconference on acoustics, speech and signal processing")]),e._v(", 2012.")])]),a("div",{attrs:{id:"ref-deif15"}},[a("p",[e._v("[229] H. Deif, W. Wang, L. Gan, and S. Alhashmi, “A local\ndiscontinuity based approach for monaural singing voice separation from\naccompanying music with multi-stage non-negative matrix factorization,”\nin "),a("em",[e._v("IEEE global conference on signal and information processing")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-zhu13"}},[a("p",[e._v("[230] B. Zhu, W. Li, R. Li, and X. Xue, “Multi-stage non-negative\nmatrix factorization for monaural singing voice separation,” "),a("em",[e._v("IEEE\nTransactions on Audio, Speech, and Language Processing")]),e._v(", vol. 21, no.\n10, pp. 2096–2107, Oct. 2013.")])]),a("div",{attrs:{id:"ref-driedger15"}},[a("p",[e._v("[231] J. Driedger and M. Müller, “Extracting singing voice from music\nrecordings by cascading audio decomposition techniques,” in "),a("em",[e._v("IEEE\ninternational conference on acoustics, speech and signal processing")]),e._v(",\n2015.")])]),a("div",{attrs:{id:"ref-driedger14"}},[a("p",[e._v("[232] J. Driedger, M. Müller, and S. Disch, “Extending\nharmonic-percussive separation of audio signals,” in "),a("em",[e._v("15th international\nsociety for music information retrieval conference")]),e._v(", 2014.")])]),a("div",{attrs:{id:"ref-talmon11"}},[a("p",[e._v("[233] R. Talmon, I. Cohen, and S. Gannot, “Transient noise reduction\nusing nonlocal diffusion filters,” "),a("em",[e._v("IEEE/ACM Transactions on Audio,\nSpeech and Language Processing")]),e._v(", vol. 19, no. 6, pp. 1584–1599, Aug.\n2011.")])]),a("div",{attrs:{id:"ref-hsu12"}},[a("p",[e._v("[234] C.-L. Hsu, D. Wang, J.-S. R. Jang, and K. Hu, “A tandem\nalgorithm for singing pitch extraction and voice separation from music\naccompaniment,” "),a("em",[e._v("IEEE Transactions on Audio, Speech, and Language\nProcessing")]),e._v(", vol. 20, no. 5, pp. 1482–1491, Jul. 2012.")])]),a("div",{attrs:{id:"ref-hu10"}},[a("p",[e._v("[235] G. Hu and D. Wang, “A tandem algorithm for pitch estimation and\nvoiced speech segregation,” "),a("em",[e._v("IEEE Transactions on Audio, Speech, and\nLanguage Processing")]),e._v(", vol. 18, no. 8, pp. 2067–2079, Nov. 2010.")])]),a("div",{attrs:{id:"ref-rumelhart86"}},[a("p",[e._v("[236] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning\ninternal representations by error propagation,” in "),a("em",[e._v("Parallel distributed\nprocessing: Explorations in the microstructure of cognition, vol. 1")]),e._v(",\nMIT Press Cambridge, 1986, pp. 318–362.")])]),a("div",{attrs:{id:"ref-bryan13"}},[a("p",[e._v("[237] N. J. Bryan and G. J. Mysore, “Interactive user-feedback for\nsound source separation,” in "),a("em",[e._v("International conference on intelligent\nuser-interfaces, workshop on interactive machine learning")]),e._v(", 2013.")])]),a("div",{attrs:{id:"ref-bryan132"}},[a("p",[e._v("[238] N. J. Bryan and G. J. Mysore, “An efficient posterior\nregularized latent variable model for interactive sound source\nseparation,” in "),a("em",[e._v("30th international conference on machine learning")]),e._v(",\n2013.")])]),a("div",{attrs:{id:"ref-bryan133"}},[a("p",[e._v("[239] N. J. Bryan and G. J. Mysore, “Interactive refinement of\nsupervised and semi-supervised sound source separation estimates,” in\n"),a("em",[e._v("IEEE international conference on acoustics, speech, and signal\nprocessing")]),e._v(", 2013.")])]),a("div",{attrs:{id:"ref-ganchev10"}},[a("p",[e._v("[240] K. Ganchev, J. Graça, J. Gillenwater, and B. Taskar, “Posterior\nregularization for structured latent variable models,” "),a("em",[e._v("Journal of\nMachine Learning Research")]),e._v(", vol. 11, pp. 2001–2049, Mar. 2010.")])]),a("div",{attrs:{id:"ref-ozerov13"}},[a("p",[e._v("[241] A. Ozerov, N. Duong, and L. Chevallier, “Weighted nonnegative\ntensor factorization: On monotonicity of multiplicative update rules and\napplication to user-guided audio source separation,” Technicolor, 2013.")])]),a("div",{attrs:{id:"ref-jaureguiberry13"}},[a("p",[e._v("[242] X. Jaureguiberry, G. Richard, P. Leveau, R. Hennequin, and E.\nVincent, “Introducing a simple fusion framework for audio source\nseparation,” in "),a("em",[e._v("IEEE international workshop on machine learning for\nsignal processing")]),e._v(", 2013.")])]),a("div",{attrs:{id:"ref-jaureguiberry14"}},[a("p",[e._v("[243] X. Jaureguiberry, E. Vincent, and G. Richard, “Variational\nBayesian model averaging for audio source separation,” in "),a("em",[e._v("IEEE workshop\non statistical signal processing workshop")]),e._v(", 2014.")])]),a("div",{attrs:{id:"ref-jaureguiberry16"}},[a("p",[e._v("[244] X. Jaureguiberry, E. Vincent, and G. Richard, “Fusion methods\nfor speech enhancement and audio source separation,” "),a("em",[e._v("IEEE/ACM\nTransactions on Audio, Speech, and Language Processing")]),e._v(", vol. 24, no. 7,\npp. 1266–1279, Jul. 2016.")])]),a("div",{attrs:{id:"ref-hoeting99"}},[a("p",[e._v("[245] J. A. Hoeting, D. Madigan, A. E. Raftery, and C. T. Volinsky,\n“Bayesian model averaging: A tutorial,” "),a("em",[e._v("Statistical Science")]),e._v(", vol.\n14, no. 4, pp. 382–417, Nov. 1999.")])]),a("div",{attrs:{id:"ref-mcvicar16"}},[a("p",[e._v("[246] M. McVicar, R. Santos-Rodriguez, and T. D. Bie, “Learning to\nseparate vocals from polyphonic mixtures via ensemble methods and\nstructured output prediction,” in "),a("em",[e._v("IEEE international conference on\nacoustics, speech and signal processing")]),e._v(", 2016.")])]),a("div",{attrs:{id:"ref-jain90"}},[a("p",[e._v("[247] A. K. Jain and F. Farrokhnia, “Unsupervised texture segmentation\nusing Gabor filters,” in "),a("em",[e._v("IEEE international conference on systems, man\nand cybernetics")]),e._v(", 1990.")])]),a("div",{attrs:{id:"ref-huang14"}},[a("p",[e._v("[248] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,\n“Singing-voice separation from monaural recordings using deep\nrecurrent neural networks,” in "),a("em",[e._v("15th international society for music\ninformation retrieval conference")]),e._v(", 2014.")])]),a("div",{attrs:{id:"ref-lacoste-julien13"}},[a("p",[e._v("[249] S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher,\n“Block-coordinate Frank-Wolfe optimization for structural SVMs,” in\n"),a("em",[e._v("30th international conference on machine learning")]),e._v(", 2013.")])]),a("div",{attrs:{id:"ref-manilow17"}},[a("p",[e._v("[250] E. Manilow, P. Seetharaman, F. Pishdadian, and B. Pardo,\n“Predicting algorithm efficacy for adaptive, multi-cue source\nseparation,” in "),a("em",[e._v("IEEE workshop on applications of signal processing to\naudio and acoustics")]),e._v(", 2017.")])]),a("div",{attrs:{id:"ref-wolf14"}},[a("p",[e._v("[251] G. Wolf, S. Mallat, and S. Shamma, “Audio source separation with\ntime-frequency velocities,” in "),a("em",[e._v("IEEE international workshop on machine\nlearning for signal processing")]),e._v(", 2014.")])]),a("div",{attrs:{id:"ref-wolf16"}},[a("p",[e._v("[252] G. Wolf, S. Mallat, and S. Shamma, “Rigid motion model for audio\nsource separation,” "),a("em",[e._v("IEEE Transactions on Signal Processing")]),e._v(", vol. 64,\nno. 7, pp. 1822–1831, Apr. 2016.")])]),a("div",{attrs:{id:"ref-anden14"}},[a("p",[e._v("[253] J. Andén and S. Mallat, “Deep scattering spectrum,” "),a("em",[e._v("IEEE\nTransactions on Signal Processing")]),e._v(", vol. 62, no. 16, pp. 4114–4128, Aug.\n2014.")])]),a("div",{attrs:{id:"ref-bernard01"}},[a("p",[e._v("[254] C. P. Bernard, “Discrete wavelet analysis for fast optic flow\ncomputation,” "),a("em",[e._v("Applied and Computational Harmonic Analysis")]),e._v(", vol. 11,\nno. 1, pp. 32–63, Jul. 2001.")])]),a("div",{attrs:{id:"ref-yen14"}},[a("p",[e._v("[255] F. Yen, Y.-J. Luo, and T.-S. Chi, “Singing voice separation\nusing spectro-temporal modulation features,” in "),a("em",[e._v("15th international\nsociety for music information retrieval conference")]),e._v(", 2014.")])]),a("div",{attrs:{id:"ref-yen15"}},[a("p",[e._v("[256] F. Yen, M.-C. Huang, and T.-S. Chi, “A two-stage singing voice\nseparation algorithm using spectro-temporal modulation features,” in\n"),a("em",[e._v("Interspeech")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-chi05"}},[a("p",[e._v("[257] T. Chi, P. Rub, and S. A. Shamma, “Multiresolution\nspectrotemporal analysis of complex sounds,” "),a("em",[e._v("Journal of the Acoustical\nSociety of America")]),e._v(", vol. 118, no. 2, pp. 887–906, Aug. 2005.")])]),a("div",{attrs:{id:"ref-chi99"}},[a("p",[e._v("[258] T. Chi, Y. Gao, M. C. Guyton, P. Ru, and S. Shamma,\n“Spectro-temporal modulation transfer functions and speech\nintelligibility,” "),a("em",[e._v("Journal of the Acoustical Society of America")]),e._v(", vol.\n106, no. 5, pp. 2719–2732, Nov. 1999.")])]),a("div",{attrs:{id:"ref-chan17"}},[a("p",[e._v("[259] T.-S. T. Chan and Y.-H. Yang, “Informed group-sparse\nrepresentation for singing voice separation,” "),a("em",[e._v("IEEE Signal Processing\nLetters")]),e._v(", vol. 24, no. 2, pp. 156–160, Feb. 2017.")])]),a("div",{attrs:{id:"ref-yuan06"}},[a("p",[e._v("[260] M. Yuan and Y. Lin, “Model selection and estimation in\nregression with grouped variables,” "),a("em",[e._v("Journal of the Royal Statistical\nSociety Series B")]),e._v(", vol. 68, no. 1, pp. 49–67, Dec. 2006.")])]),a("div",{attrs:{id:"ref-ma16"}},[a("p",[e._v("[261] S. Ma, “Alternating proximal gradient method for convex\nminimization,” "),a("em",[e._v("Journal of Scientific Computing")]),e._v(", vol. 68, no. 2, pp.\n546–572, Aug. 2016.")])]),a("div",{attrs:{id:"ref-liu13"}},[a("p",[e._v("[262] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, “Robust\nrecovery of subspace structures by low-rank representation,” "),a("em",[e._v("IEEE\nTransactions on Pattern Analysis and Machine Intelligence")]),e._v(", vol. 35, no.\n1, pp. 171–184, Jan. 2007.")])]),a("div",{attrs:{id:"ref-varga93"}},[a("p",[e._v("[263] A. Varga and H. J. Steeneken, “Assessment for automatic speech\nrecognition: II. NOISEX-92: A database and an experiment to study the\neffect of additive noise on speech recognition systems,” "),a("em",[e._v("Speech\nCommunication")]),e._v(", vol. 12, no. 3, pp. 247–251, Jul. 1993.")])]),a("div",{attrs:{id:"ref-garofolo93"}},[a("p",[e._v("[264] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, and D.\nS. Pallett, “DARPA TIMIT acoustic-phonetic continuous speech corpus\nCD-ROM. NIST speech disc 1-1.1,” "),a("em",[e._v("NASA STI/Recon technical report n")]),e._v(".\n1993.")])]),a("div",{attrs:{id:"ref-sturmel12"}},[a("p",[e._v("[265] N. Sturmel "),a("em",[e._v("et al.")]),e._v(", “Linear mixing models for active listening\nof music productions in realistic studio conditions,” in "),a("em",[e._v("132nd aes\nconvention")]),e._v(", 2012.")])]),a("div",{attrs:{id:"ref-MTGMASSdb"}},[a("p",[e._v("[266] M. Vinyes, “MTG MASS database.” 2008.")])]),a("div",{attrs:{id:"ref-vincent09"}},[a("p",[e._v("[267] E. Vincent, S. Araki, and P. Bofill, “The 2008 signal separation\nevaluation campaign: A community-based approach to large-scale\nevaluation,” in "),a("em",[e._v("8th international conference on independent component\nanalysis and signal separation")]),e._v(", 2009.")])]),a("div",{attrs:{id:"ref-araki10"}},[a("p",[e._v("[268] S. Araki "),a("em",[e._v("et al.")]),e._v(", “The 2010 signal separation evaluation\ncampaign (SiSEC2010): - audio source separation -,” in "),a("em",[e._v("9th\ninternational conference on latent variable analysis and signal\nseparation")]),e._v(", 2010.")])]),a("div",{attrs:{id:"ref-araki12"}},[a("p",[e._v("[269] S. Araki "),a("em",[e._v("et al.")]),e._v(", “The 2011 signal separation evaluation\ncampaign (SiSEC2011): - audio source separation -,” in "),a("em",[e._v("10th\ninternational conference on latent variable analysis and signal\nseparation")]),e._v(", 2012.")])]),a("div",{attrs:{id:"ref-vincent12"}},[a("p",[e._v("[270] E. Vincent "),a("em",[e._v("et al.")]),e._v(", “The signal separation evaluation campaign\n(2007-2010): Achievements and remaining challenges,” "),a("em",[e._v("Signal\nProcessing")]),e._v(", vol. 92, no. 8, pp. 1928–1936, Aug. 2012.")])]),a("div",{attrs:{id:"ref-ono15"}},[a("p",[e._v("[271] N. Ono, Z. Rafii, D. Kitamura, N. Ito, and A. Liutkus, “The 2015\nsignal separation evaluation campaign,” in "),a("em",[e._v("12th international\nconference on latent variable analysis and signal separation")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-liutkus17"}},[a("p",[e._v("[272] A. Liutkus "),a("em",[e._v("et al.")]),e._v(", “The 2016 signal separation evaluation\ncampaign,” in "),a("em",[e._v("13th international conference on latent variable analysis\nand signal separation")]),e._v(", 2017.")])]),a("div",{attrs:{id:"ref-liutkus11"}},[a("p",[e._v("[273] A. Liutkus, R. Badeau, and G. Richard, “Gaussian processes for\nunderdetermined source separation,” "),a("em",[e._v("IEEE Transactions on Audio, Speech,\nand Language Processing")]),e._v(", vol. 59, no. 7, pp. 3155–3167, Feb. 2011.")])]),a("div",{attrs:{id:"ref-bittner14"}},[a("p",[e._v("[274] R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam, and and\nJuan P. Bello, “MedleyDB: A multitrack dataset for annotation-intensive\nmir research,” in "),a("em",[e._v("15th international society for music information\nretrieval conference")]),e._v(", 2014.")])]),a("div",{attrs:{id:"ref-rafii17"}},[a("p",[e._v("[275] Z. Rafii, A. Liutkus, F.-R. Stöter, S. I. Mimilakis, and R.\nBittner, “MUSDB18, a dataset for audio source separation.” Dec-2017.")])]),a("div",{attrs:{id:"ref-ozerov05"}},[a("p",[e._v("[276] A. Ozerov, P. Philippe, R. Gribonval, and F. Bimbot, “One\nmicrophone singing voice separation using source-adapted models,” in\n"),a("em",[e._v("IEEE workshop on applications of signal processing to audio and\nacoustics")]),e._v(", 2005.")])]),a("div",{attrs:{id:"ref-tsai04"}},[a("p",[e._v("[277] W.-H. Tsai, D. Rogers, and H.-M. Wang, “Blind clustering of\npopular music recordings based on singer voice characteristics,”\n"),a("em",[e._v("Computer Music Journal")]),e._v(", vol. 28, no. 3, pp. 68–78, 2004.")])]),a("div",{attrs:{id:"ref-gauvain94"}},[a("p",[e._v("[278] J.-L. Gauvain and C.-H. Lee, “Maximum a posteriori estimation\nfor multivariate Gaussian mixture observations of Markov chains,” "),a("em",[e._v("IEEE\nTransactions on Audio, Speech, and Language Processing")]),e._v(", vol. 2, no. 2,\npp. 291–298, Apr. 1994.")])]),a("div",{attrs:{id:"ref-vincent10"}},[a("p",[e._v("[279] E. Vincent, M. Jafari, S. Abdallah, M. Plumbley, and M. Davies,\n“Probabilistic modeling paradigms for audio source separation,” in\n"),a("em",[e._v("Machine audition: Principles, algorithms and systems")]),e._v(", IGI Global,\n2010, pp. 162–185.")])]),a("div",{attrs:{id:"ref-rafii132"}},[a("p",[e._v("[280] Z. Rafii, D. L. Sun, F. G. Germain, and G. J. Mysore, “Combining\nmodeling of singing voice and background music for automatic separation\nof musical mixtures,” in "),a("em",[e._v("14th international society for music\ninformation retrieval conference")]),e._v(", 2013.")])]),a("div",{attrs:{id:"ref-boulanger-lewandowski14"}},[a("p",[e._v("[281] N. Boulanger-Lewandowski, G. J. Mysore, and M. Hoffman,\n“Exploiting long-term temporal dependencies in NMF using recurrent\nneural networks with application to source separation,” in "),a("em",[e._v("IEEE\ninternational conference on acoustics, speech and signal processing")]),e._v(",\n2014.")])]),a("div",{attrs:{id:"ref-mysore10"}},[a("p",[e._v("[282] G. J. Mysore, P. Smaragdis, and B. Raj, “Non-negative hidden\nMarkov modeling of audio with application to source separation,” in "),a("em",[e._v("9th\ninternational conference on latent variable analysis and signal\nseparation")]),e._v(", 2010.")])]),a("div",{attrs:{id:"ref-qian17"}},[a("p",[e._v("[283] K. Qian, Y. Zhang, S. Chang, X. Yang, D. Florêncio, and M.\nHasegawa-Johnson, “Speech enhancement using bayesian wavenet,” "),a("em",[e._v("Proc.\nInterspeech 2017")]),e._v(", pp. 2013–2017, 2017.")])]),a("div",{attrs:{id:"ref-deng14"}},[a("p",[e._v("[284] L. Deng and D. Yu, “Deep learning: Methods and applications,”\n"),a("em",[e._v("Foundations and Trends in Signal Processing")]),e._v(", vol. 7, nos. 3-4, pp.\n197–387, Jun. 2014.")])]),a("div",{attrs:{id:"ref-lecun15"}},[a("p",[e._v("[285] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” "),a("em",[e._v("Nature")]),e._v(",\nvol. 521, pp. 436–444, May 2015.")])]),a("div",{attrs:{id:"ref-goodfellow16"}},[a("p",[e._v("[286] I. Goodfellow, Y. Bengio, and A. Courville, "),a("em",[e._v("Deep learning")]),e._v(". MIT\nPress, 2016.")])]),a("div",{attrs:{id:"ref-robbins51"}},[a("p",[e._v("[287] H. Robbins and S. Monro, “A stochastic approximation method,”\n"),a("em",[e._v("Annals of Mathematical Statistics")]),e._v(", vol. 22, no. 3, pp. 400–407, Sep.\n1951.")])]),a("div",{attrs:{id:"ref-rumelhart862"}},[a("p",[e._v("[288] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning\nrepresentations by back-propagating errors,” "),a("em",[e._v("Nature")]),e._v(", vol. 323, pp.\n533–536, Oct. 1986.")])]),a("div",{attrs:{id:"ref-hermans13"}},[a("p",[e._v("[289] M. Hermans and B. Schrauwen, “Training and analysing deep\nrecurrent neural networks,” in "),a("em",[e._v("26th international conference on neural\ninformation processing systems")]),e._v(", 2013.")])]),a("div",{attrs:{id:"ref-pascanu14"}},[a("p",[e._v("[290] R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio, “How to\nconstruct deep recurrent neural networks,” in "),a("em",[e._v("International conference\non learning representations")]),e._v(", 2014.")])]),a("div",{attrs:{id:"ref-huang15"}},[a("p",[e._v("[291] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,\n“Joint optimization of masks and deep recurrent neural networks for\nmonaural source separation,” "),a("em",[e._v("IEEE/ACM Transactions on Audio, Speech,\nand Language Processing")]),e._v(", vol. 23, 2015.")])]),a("div",{attrs:{id:"ref-huang142"}},[a("p",[e._v("[292] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,\n“Deep learning for monaural speech separation,” in "),a("em",[e._v("IEEE international\nconference on acoustics, speech and signal processing")]),e._v(", 2014.")])]),a("div",{attrs:{id:"ref-uhlich15"}},[a("p",[e._v("[293] S. Uhlich, F. Giron, and Y. Mitsufuji, “Deep neural network\nbased instrument extraction from music,” in "),a("em",[e._v("IEEE international\nconference on acoustics, speech and signal processing")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-uhlich17"}},[a("p",[e._v("[294] S. Uhlich "),a("em",[e._v("et al.")]),e._v(", “Improving music source separation based on\ndeep neural networks through data augmentation and network blending,” in\n"),a("em",[e._v("IEEE international conference on acoustics, speech and signal\nprocessing")]),e._v(", 2017.")])]),a("div",{attrs:{id:"ref-simpson15"}},[a("p",[e._v("[295] A. J. R. Simpson, G. Roma, and M. D. Plumbley, “Deep karaoke:\nExtracting vocals from musical mixtures using a convolutional deep\nneural network,” in "),a("em",[e._v("12th international conference on latent variable\nanalysis and signal separation")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-schlueter16"}},[a("p",[e._v("[296] J. Schlüter, “Learning to pinpoint singing voice from weakly\nlabeled examples,” in "),a("em",[e._v("17th international society for music information\nretrieval conference")]),e._v(", 2016.")])]),a("div",{attrs:{id:"ref-chandna17"}},[a("p",[e._v("[297] P. Chandna, M. Miron, J. Janer, and E. Gómez, “Monoaural audio\nsource separation using deep convolutional neural networks,” in "),a("em",[e._v("13th\ninternational conference on latent variable analysis and signal\nseparation")]),e._v(", 2017.")])]),a("div",{attrs:{id:"ref-mimilakis16"}},[a("p",[e._v("[298] S. I. Mimilakis, E. Cano, J. Abeßer, and G. Schuller, “New\nsonorities for jazz recordings: Separation and mixing using deep neural\nnetworks,” in "),a("em",[e._v("2nd aes workshop on intelligent music production")]),e._v(", 2016.")])]),a("div",{attrs:{id:"ref-mimilakis17"}},[a("p",[e._v("[299] S. I. Mimilakis, K. Drossos, T. Virtanen, and G. Schuller, “A\nrecurrent encoder-decoder approach with skip-filtering connections for\nmonaural singing voice separation,” in "),a("em",[e._v("IEEE international workshop on\nmachine learning for signal processing")]),e._v(", 2017.")])]),a("div",{attrs:{id:"ref-mimilakis172"}},[a("p",[e._v("[300] S. I. Mimilakis, K. Drossos, J. F. Santos, G. Schuller, T.\nVirtanen, and Y. Bengio, “Monaural singing voice separation with\nskip-filtering connections and recurrent inference of time-frequency\nmask,” in "),a("em",[e._v("IEEE international conference on acoustics, speech and signal\nprocessing")]),e._v(", 2018.")])]),a("div",{attrs:{id:"ref-jansson17"}},[a("p",[e._v("[301] A. Jansson, E. Humphrey, N. Montecchio, R. Bittner, A. Kumar,\nand T. Weyde, “Singing voice separation with deep U-Net convolutional\nnetworks,” in "),a("em",[e._v("18th international society for music information\nretrieval conferenceng")]),e._v(", 2017.")])]),a("div",{attrs:{id:"ref-takahashi17"}},[a("p",[e._v("[302] N. Takahashi and Y. Mitsufuji, “Multi-scale multi-band densenets\nfor audio source separation,” in "),a("em",[e._v("IEEE workshop on applications of\nsignal processing to audio and acoustics")]),e._v(", 2017.")])]),a("div",{attrs:{id:"ref-hershey16"}},[a("p",[e._v("[303] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep\nclustering: Discriminative embeddings for segmentation and separation,”\nin "),a("em",[e._v("IEEE international conference on acoustics, speech and signal\nprocessing")]),e._v(", 2016.")])]),a("div",{attrs:{id:"ref-isik16"}},[a("p",[e._v("[304] Y. Isik, J. L. Roux, Z. Chen, S. Watanabe, and J. R. Hershey,\n“Single-channel multispeaker separation using deep clustering,” in\n"),a("em",[e._v("Interspeech")]),e._v(", 2016.")])]),a("div",{attrs:{id:"ref-luo17"}},[a("p",[e._v("[305] Y. Luo, Z. Chen, J. R. Hershey, J. L. Roux, and N. Mesgarani,\n“Deep clustering and conventional networks for music separation:\nStronger together,” in "),a("em",[e._v("IEEE international conference on acoustics,\nspeech and signal processing")]),e._v(", 2017.")])]),a("div",{attrs:{id:"ref-kim15"}},[a("p",[e._v("[306] M. Kim and P. Smaragdis, “Adaptive denoising autoencoders: A\nfine-tuning scheme to learn from test mixtures,” in "),a("em",[e._v("12th international\nconference on latent variable analysis and signal separation")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-vincentp10"}},[a("p",[e._v("[307] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A.\nManzagol, “Stacked denoising autoencoders: Learning useful\nrepresentations in a deep network with a local denoising criterion,”\n"),a("em",[e._v("Journal of Machine Learning Research")]),e._v(", vol. 11, pp. 3371–3408, Dec.\n2010.")])]),a("div",{attrs:{id:"ref-grais16"}},[a("p",[e._v("[308] E. M. Grais, G. Roma, A. J. R. Simpson, and M. D. Plumbley,\n“Single channel audio source separation using deep neural network\nensembles,” in "),a("em",[e._v("140th aes convention")]),e._v(", 2016.")])]),a("div",{attrs:{id:"ref-grais162"}},[a("p",[e._v("[309] E. M. Grais, G. Roma, A. J. R. Simpson, and M. D. Plumbley,\n“Combining mask estimates for single channel audio source separation\nusing deep neural networks,” in "),a("em",[e._v("Interspeech")]),e._v(", 2016.")])]),a("div",{attrs:{id:"ref-grais17"}},[a("p",[e._v("[310] E. M. Grais, G. Roma, A. J. R. Simpson, and M. D. Plumbley,\n“Discriminative enhancement for single channel audio source separation\nusing deep neural networks,” in "),a("em",[e._v("13th international conference on latent\nvariable analysis and signal separation")]),e._v(", 2017.")])]),a("div",{attrs:{id:"ref-grais172"}},[a("p",[e._v("[311] E. M. Grais, G. Roma, A. J. R. Simpson, and M. D. Plumbley,\n“Two-stage single-channel audio source separation using deep neural\nnetworks,” "),a("em",[e._v("IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing")]),e._v(", vol. 25, no. 9, pp. 1773–1783, Sep. 2017.")])]),a("div",{attrs:{id:"ref-nie15"}},[a("p",[e._v("[312] S. Nie "),a("em",[e._v("et al.")]),e._v(", “Joint optimization of recurrent networks\nexploiting source auto-regression for source separation,” in\n"),a("em",[e._v("Interspeech")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-sebastian16"}},[a("p",[e._v("[313] J. Sebastian and H. A. Murthy, “Group delay based music source\nseparation using deep recurrent neural networks,” in "),a("em",[e._v("International\nconference on signal processing and communications")]),e._v(", 2016.")])]),a("div",{attrs:{id:"ref-yegnanarayana91"}},[a("p",[e._v("[314] B. Yegnanarayana, H. A. Murthy, and V. R. Ramachandran,\n“Processing of noisy speech using modified group delay functions,” in\n"),a("em",[e._v("IEEE international conference on acoustics, speech and signal\nprocessing")]),e._v(", 1991.")])]),a("div",{attrs:{id:"ref-fan16"}},[a("p",[e._v("[315] Z.-C. Fan, J.-S. R. Jang, and C.-L. Lu, “Singing voice\nseparation and pitch extraction from monaural polyphonic audio music via\nDNN and adaptive pitch tracking,” in "),a("em",[e._v("IEEE international conference on\nmultimedia big data")]),e._v(", 2016.")])]),a("div",{attrs:{id:"ref-avendano03"}},[a("p",[e._v("[316] C. Avendano, “Frequency-domain source identification and\nmanipulation in stereo mixes for enhancement, suppression and re-panning\napplications,” in "),a("em",[e._v("IEEE workshop on applications of signal processing to\naudio and acoustics")]),e._v(", 2003.")])]),a("div",{attrs:{id:"ref-avendano02"}},[a("p",[e._v("[317] C. Avendano and J.-M. Jot, “Frequency domain techniques for\nstereo to multichannel upmix,” in "),a("em",[e._v("AES 22nd international conference")]),e._v(",\n2002.")])]),a("div",{attrs:{id:"ref-barry04"}},[a("p",[e._v("[318] D. Barry, B. Lawlor, and E. Coyle, “Sound source separation:\nAzimuth discrimination and resynthesis,” in "),a("em",[e._v("7th international\nconference on digital audio effects")]),e._v(", 2004.")])]),a("div",{attrs:{id:"ref-vinyes06"}},[a("p",[e._v("[319] M. Vinyes, J. Bonada, and A. Loscos, “Demixing commercial music\nproductions via human-assisted time-frequency masking,” in "),a("em",[e._v("120th aes\nconvention")]),e._v(", 2006.")])]),a("div",{attrs:{id:"ref-cobos082"}},[a("p",[e._v("[320] M. Cobos and J. J. López, “Stereo audio source separation based\non time-frequency masking and multilevel thresholding,” "),a("em",[e._v("Digital Signal\nProcessing")]),e._v(", vol. 18, no. 6, pp. 960–976, Nov. 2008.")])]),a("div",{attrs:{id:"ref-yilmaz04"}},[a("p",[e._v("[321] Ö. Yilmaz and S. Rickard, “Blind separation of speech mixtures\nvia time-frequency masking,” "),a("em",[e._v("IEEE Transactions on Signal Processing")]),e._v(",\nvol. 52, no. 7, pp. 1830–1847, Jul. 2004.")])]),a("div",{attrs:{id:"ref-otsu79"}},[a("p",[e._v("[322] N. Otsu, “A threshold selection method from gray-level\nhistograms,” "),a("em",[e._v("IEEE Transactions on Systems, Man, and Cybernetics")]),e._v(", vol.\n9, no. 1, pp. 62–66, Jan. 1979.")])]),a("div",{attrs:{id:"ref-sofianos10"}},[a("p",[e._v("[323] S. Sofianos, A. Ariyaeeinia, and R. Polfreman, “Towards\neffective singing voice extraction from stereophonic recordings,” in\n"),a("em",[e._v("IEEE international conference on acoustics, speech and signal\nprocessing")]),e._v(", 2010.")])]),a("div",{attrs:{id:"ref-sofianos102"}},[a("p",[e._v("[324] S. Sofianos, A. Ariyaeeinia, and R. Polfreman, “Singing voice\nseparation based on non-vocal independent component subtraction,” in\n"),a("em",[e._v("13th international conference on digital audio effects")]),e._v(", 2010.")])]),a("div",{attrs:{id:"ref-sofianos12"}},[a("p",[e._v("[325] S. Sofianos, A. Ariyaeeinia, R. Polfreman, and R. Sotudeh,\n“H-semantics: A hybrid approach to singing voice separation,” "),a("em",[e._v("Journal\nof the Audio Engineering Society")]),e._v(", vol. 60, no. 10, pp. 831–841, Oct.\n2012.")])]),a("div",{attrs:{id:"ref-kim11"}},[a("p",[e._v("[326] M. Kim, S. Beack, K. Choi, and K. Kang, “Gaussian mixture model\nfor singing voice separation from stereophonic music,” in "),a("em",[e._v("AES 43rd\nconference")]),e._v(", 2011.")])]),a("div",{attrs:{id:"ref-cobos08"}},[a("p",[e._v("[327] M. Cobos and J. J. López, “Singing voice separation combining\npanning information and pitch tracking,” in "),a("em",[e._v("AES 124th convention")]),e._v(",\n2008.")])]),a("div",{attrs:{id:"ref-fitzgerald13"}},[a("p",[e._v("[328] D. FitzGerald, “Stereo vocal extraction using ADRess and nearest\nneighbours median filtering,” in "),a("em",[e._v("16th international conference on\ndigital audio effects")]),e._v(", 2013.")])]),a("div",{attrs:{id:"ref-fitzgerald132"}},[a("p",[e._v("[329] D. FitzGerald and R. Jaiswal, “Improved stereo instrumental\ntrack recovery using median nearest-neighbour inpainting,” in "),a("em",[e._v("24th iet\nirish signals and systems conference")]),e._v(", 2013.")])]),a("div",{attrs:{id:"ref-alder12"}},[a("p",[e._v("[330] A. Adler, V. Emiya, M. G. Jafari, M. Elad, R. Gribonval, and M.\nD. Plumbley, “Audio inpainting,” "),a("em",[e._v("IEEE Transactions on Audio, Speech,\nand Language Processing")]),e._v(", vol. 20, no. 3, pp. 922–932, Mar. 2012.")])]),a("div",{attrs:{id:"ref-ozerov09"}},[a("p",[e._v("[331] A. Ozerov and C. Févotte, “Multichannel nonnegative matrix\nfactorization in convolutive mixtures with application to blind audio\nsource separation,” in "),a("em",[e._v("IEEE international conference on acoustics,\nspeech and signal processing")]),e._v(", 2009.")])]),a("div",{attrs:{id:"ref-ozerov10"}},[a("p",[e._v("[332] A. Ozerov and C. Févotte, “Multichannel nonnegative matrix\nfactorization in convolutive mixtures for audio source separation,”\n"),a("em",[e._v("IEEE Transactions on Audio, Speech, and Language Processing")]),e._v(", vol. 18,\nno. 3, pp. 550–563, Mar. 2010.")])]),a("div",{attrs:{id:"ref-ozerov11"}},[a("p",[e._v("[333] A. Ozerov, C. Févotte, R. Blouet, and J.-L. Durrieu,\n“Multichannel nonnegative tensor factorization with structured\nconstraints for user-guided audio source separation,” in "),a("em",[e._v("IEEE\ninternational conference on acoustics, speech and signal processing")]),e._v(",\n2011.")])]),a("div",{attrs:{id:"ref-liutkus10"}},[a("p",[e._v("[334] A. Liutkus, R. Badeau, and G. Richard, “Informed source\nseparation using latent components,” in "),a("em",[e._v("9th international conference on\nlatent variable analysis and signal separation")]),e._v(", 2010.")])]),a("div",{attrs:{id:"ref-fevotte10"}},[a("p",[e._v("[335] C. Févotte and A. Ozerov, “Notes on nonnegative tensor\nfactorization of the spectrogram for audio source separation:\nStatistical insights and towards self-clustering of the spatial cues,”\nin "),a("em",[e._v("7th international symposium on computer music modeling and\nretrieval")]),e._v(", 2010.")])]),a("div",{attrs:{id:"ref-ozerov14"}},[a("p",[e._v("[336] A. Ozerov, N. Duong, and L. Chevallier, “On monotonicity of\nmultiplicative update rules for weighted nonnegative tensor\nfactorization,” in "),a("em",[e._v("International symposium on nonlinear theory and its\napplications")]),e._v(", 2014.")])]),a("div",{attrs:{id:"ref-sawada11"}},[a("p",[e._v("[337] H. Sawada, H. Kameoka, S. Araki, and N. Ueda, “New formulations\nand efficient algorithms for multichannel NMF,” in "),a("em",[e._v("IEEE workshop on\napplications of signal processing to audio and acoustics")]),e._v(", 2011.")])]),a("div",{attrs:{id:"ref-sawada12"}},[a("p",[e._v("[338] H. Sawada, H. Kameoka, S. Araki, and N. Ueda, “Efficient\nalgorithms for multichannel extensions of Itakura-Saito nonnegative\nmatrix factorization,” in "),a("em",[e._v("IEEE international conference on acoustics,\nspeech and signal processing")]),e._v(", 2012.")])]),a("div",{attrs:{id:"ref-sawada13"}},[a("p",[e._v("[339] H. Sawada, H. Kameoka, S. Araki, and N. Ueda, “Multichannel\nextensions of non-negative matrix factorization with complex-valued\ndata,” "),a("em",[e._v("IEEE Transactions on Audio, Speech, and Language Processing")]),e._v(",\nvol. 21, no. 5, pp. 971–982, May 2013.")])]),a("div",{attrs:{id:"ref-sivasankaran15"}},[a("p",[e._v("[340] S. Sivasankaran "),a("em",[e._v("et al.")]),e._v(", “Robust ASR using neural network based\nspeech enhancement and feature simulation,” in "),a("em",[e._v("IEEE automatic speech\nrecognition and understanding workshop")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-nugraha162"}},[a("p",[e._v("[341] A. A. Nugraha, A. Liutkus, and E. Vincent, “Multichannel audio\nsource separation with deep neural networks,” "),a("em",[e._v("IEEE/ACM Transactions on\nAudio, Speech, and Language Processing")]),e._v(", vol. 24, no. 9, pp. 1652–1664,\nSep. 2016.")])]),a("div",{attrs:{id:"ref-nugraha15"}},[a("p",[e._v("[342] A. A. Nugraha, A. Liutkus, and E. Vincent, “Multichannel audio\nsource separation with deep neural networks,” Inria, 2015.")])]),a("div",{attrs:{id:"ref-nugraha16"}},[a("p",[e._v("[343] A. A. Nugraha, A. Liutkus, and E. Vincent, “Multichannel music\nseparation with deep neural networks,” in "),a("em",[e._v("24th european signal\nprocessing conference")]),e._v(", 2016.")])]),a("div",{attrs:{id:"ref-duong10"}},[a("p",[e._v("[344] N. Q. K. Duong, E. Vincent, and R. Gribonval, “Under-determined\nreverberant audio source separation using a full-rank spatial covariance\nmodel,” "),a("em",[e._v("IEEE Transactions on Audio, Speech, and Language Processing")]),e._v(",\nvol. 18, no. 7, pp. 1830–1840, Sep. 2010.")])]),a("div",{attrs:{id:"ref-ozerov112"}},[a("p",[e._v("[345] A. Ozerov, A. Liutkus, R. Badeau, and G. Richard, “Informed\nsource separation: Source coding meets source separation,” in "),a("em",[e._v("IEEE\nworkshop on applications of signal processing to audio and acoustics")]),e._v(",\n2011.")])]),a("div",{attrs:{id:"ref-zwicker13"}},[a("p",[e._v("[346] E. Zwicker and H. Fastl, "),a("em",[e._v("Psychoacoustics: Facts and models")]),e._v(".\nSpringer-Verlag Berlin Heidelberg, 2013.")])]),a("div",{attrs:{id:"ref-rix01"}},[a("p",[e._v("[347] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra,\n“Perceptual evaluation of speech quality (PESQ)-a new method for\nspeech quality assessment of telephone networks and codecs,” in "),a("em",[e._v("IEEE\ninternational conference on acoustics, speech and signal processing")]),e._v(",\n2001.")])]),a("div",{attrs:{id:"ref-wang09"}},[a("p",[e._v("[348] Z. Wang and A. C. Bovik, “Mean squared error: Love it or leave\nit? A new look at signal fidelity measures,” "),a("em",[e._v("IEEE Signal Processing\nMagazine")]),e._v(", vol. 26, no. 1, pp. 98–117, Jan. 2009.")])]),a("div",{attrs:{id:"ref-barker15"}},[a("p",[e._v("[349] J. Barker, R. Marxer, E. Vincent, and S. Watanabe, “The third\n‘CHiME’ speech separation and recognition challenge: Dataset, task and\nbaselines,” in "),a("em",[e._v("IEEE workshop on automatic speech recognition and\nunderstanding")]),e._v(", 2015.")])]),a("div",{attrs:{id:"ref-recommendation2001MUSHRA"}},[a("p",[e._v("[350] I. Recommendation, “Bs. 1534-1. method for the subjective\nassessment of intermediate sound quality (MUSHRA),” "),a("em",[e._v("International\nTelecommunications Union, Geneva")]),e._v(", 2001.")])]),a("div",{attrs:{id:"ref-vincent062"}},[a("p",[e._v("[351] E. Vincent, M. Jafari, and M. Plumbley, “Preliminary guidelines\nfor subjective evaluation of audio source separation algorithms,” in\n"),a("em",[e._v("ICA research network international workshop")]),e._v(", 2006.")])]),a("div",{attrs:{id:"ref-cano11"}},[a("p",[e._v("[352] E. Cano, C. Dittmar, and G. Schuller, “Influence of phase,\nmagnitude and location of harmonic components in the perceived quality\nof extracted solo signals,” in "),a("em",[e._v("AES 42nd conference on semantic audio")]),e._v(",\n2011.")])]),a("div",{attrs:{id:"ref-fevotte05"}},[a("p",[e._v("[353] C. Févotte, R. Gribonval, and E. Vinvent, “BSS_EVAL toolbox\nuser guide - revision 2.0,” IRISA, 2005.")])]),a("div",{attrs:{id:"ref-vincent06"}},[a("p",[e._v("[354] E. Vincent, R. Gribonval, and C. Févotte, “Performance\nmeasurement in blind audio source separation,” "),a("em",[e._v("IEEE Transactions on\nAudio, Speech, and Language Processing")]),e._v(", vol. 14, no. 4, pp. 1462–1469,\nJul. 2006.")])]),a("div",{attrs:{id:"ref-fox07"}},[a("p",[e._v("[355] B. Fox, A. Sabin, B. Pardo, and A. Zopf, “Modeling perceptual\nsimilarity of audio signals for blind source separation evaluation,” in\n"),a("em",[e._v("7th international conference on latent variable analysis and signal\nseparation")]),e._v(", 2007.")])]),a("div",{attrs:{id:"ref-fox072"}},[a("p",[e._v("[356] B. Fox and B. Pardo, “Towards a model of perceived quality of\nblind audio source separation,” in "),a("em",[e._v("IEEE international conference on\nmultimedia and expo")]),e._v(", 2007.")])]),a("div",{attrs:{id:"ref-kornycky08"}},[a("p",[e._v("[357] J. Kornycky, B. Gunel, and A. Kondoz, “Comparison of subjective\nand objective evaluation methods for audio source separation,” "),a("em",[e._v("Journal\nof the Acoustical Society of America")]),e._v(", vol. 4, no. 1, 2008.")])]),a("div",{attrs:{id:"ref-emiya10"}},[a("p",[e._v("[358] V. Emiya, E. Vincent, N. Harlander, and V. Hohmann,\n“Multi-criteria subjective and objective evaluation of audio source\nseparation,” in "),a("em",[e._v("38th international aes conference")]),e._v(", 2010.")])]),a("div",{attrs:{id:"ref-emiya11"}},[a("p",[e._v("[359] V. Emiya, E. Vincent, N. Harlander, and V. Hohmann, “Subjective\nand objective quality assessment of audio source separation,” "),a("em",[e._v("IEEE\nTransactions on Audio, Speech, and Language Processing")]),e._v(", vol. 19, no. 7,\npp. 2046–2057, Sep. 2011.")])]),a("div",{attrs:{id:"ref-vincent122"}},[a("p",[e._v("[360] E. Vincent, “Improved perceptual metrics for the evaluation of\naudio source separation,” in "),a("em",[e._v("10th international conference on latent\nvariable analysis and signal separation")]),e._v(", 2012.")])]),a("div",{attrs:{id:"ref-cartwright16"}},[a("p",[e._v("[361] M. Cartwright, B. Pardo, G. J. Mysore, and M. Hoffman, “Fast and\neasy crowdsourced perceptual audio evaluation,” in "),a("em",[e._v("IEEE international\nconference on acoustics, speech and signal processing")]),e._v(", 2016.")])]),a("div",{attrs:{id:"ref-gupta15"}},[a("p",[e._v("[362] U. Gupta, E. Moore, and A. Lerch, “On the perceptual relevance\nof objective source separation measures for singing voice separation,”\nin "),a("em",[e._v("IEEE workshop on applications of signal processing to audio and\nacoustics")]),e._v(", 2005.")])]),a("div",{attrs:{id:"ref-stoter16"}},[a("p",[e._v("[363] F.-R. Stöter, A. Liutkus, R. Badeau, B. Edler, and P. Magron,\n“Common fate model for unison source separation,” in "),a("em",[e._v("IEEE\ninternational conference on acoustics, speech and signal processing")]),e._v(",\n2016.")])]),a("div",{attrs:{id:"ref-roma16"}},[a("p",[e._v("[364] G. Roma, E. M. Grais, A. J. Simpson, I. Sobieraj, and M. D.\nPlumbley, “Untwist: A new toolbox for audio source separation,” in "),a("em",[e._v("17th\ninternational society on music information retrieval conference")]),e._v(", 2016.")])])])])}],!1,null,null,null);n.default=i.exports}}]);