<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>SigSep | Literature</title>
    <meta name="description" content="Open Resources for Audio Source Separation">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
  <script type="text/javascript" src="https://cdn.rawgit.com/larsgw/citation.js/archive/citation.js/citation-0.3.4.min.js"></script>
    
    <link rel="preload" href="/tmp_website/assets/css/0.styles.baa4e0f7.css" as="style"><link rel="preload" href="/tmp_website/assets/js/app.ff25cdb7.js" as="script"><link rel="preload" href="/tmp_website/assets/js/3.ffd6b151.js" as="script"><link rel="prefetch" href="/tmp_website/assets/js/1.7b84d260.js"><link rel="prefetch" href="/tmp_website/assets/js/2.79b93880.js"><link rel="prefetch" href="/tmp_website/assets/js/4.17488af6.js"><link rel="prefetch" href="/tmp_website/assets/js/5.1d05a4a4.js"><link rel="prefetch" href="/tmp_website/assets/js/6.ff4ee77a.js"><link rel="prefetch" href="/tmp_website/assets/js/7.04902d76.js"><link rel="prefetch" href="/tmp_website/assets/js/8.11545fc7.js">
    <link rel="stylesheet" href="/tmp_website/assets/css/0.styles.baa4e0f7.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div><a href="/tmp_website/" class="home-link router-link-active"><img src="/tmp_website/logo.png" class="logo"><span class="site-name can-hide">
      SigSep
    </span></a><div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""><!----></div><nav class="nav-links can-hide"><div class="nav-item"><a href="/tmp_website/" class="nav-link">Home</a></div><div class="nav-item"><a href="/tmp_website/datasets/" class="nav-link">Datasets</a></div><div class="nav-item"><a href="/tmp_website/software.html" class="nav-link">Software</a></div><div class="nav-item"><a href="/tmp_website/literature/" class="nav-link router-link-exact-active router-link-active">Literature</a></div><a href="https://github.com/sigsep/tmp_website" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></nav></div></header><div class="sidebar-mask"></div><div class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/tmp_website/" class="nav-link">Home</a></div><div class="nav-item"><a href="/tmp_website/datasets/" class="nav-link">Datasets</a></div><div class="nav-item"><a href="/tmp_website/software.html" class="nav-link">Software</a></div><div class="nav-item"><a href="/tmp_website/literature/" class="nav-link router-link-exact-active router-link-active">Literature</a></div><a href="https://github.com/sigsep/tmp_website" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></nav><ul class="sidebar-links"><li><div class="sidebar-group first"><p class="sidebar-heading open"><span>Literature</span><!----></p><ul class="sidebar-group-items"><li><a href="/tmp_website/literature/#modeling-the-lead-signal-harmonicity" class="sidebar-link">Modeling the lead signal: harmonicity</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/tmp_website/literature/#analysis-synthesis-approaches" class="sidebar-link">Analysis-synthesis approaches</a></li><li class="sidebar-sub-header"><a href="/tmp_website/literature/#comb-filtering-approaches" class="sidebar-link">Comb-filtering approaches</a></li><li class="sidebar-sub-header"><a href="/tmp_website/literature/#shortcomings" class="sidebar-link">Shortcomings</a></li></ul></li><li><a href="/tmp_website/literature/#modeling-the-accompaniment-redundancy" class="sidebar-link">Modeling the accompaniment: redundancy</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/tmp_website/literature/#grouping-low-rank-components" class="sidebar-link">Grouping low-rank components</a></li><li class="sidebar-sub-header"><a href="/tmp_website/literature/#low-rank-accompaniment-sparse-vocals" class="sidebar-link">Low-rank accompaniment, sparse vocals</a></li><li class="sidebar-sub-header"><a href="/tmp_website/literature/#repetitions-within-the-accompaniment" class="sidebar-link">Repetitions within the accompaniment</a></li><li class="sidebar-sub-header"><a href="/tmp_website/literature/#shortcomings-2" class="sidebar-link">Shortcomings</a></li></ul></li><li><a href="/tmp_website/literature/#joint-models-for-lead-and-accompaniment" class="sidebar-link">Joint models for lead and accompaniment</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/tmp_website/literature/#using-music-structure-analysis-to-drive-learning" class="sidebar-link">Using music structure analysis to drive learning</a></li><li class="sidebar-sub-header"><a href="/tmp_website/literature/#factorization-with-a-known-melody" class="sidebar-link">Factorization with a known melody</a></li><li class="sidebar-sub-header"><a href="/tmp_website/literature/#joint-factorization-and-melody-estimation" class="sidebar-link">Joint factorization and melody estimation</a></li><li class="sidebar-sub-header"><a href="/tmp_website/literature/#different-constraints-for-different-sources" class="sidebar-link">Different constraints for different sources</a></li><li class="sidebar-sub-header"><a href="/tmp_website/literature/#cascaded-and-iterated-methods" class="sidebar-link">Cascaded and iterated methods</a></li><li class="sidebar-sub-header"><a href="/tmp_website/literature/#source-dependent-representations" class="sidebar-link">Source-dependent representations</a></li><li class="sidebar-sub-header"><a href="/tmp_website/literature/#shortcomings-3" class="sidebar-link">Shortcomings</a></li></ul></li><li><a href="/tmp_website/literature/#data-driven-approaches" class="sidebar-link">Data-driven approaches</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/tmp_website/literature/#algebraic-approaches" class="sidebar-link">Algebraic approaches</a></li><li class="sidebar-sub-header"><a href="/tmp_website/literature/#deep-neural-networks" class="sidebar-link">Deep neural networks</a></li><li class="sidebar-sub-header"><a href="/tmp_website/literature/#shortcomings-4" class="sidebar-link">Shortcomings</a></li></ul></li><li><a href="/tmp_website/literature/#including-multichannel-information" class="sidebar-link">Including multichannel information</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/tmp_website/literature/#extracting-the-lead-based-on-panning" class="sidebar-link">Extracting the lead based on panning</a></li><li class="sidebar-sub-header"><a href="/tmp_website/literature/#augmenting-models-with-stereo" class="sidebar-link">Augmenting models with stereo</a></li><li class="sidebar-sub-header"><a href="/tmp_website/literature/#shortcomings-5" class="sidebar-link">Shortcomings</a></li></ul></li><li><a href="/tmp_website/literature/#references" class="sidebar-link">References</a><ul class="sidebar-sub-headers"></ul></li></ul></div></li></ul></div><div class="page"><div class="content"><h1 id="literature"><a href="#literature" aria-hidden="true" class="header-anchor">#</a> Literature</h1><p>In this article, we describe the basic ideas of existing methods for musical source separation (and specifically Lead/Accompaniment Separation) classified into three main categories: signal processing, audio
modeling and probability theory. The interested reader is strongly
encouraged to delve into the many online courses or textbooks available
for a more detailed presentation of these topics, such
as [<a href="#ref-zolzer11">12</a>], [<a href="#ref-muller2015">13</a>] for signal
processing, [<a href="#ref-loizou13">9</a>] for speech modeling, and
[<a href="#ref-jaynes2003probability">14</a>], [<a href="#ref-cappe2005">15</a>] for
probability theory.</p><div class="tip custom-block"><p class="custom-block-title">CITE</p><p>This article is based on <a href="https://ieeexplore.ieee.org/document/8336997/" target="_blank" rel="noopener noreferrer">a publication in the IEEE Journal of Transactions</a>.
If you want to cite this article, please use the following reference.</p><pre class="language-text"><code>@ARTICLE{rafii18,
  author={Z. Rafii and A. Liutkus and
          F. R. Stöter and S. I. Mimilakis
          and D. FitzGerald and B. Pardo},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  title={An Overview of Lead and Accompaniment Separation in Music},
  year={2018},
  volume={26},
  number={8},
  pages={1307-1335},
  doi={10.1109/TASLP.2018.2825440},
  ISSN={2329-9290},
  month={Aug}
}

</code></pre></div><h3 id="signal-processing"><a href="#signal-processing" aria-hidden="true" class="header-anchor">#</a> Signal processing</h3><p>Sound is a series of pressure waves in the air. It is recorded as a
<em>waveform</em>, a time-series of measurements of the displacement of the
microphone diaphragm in response to these pressure waves. Sound is
reproduced if a loudspeaker diaphragm is moved according to the recorded
waveform. Multichannel signals simply consist of several waveforms,
captured by more than one microphone. Typically, music signals are
stereophonic, containing two waveforms.</p><p>Microphone displacement is typically measured at a fixed  <em>sampling
frequency</em>. In music processing, it is common to have sampling
frequencies of (44.1) kHz (the sample frequency on a compact disc) or
(48) kHz, which are higher than the typical sampling rates of
(16) kHz or (8) kHz used for speech in telephony. This is because
musical signals contain much higher frequency content than speech and
the goal is aesthetic beauty in addition to basic intelligibility.</p><p>A time-frequency (TF) representation of sound is a matrix that encodes
the time-varying <em>spectrum</em> of the waveform. Its entries are called
TF <em>bins</em> and encode the varying spectrum of the waveform for all time
frames and frequency channels. The most commonly-used TF representation
is the short time Fourier transform (STFT) [<a href="#ref-mcaulay86">16</a>],
which has complex entries: the angle accounts for the phase, i.e., the
actual shift of the corresponding sinusoid at that time bin and
frequency bin, and the magnitude accounts for the amplitude of that
sinusoid in the signal. The magnitude (or power) of the STFT is called
<em>spectrogram</em>. When the mixture is multichannel, the TF representation
for each channel is computed, leading to a three-dimensional array:
frequency, time and channel.</p><p>A TF representation is typically used as a first step in processing the
audio because sources tend to be less overlapped in the TF
representation than in the waveform [<a href="#ref-rickard02">17</a>]. This makes
it easier to select portions of a mixture that correspond to only a
single source. An STFT is typically used because it can be inverted back
to the original waveform. Therefore, modifications made to the STFT can
be used to create a modified waveform. Generally, a linear mixing
process is considered, i.e., the mixture signal is equal to the sum of
the source signals. Since the Fourier transform is a linear operation,
this equality holds for the STFT. While that is not the case for the
magnitude (or power) of the STFT, it is commonly assumed that the
spectrograms of the sources sum to the spectrogram of the mixture.</p><p>In many methods, the separated sources are obtained by <em>filtering</em> the
mixture. This can be understood as performing some equalization on the
mixture, where each frequency is attenuated or kept intact. Since both
the lead and the accompaniment signals change over time, the filter also
changes. This is typically done using a TF <em>mask</em>, which, in its
simplest form, is defined as the gain between (0) and (1) to apply
on each element of the TF representation of the mixture (e.g., an STFT)
in order to estimate the desired signal. Loosely speaking, it can be
understood as an equalizer whose setting changes every few milliseconds.
After multiplication of the mixture by a mask, the separated signal is
recovered through an inverse TF transform. In the multichannel setting,
more sophisticated filters may be designed that incorporate some delay
and combine different channels; this is usually called <em>beamforming</em>. In
the frequency domain, this is often equivalent to using complex matrices
to multiply the mixture TF representation with, instead of just scalars
between (0) and (1).</p><p>In practice, masks can be designed to filter the mixture in several
ways. One may estimate the spectrogram for a single source or component,
e.g., the accompaniment, and subtract it from the mixture spectrogram,
e.g., in order to estimate the lead [<a href="#ref-boll1979">18</a>]. Another way
would be to estimate separate spectrograms for both lead and
accompaniment and combine them to yield a mask. For instance, a TF mask
for the lead can be taken as the proportion of the lead spectrogram over
the sum of both spectrograms, at each TF bin. Such filters are often
called <em>Wiener filters</em> [<a href="#ref-wiener1975">19</a>] or <em>ratio masks</em>. How
they are calculated may involve some additional techniques like
exponentiation and may be understood according to assumptions regarding
the underlying statistics of the sources. For recent work in this area,
and many useful pointers in designing such masks, the reader is referred
to [<a href="#ref-liutkus15c">20</a>].</p><h3 id="audio-and-speech-modeling"><a href="#audio-and-speech-modeling" aria-hidden="true" class="header-anchor">#</a> Audio and speech modeling</h3><p>It is typical in audio processing to describe audio waveforms as
belonging to one of two different categories, which are <em>sinusoidal
signals</em> — or pure tones — and <em>noise</em>. Actually, both are just the two
extremes in a continuum of varying <em>predictability</em>: on the one hand,
the shape of a sinusoidal wave in the future can reliably be guessed
from previous samples. On the other hand, white noise is <em>defined</em> as an
unpredictable signal and its spectrogram has constant energy everywhere.
Different noise profiles may then be obtained by attenuating the energy
of some frequency regions. This in turn induces some predictability in
the signal, and in the extreme case where all the energy content is
concentrated in one frequency, a pure tone is obtained.</p><p>A waveform may always be modeled as some <em>filter</em> applied on some
<em>excitation signal</em>. Usually, the filter is assumed to vary smoothly
across frequencies, hence modifying only what is called <em>the spectral
envelope</em> of the signal, while the excitation signal comprises the rest.
This is the basis for the <em>source-filter</em> model [<a href="#ref-fant70">21</a>],
which is of great importance in speech modeling, and thus also in vocal
separation. As for speech, the filter is created by the shape of the
vocal tract. The excitation signal is made of the glottal pulses
generated by the vibration of the vocal folds. This results into
<em>voiced</em> speech sounds made of time-varying harmonic/sinusoidal
components. The excitation signal can also be the air flow passing
through some constriction of the vocal tract. This results into
<em>unvoiced</em>, noise-like, speech sounds. In this context, vowels are said
to be voiced and tend to feature many sinusoids, while some phonemes
such as fricatives are unvoiced and noisier.</p><p>A classical tool for dissociating the envelope from the excitation is
the <em>cepstrum</em> [<a href="#ref-bogert1963">22</a>]. It has applications for
estimating the fundamental frequency [<a href="#ref-noll64">23</a>],
[<a href="#ref-noll67">24</a>], for deriving the Mel-frequency cepstral
coefficients (MFCC) [<a href="#ref-david80">25</a>], or for filtering signals
through a so-called <em>liftering</em> operation [<a href="#ref-oppenheim69">26</a>]
that enables modifications of either the excitation or the envelope
parts through the source-filter
paradigm.</p><p><span id="fig:stylized_vocals_accompaniment" label="fig:stylized_vocals_accompaniment">[fig:stylized_vocals_accompaniment]</span></p><p>An advantage of the source-filter model approach is indeed that one can
dissociate the pitched content of the signal, embodied by the position
of its harmonics, from its TF envelope which describes where the energy
of the sound lies. In the case of vocals, it yields the ability to
distinguish between the actual note being sung (pitch content) and the
phoneme being uttered (mouth and vocal tract configuration),
respectively. One key feature of vocals is they typically exhibit great
variability in fundamental frequency over time. They can also exhibit
larger <em>vibratos</em> (fundamental frequency modulations) and <em>tremolos</em>
(amplitude modulations) in comparison to other instruments, as seen in
the top spectrogram in
Figure <a href="#fig:stylized_vocals_accompaniment">[fig:stylized_vocals_accompaniment]</a>.</p><p>A particularity of musical signals is that they typically consist of
sequences of pitched notes. A sound gives the perception of having a
pitch if the majority of the energy in the audio signal is at
frequencies located at integer multiples of some fundamental frequency.
These integer multiples are called <em>harmonics</em>. When the fundamental
frequency changes, the frequencies of these harmonics also change,
yielding the typical comb spectrograms of harmonic signals, as depicted
in the top spectrogram in
Figure <a href="#fig:stylized_vocals_accompaniment">[fig:stylized_vocals_accompaniment]</a>.
Another noteworthy feature of sung melodies over simple speech is that
their fundamental frequencies are, in general, located at precise
frequency values corresponding to the musical key of the song. These
very peculiar features are often exploited in separation methods. For
simplicity reasons, we use the terms <em>pitch</em> and <em>fundamental frequency</em>
interchangeably throughout the paper.</p><h3 id="probability-theory"><a href="#probability-theory" aria-hidden="true" class="header-anchor">#</a> Probability theory</h3><p>Probability theory [<a href="#ref-jaynes2003probability">14</a>],
[<a href="#ref-durrett2010probability">27</a>] is an important framework for
designing many data analysis and processing methods. Many of the methods
described in this article use it and it is far beyond the scope of this
paper to present it rigorously. For our purpose, it will suffice to say
that the <em>observations</em> consist of the mixture signals. On the other
hand, the <em>parameters</em> are any relevant feature about the source signal
(such as pitch or time-varying envelope) or how the signals are mixed
(e.g., the panning position). These parameters can be used to derive
estimates about the target lead and accompaniment signals.</p><p>We understand a probabilistic <em>model</em> as a function of both the
observations and the parameters: it describes how likely the
observations are, given the parameters. For instance, a flat spectrum is
likely under the noise model, and a mixture of comb spectrograms is
likely under a harmonic model with the appropriate pitch parameters for
the sources. When the observations are given, variation in the model
depends only on the parameters. For some parameter value, it tells how
likely the observations are. Under a harmonic model for instance, pitch
may be estimated by finding the pitch parameter that makes the observed
waveform as likely as possible. Alternatively, we may want to choose
between several possible models such as voiced or unvoiced. In such
cases, <em>model selection</em> methods are available, such as the Bayesian
information criterion (BIC) [<a href="#ref-schwarz78">28</a>].</p><p>Given these basic ideas, we briefly mention two models that are of
particular importance. Firstly, the hidden Markov model
(HMM) [<a href="#ref-cappe2005">15</a>], [<a href="#ref-rabiner89">29</a>] is relevant
for time-varying observations. It basically defines several <em>states</em>,
each one related to a specific model and with some probabilities for
transitions between them. For instance, we could define as many states
as possible notes played by the lead guitar, each one associated with a
typical spectrum. The <em>Viterbi algorithm</em> is a dynamic programming
method which actually estimates the most likely sequence of states given
a sequence of observations [<a href="#ref-viterbi2006">30</a>]. Secondly, the
Gaussian mixture model (GMM) [<a href="#ref-bishop96">31</a>] is a way to
approximate any distribution as a weighted sum of Gaussians. It is
widely used in clustering, because it works well with the celebrated
Expectation-Maximization (EM) algorithm [<a href="#ref-dempster77">32</a>] to
assign one particular cluster to each data point, while automatically
estimating the clusters parameters. As we will see later, many methods
work by assigning each TF bin to a given source in a similar way.</p><h2 id="modeling-the-lead-signal-harmonicity"><a href="#modeling-the-lead-signal-harmonicity" aria-hidden="true" class="header-anchor">#</a> Modeling the lead signal: harmonicity</h2><p><img src="https://docs.google.com/drawings/d/e/2PACX-1vS1ciSejDMm1qrkhaPSc9btYmTvnGc3p5XgxeFsI0De8I5IWYxR73ctpzu0E4Ud7S9KEWRHqcng__Q2/pub?w=592&h=403" alt></p><h4 id="the-approaches-based-on-a-harmonic-assumption-for-vocals-in-a-first-analysis-step-the-fundamental-frequency-of-the-lead-signal-is-extracted-from-it-a-separation-is-obtained-either-by-resynthesis-section-3-1-or-by-filtering-the-mixture-section-3-2-figures-figure2-pdf"><a href="#the-approaches-based-on-a-harmonic-assumption-for-vocals-in-a-first-analysis-step-the-fundamental-frequency-of-the-lead-signal-is-extracted-from-it-a-separation-is-obtained-either-by-resynthesis-section-3-1-or-by-filtering-the-mixture-section-3-2-figures-figure2-pdf" aria-hidden="true" class="header-anchor">#</a> The approaches based on a <em>harmonic assumption</em> for vocals. In a first analysis step, the fundamental frequency of the lead signal is extracted. From it, a separation is obtained either by resynthesis (Section <a href="#ssec:harmonicity-synthesis">3.1</a>), or by filtering the mixture (Section <a href="#ssec:harmonicity-combfiltering">3.2</a>).](figures/Figure2.pdf)</h4><p>As mentioned in Section <a href="#ssec:audio_and_speech_models">2.2</a>, one
particularity of vocals is their production by the vibration of the
vocal folds, further filtered by the vocal tract. As a consequence, sung
melodies are <em>mostly</em> harmonic, as depicted in
Figure <a href="#fig:stylized_vocals_accompaniment">[fig:stylized_vocals_accompaniment]</a>,
and therefore have a fundamental frequency. If one can track the pitch
of the vocals, one can then estimate the energy at the harmonics of the
fundamental frequency and reconstruct the voice. This is the basis of
the oldest methods (as well as some more recent methods) we are aware of
for separating the lead signal from a musical mixture.</p><p>Such methods are summarized in
Figure <a href="#fig:methods_harmonicity">[fig:methods_harmonicity]</a>. In a
first step, the objective is to get estimates of the time-varying
fundamental frequency for the lead at each time frame. A second step in
this respect is then to track this fundamental frequency over time, in
other words, to find the best sequence of estimates, in order to
identify the melody line. This can done either by a suitable pitch
detection method, or by exploiting the availability of the score. Such
algorithms typically assume that the lead corresponds to the harmonic
signal with strongest amplitude. For a review on the particular topic of
melody extraction, the reader is referred to [<a href="#ref-salamon14">33</a>].</p><p>From this starting point, we can distinguish between two kinds of
approaches, depending on how they exploit the pitch information.</p><h3 id="analysis-synthesis-approaches"><a href="#analysis-synthesis-approaches" aria-hidden="true" class="header-anchor">#</a> Analysis-synthesis approaches</h3><p>The first option to obtain the separated lead signal is to resynthesize
it using a sinusoidal model. A sinusoidal model decomposes the sound
with a set of sine waves of varying frequency and amplitude. If one
knows the fundamental frequency of a pitched sound (like a singing
voice), as well as the spectral envelope of the recording, then one can
reconstruct the sound by making a set of sine waves whose frequencies
are those of the harmonics of the fundamental frequency, and whose
amplitudes are estimated from the spectral envelope of the audio. While
the spectral envelope of the recording is generally not exactly the same
as the spectral envelope of the target source, it can be a reasonable
approximation, especially assuming that different sources do not overlap
too much with each other in the TF representation of the mixture.</p><p>This idea allows for time-domain processing and was used in the earliest
methods we are aware of. In 1973, Miller proposed in
[<a href="#ref-miller73">34</a>] to use the homomorphic vocoder
[<a href="#ref-oppenheim68">35</a>] to separate the excitation function and
impulse response of the vocal tract. Further refinements include
segmenting parts of the signal as voiced, unvoiced, or silences using a
heuristic program and manual interaction. Finally, cepstral
liftering [<a href="#ref-oppenheim69">26</a>] was exploited to compensate for
the noise or accompaniment.</p><p>Similarly, Maher used an analysis-synthesis approach
in [<a href="#ref-maher89">36</a>], assuming the mixtures are composed of only
two harmonic sources. In his case, pitch detection was performed on the
STFT and included heuristics to account for possibly colliding
harmonics. He finally resynthesized each musical voice with a sinusoidal
model.</p><p>Wang proposed instantaneous and frequency-warped techniques for signal
parameterization and source separation, with application to voice
separation in music [<a href="#ref-wang94">37</a>], [<a href="#ref-wang95">38</a>]. He
introduced a frequency-locked loop algorithm which uses multiple
harmonically constrained trackers. He computed the estimated fundamental
frequency from a maximum-likelihood weighting of the tracking estimates.
He was then able to estimate harmonic signals such as voices from
complex mixtures.</p><p>Meron and Hirose proposed to separate singing voice and piano
accompaniment [<a href="#ref-meron98">39</a>]. In their case, prior knowledge
consisting of musical scores was considered. Sinusoidal modeling as
described in [<a href="#ref-quatieri92">40</a>] was used.</p><p>Ben-Shalom and Dubnov proposed to filter an instrument or a singing
voice out in such a way [<a href="#ref-ben-shalom04">41</a>]. They first used a
score alignment algorithm [<a href="#ref-shalev-shwartz02">42</a>], assuming a
known score. Then, they used the estimated pitch information to design a
filter based on a harmonic model [<a href="#ref-serra97">43</a>] and performed
the filtering using the linear constraint minimum variance approach
[<a href="#ref-vanveen97">44</a>]. They additionally used a heuristic to deal
with the unvoiced parts of the singing voice.</p><p>Zhang and Zhang proposed an approach based on harmonic structure
modeling [<a href="#ref-zhang05">45</a>], [<a href="#ref-zhang06">46</a>]. They first
extracted harmonic structures for singing voice and background music
signals using a sinusoidal model [<a href="#ref-serra97">43</a>], by extending
the pitch estimation algorithm in [<a href="#ref-terhardt79">47</a>]. Then, they
used the clustering algorithm in [<a href="#ref-zhang03">48</a>] to learn
harmonic structure models for the background music signals. Finally,
they extracted the harmonic structures for all the instruments to
reconstruct the background music signals and subtract them from the
mixture, leaving only the singing voice signal.</p><p>More recently, Fujihara et al. proposed an accompaniment reduction
method for singer identification [<a href="#ref-fujihara05">49</a>],
[<a href="#ref-fujihara10">50</a>]. After fundamental frequency estimation
using [<a href="#ref-goto04">51</a>], they extracted the harmonic structure of
the melody, i.e., the power and phase of the sinusoidal components at
fundamental frequency and harmonics. Finally, they resynthesized the
audio signal of the melody using the sinusoidal model in
[<a href="#ref-moorer05">52</a>].</p><p>Similarly, Mesaros et al. proposed a vocal separation method to help
with singer identification [<a href="#ref-mesaros07">53</a>]. They first applied
a melody transcription system [<a href="#ref-ryynanen06">54</a>] which estimates
the melody line with the corresponding MIDI note numbers. Then, they
performed sinusoidal resynthesis, estimating amplitudes and phases from
the polyphonic signal.</p><p>In a similar manner, Duan et al. proposed to separate harmonic sources,
including singing voices, by using harmonic structure models
[<a href="#ref-duan08">55</a>]. They first defined an average harmonic structure
model for an instrument. Then, they learned a model for each source by
detecting the spectral peaks using a cross-correlation method
[<a href="#ref-rodet97">56</a>] and quadratic
interpolation [<a href="#ref-smith87">57</a>]. Then, they extracted the harmonic
structures using BIC and a clustering algorithm [<a href="#ref-zhang03">48</a>].
Finally, they separated the sources by re-estimating the fundamental
frequencies, re-extracting the harmonics, and reconstructing the signals
using a phase generation method [<a href="#ref-slaney94">58</a>].</p><p>Lagrange et al. proposed to formulate lead separation as a graph
partition problem [<a href="#ref-lagrange07">59</a>], [<a href="#ref-lagrange08">60</a>].
They first identified peaks in the spectrogram and grouped the peaks
into clusters by using a similarity measure which accounts for
harmonically related peaks, and the normalized cut criterion
[<a href="#ref-shi00">61</a>] which is used for segmenting graphs in computer
vision. They finally selected the cluster of peaks which corresponds to
a predominant harmonic source and resynthesized it using a bank of
sinusoidal oscillators.</p><p>Ryynänen et al. proposed to separate accompaniment from polyphonic music
using melody transcription for karaoke
applications [<a href="#ref-ryynanen08">62</a>]. They first transcribed the
melody into a MIDI note sequence and a fundamental frequency trajectory,
using the method in [<a href="#ref-ryynanen082">63</a>], an improved version of
the earlier method [<a href="#ref-ryynanen06">54</a>]. Then, they used sinusoidal
modeling to estimate, resynthesize, and remove the lead vocals from the
musical mixture, using the quadratic polynomial-phase model in
[<a href="#ref-ding97">64</a>].</p><h3 id="comb-filtering-approaches"><a href="#comb-filtering-approaches" aria-hidden="true" class="header-anchor">#</a> Comb-filtering approaches</h3><p>Using sinusoidal synthesis to generate the lead signal suffers from a
typical <em>metallic</em> sound quality, which is mostly due to discrepancies
between the estimated excitation signals of the lead signal compared to
the ground truth. To address this issue, an alternative approach is to
exploit harmonicity in another way, by filtering out everything from the
mixture that is not located close to the detected harmonics.</p><p>Li and Wang proposed to use a vocal/non-vocal classifier and a
predominant pitch detection algorithm [<a href="#ref-li06">65</a>],
[<a href="#ref-li07">66</a>]. They first detected the singing voice by using a
spectral change detector [<a href="#ref-duxbury03">67</a>] to partition the
mixture into homogeneous portions, and GMMs on MFCCs to classify the
portions as vocal or non-vocal. Then, they used the predominant pitch
detection algorithm in [<a href="#ref-li05">68</a>] to detect the pitch contours
from the vocal portions, extending the multi-pitch tracking algorithm
in [<a href="#ref-wu03">69</a>]. Finally, they extracted the singing voice by
decomposing the vocal portions into TF units and labeling them as
singing or accompaniment dominant, extending the speech separation
algorithm in [<a href="#ref-hu02">70</a>].</p><p>Han and Raphael proposed an approach for desoloing a recording of a
soloist with an accompaniment given a musical score and its time
alignment with the recording [<a href="#ref-han07">71</a>]. They derived a mask
[<a href="#ref-roweis01">72</a>] to remove the solo part after using an EM
algorithm to estimate its melody, that exploits the score as side
information.</p><p>Hsu et al. proposed an approach which also identifies and separates the
unvoiced singing voice [<a href="#ref-hsu08">73</a>], [<a href="#ref-hsu10">74</a>].
Instead of processing in the STFT domain, they use the perceptually
motivated gammatone filter-bank as in [<a href="#ref-li07">66</a>],
[<a href="#ref-hu02">70</a>]. They first detected accompaniment, unvoiced, and
voiced segments using an HMM and identified voice-dominant TF units in
the voiced frames by using the singing voice separation method in
[<a href="#ref-li07">66</a>], using the predominant pitch detection algorithm in
[<a href="#ref-dressler062">75</a>]. Unvoiced-dominant TF units were identified
using a GMM classifier with MFCC features learned from training data.
Finally, filtering was achieved with spectral
subtraction [<a href="#ref-scalart96">76</a>].</p><p>Raphael and Han then proposed a classifier-based approach to separate a
soloist from accompanying instruments using a time-aligned symbolic
musical score [<a href="#ref-raphael08">77</a>]. They built a tree-structured
classifier [<a href="#ref-breiman84">78</a>] learned from labeled training data
to classify TF points in the STFT as belonging to solo or accompaniment.
They additionally constrained their classifier to estimate masks having
a connected structure.</p><p>Cano et al. proposed various approaches for solo and accompaniment
separation. In [<a href="#ref-cano09">79</a>], they separated saxophone melodies
from mixtures with piano and/or orchestra by using a melody line
detection algorithm, incorporating information about typical saxophone
melody lines. In [<a href="#ref-grollmisch11">80</a>]–[<a href="#ref-cano12">82</a>], they
proposed to use the pitch detection algorithm
in [<a href="#ref-dressler11">83</a>]. Then, they refined the fundamental
frequency and the harmonics, and created a binary mask for the solo and
accompaniment. They finally used a post-processing stage to refine the
separation. In [<a href="#ref-cano13">84</a>], they included a noise spectrum in
the harmonic refinement stage to also capture noise-like sounds in
vocals. In [<a href="#ref-cano14">85</a>], they additionally included common
amplitude modulation characteristics in the separation scheme.</p><p>Bosch et al. proposed to separate the lead instrument using a musical
score [<a href="#ref-bosch12">86</a>]. After a preliminary alignment of the score
to the mixture, they estimated a score confidence measure to deal with
local misalignments and used it to guide the predominant pitch tracking.
Finally, they performed low-latency separation based on the method in
[<a href="#ref-marxer12">87</a>], by combining harmonic masks derived from the
estimated pitch and additionally exploiting stereo information as
presented later in Section <a href="#sec:multichannel">7</a>.</p><p>Vaneph et al. proposed a framework for vocal isolation to help spectral
editing [<a href="#ref-vaneph16">88</a>]. They first used a voice activity
detection process based on a deep learning technique
[<a href="#ref-leglaive15">89</a>]. Then, they used pitch tracking to detect the
melodic line of the vocal and used it to separate the vocal and
background, allowing a user to provide manual annotations when
necessary.</p><h3 id="shortcomings"><a href="#shortcomings" aria-hidden="true" class="header-anchor">#</a> Shortcomings</h3><p>As can be seen, explicitly assuming that the lead signal is harmonic led
to an important body of research. While the aforementioned methods show
excellent performance when their assumptions are valid, their
performance can drop significantly in adverse, but common situations.</p><p>Firstly, vocals are not always purely harmonic as they contain unvoiced
phonemes that are not harmonic. As seen above, some methods already
handle this situation. However, vocals can also be whispered or
saturated, both of which are difficult to handle with a harmonic model.</p><p>Secondly, methods based on the harmonic model depend on the quality of
the pitch detection method. If the pitch detector switches from
following the pitch of the lead (e.g., the voice) to another instrument,
the wrong sound will be isolated from the mix. Often, pitch detectors
assume the lead signal is the <em>loudest</em> harmonic sound in the mix.
Unfortunately, this is not always the case. Another instrument may be
louder or the lead may be silent for a passage. The tendency to follow
the pitch of the wrong instrument can be mitigated by applying
constraints on the pitch range to estimate and by using a perceptually
relevant weighting filter before performing pitch tracking. Of course,
these approaches do not help when the lead signal is silent.</p><h2 id="modeling-the-accompaniment-redundancy"><a href="#modeling-the-accompaniment-redundancy" aria-hidden="true" class="header-anchor">#</a> Modeling the accompaniment: redundancy</h2><p>In the previous section, we presented methods whose main focus was the
modeling of a harmonic lead melody. Most of these studies did not make
modeling the accompaniment a core focus. On the contrary, it was often
dealt with as adverse noise to which the harmonic processing method
should be robust to.</p><p>In this section, we present another line of research which concentrates
on modeling the accompaniment under the assumption it is somehow more
<em>redundant</em> than the lead signal. This assumption stems from the fact
that musical accompaniments are often highly structured, with elements
being repeated many times. Such repetitions can occur at the note level,
in terms of rhythmic structure, or even from a harmonic point of view:
instrumental notes are often constrained to have their pitch lie in a
small set of frequencies. Therefore, modeling and removing the redundant
elements of the signal are assumed to result in removal of the
accompaniment.</p><p>In this paper, we identify three families of methods that exploit the
redundancy of the accompaniment for separation.</p><h3 id="grouping-low-rank-components"><a href="#grouping-low-rank-components" aria-hidden="true" class="header-anchor">#</a> Grouping low-rank components</h3><p><img src="https://docs.google.com/drawings/d/e/2PACX-1vS1JrZuBXyzbaLEpt_ekUHVqENMcUWZkDjvhWvQSpN4vdUAr2asfRZzL471bpoUhbSNTN7b1nPojviG/pub?w=406&h=547" alt></p><h4 id="the-approaches-based-on-a-low-rank-assumption-non-negative-matrix-factorization-nmf-is-used-to-identify-components-from-the-mixture-that-are-subsequently-clustered-into-lead-or-accompaniment-additional-constraints-may-be-incorporated"><a href="#the-approaches-based-on-a-low-rank-assumption-non-negative-matrix-factorization-nmf-is-used-to-identify-components-from-the-mixture-that-are-subsequently-clustered-into-lead-or-accompaniment-additional-constraints-may-be-incorporated" aria-hidden="true" class="header-anchor">#</a> The approaches based on a <em>low-rank</em> assumption. Non-negative matrix factorization (NMF) is used to identify <em>components</em> from the mixture, that are subsequently clustered into lead or accompaniment. Additional constraints may be incorporated.</h4><p>The first set of approaches we consider is the identification of
redundancy in the accompaniment through the assumption that its
spectrogram may be well represented by only a few components. Techniques
exploiting this idea then focus on algebraic methods that decompose the
mixture spectrogram into the product of a few template spectra activated
over time. One way to do so is via non-negative matrix factorization
(NMF) [<a href="#ref-lee99">90</a>], [<a href="#ref-lee01">91</a>], which incorporates
non-negative constraints. In
Figure <a href="#fig:methods_low_rank">[fig:methods_low_rank]</a>, we picture
methods exploiting such techniques. After factorization, we obtain
several spectra, along with their activations over time. A subsequent
step is the clustering of these spectra (and activations) into the lead
or the accompaniment. Separation is finally performed by deriving Wiener
filters to estimate the lead and the accompaniment from the mixture. For
related applications of NMF in music analysis, the reader is referred to
[<a href="#ref-smaragdis03">92</a>]–[<a href="#ref-fevotte09">94</a>].</p><p>Vembu and Baumann proposed to use NMF (and also ICA
[<a href="#ref-common94">95</a>]) to separate vocals from mixtures
[<a href="#ref-vembu05">96</a>]. They first discriminated between vocal and
non-vocal sections in a mixture by using different combinations of
features, such as MFCCs [<a href="#ref-david80">25</a>], perceptual linear
predictive (PLP) coefficients [<a href="#ref-hermansky90">97</a>], and log
frequency power coefficients (LFPC) [<a href="#ref-nwe04">98</a>], and training
two classifiers, namely neural networks and support vector machines
(SVM). They then applied redundancy reduction techniques on the TF
representation of the mixture to separate the sources
[<a href="#ref-casey00">99</a>], by using NMF (or ICA). The components were then
grouped as vocal and non-vocal by reusing a vocal/non-vocal classifier
with MFCC, LFPC, and PLP coefficients.</p><p>Chanrungutai and Ratanamahatana proposed to use NMF with automatic
component selection[<a href="#ref-chanrungutai08">100</a>],
[<a href="#ref-chanrungutai082">101</a>]. They first decomposed the mixture
spectrogram using NMF with a fixed number of basis components. They then
removed the components with brief rhythmic and long-lasting continuous
events, assuming that they correspond to instrumental sounds. They
finally used the remaining components to reconstruct the singing voice,
after refining them using a high-pass filter.</p><p>Marxer and Janer proposed an approach based on a Tikhonov
regularization [<a href="#ref-tikhonov63">102</a>] as an alternative to NMF, for
singing voice separation [<a href="#ref-marxer122">103</a>]. Their method
sacrificed the non-negativity constraints of the NMF in exchange for a
computationally less expensive solution for spectrum decomposition,
making it more interesting in low-latency scenarios.</p><p>Yang et al. proposed a Bayesian NMF approach [<a href="#ref-yang14">104</a>],
[<a href="#ref-chien15">105</a>]. Following the approaches in
[<a href="#ref-cemgil09">106</a>] and [<a href="#ref-schmidt09">107</a>], they used a
Poisson distribution for the likelihood function and exponential
distributions for the model parameters in the NMF algorithm, and derived
a variational Bayesian EM algorithm [<a href="#ref-dempster77">32</a>] to solve
the NMF problem. They also adaptively determined the number of bases
from the mixture. They finally grouped the bases into singing voice and
background music by using a <em>k</em>-means clustering algorithm
[<a href="#ref-spiertz09">108</a>] or an NMF-based clustering algorithm.</p><p>In a different manner, Smaragdis and Mysore proposed a user-guided
approach for removing sounds from mixtures by humming the target sound
to be removed, for example a vocal track [<a href="#ref-smaragdis09">109</a>].
They modeled the mixture using probabilistic latent component analysis
(PLCA) [<a href="#ref-smaragdis07">110</a>], another equivalent formulation of
NMF. One key feature of exploiting user input was to facilitate the
grouping of components into vocals and accompaniment, as humming helped
to identify some of the parameters for modeling the vocals.</p><p>Nakamuray and Kameoka proposed an (L_p)-norm
NMF [<a href="#ref-nakamuray15">111</a>], with (p) controlling the sparsity of
the error. They developed an algorithm for solving this NMF problem
based on the auxiliary function principle [<a href="#ref-ortega70">112</a>],
[<a href="#ref-kameoka06">113</a>]. Setting an adequate number of bases and (p)
taken as small enough allowed them to estimate the accompaniment as the
low-rank decomposition, and the singing voice as the error of the
approximation, respectively. Note that, in this case, the singing voice
was not explicitly modeled as a sparse component but rather corresponded
to the error which happened to be constrained as sparse. The next
subsection will actually deal with approaches that explicitly model the
vocals as the sparse component.</p><h3 id="low-rank-accompaniment-sparse-vocals"><a href="#low-rank-accompaniment-sparse-vocals" aria-hidden="true" class="header-anchor">#</a> Low-rank accompaniment, sparse vocals</h3><p><img src="https://docs.google.com/drawings/d/e/2PACX-1vSDMQhw6sU4gz4pG1sne-HLDS1qaEnfZ0fVL23yawA9EoHTQa_ZTBiJYwZVqnDVYfikANm9ZIiIU8Xh/pub?w=720" alt></p><h4 id="the-approaches-based-on-a-low-rank-accompaniment-sparse-vocals-assumption-as-opposed-to-methods-based-on-nmf-methods-based-on-robust-principal-component-analysis-rpca-assume-the-lead-signal-has-a-sparse-and-non-structured-spectrogram"><a href="#the-approaches-based-on-a-low-rank-accompaniment-sparse-vocals-assumption-as-opposed-to-methods-based-on-nmf-methods-based-on-robust-principal-component-analysis-rpca-assume-the-lead-signal-has-a-sparse-and-non-structured-spectrogram" aria-hidden="true" class="header-anchor">#</a> The approaches based on a <em>low-rank accompaniment, sparse vocals</em> assumption. As opposed to methods based on NMF, methods based on robust principal component analysis (RPCA) assume the lead signal has a sparse and non-structured spectrogram.</h4><p>The methods presented in the previous section first compute a
decomposition of the mixture into many components that are sorted <em>a
posteriori</em> as accompaniment or lead. As can be seen, this means they
make a low-rank assumption for the accompaniment, but typically <em>also
for the vocals</em>. However, as can for instance be seen on
Figure <a href="#fig:stylized_vocals_accompaniment">[fig:stylized_vocals_accompaniment]</a>,
the spectrogram for the vocals do exhibit much more freedom than
accompaniment, and experience shows they are not adequately described by
a small number of spectral bases. For this reason, another track of
research depicted in Figure <a href="#fig:methods_rpca">[fig:methods_rpca]</a>
focused on using a low-rank assumption on the accompaniment <em>only</em>,
while assuming the vocals are <em>sparse and not structured</em>. This loose
assumption means that only a few coefficients from their spectrogram
should have significant magnitude, and that they should not feature
significant redundancy. Those ideas are in line with robust principal
component analysis (RPCA) [<a href="#ref-candes11">114</a>], which is the
mathematical tool used by this body of methods, initiated by Huang et
al. for singing voice separation [<a href="#ref-huang12">115</a>] . It decomposes
a matrix into a sparse and low-rank component.</p><p>Sprechmann et al. proposed an approach based on RPCA for online singing
voice separation [<a href="#ref-sprechmann12">116</a>]. They used ideas from
convex optimization [<a href="#ref-recht10">117</a>], [<a href="#ref-recht13">118</a>] and
multi-layer neural networks [<a href="#ref-gregor10">119</a>]. They presented two
extensions of RPCA and robust NMF models [<a href="#ref-zhang11">120</a>]. They
then used these extensions in a multi-layer neural network framework
which, after an initial training stage, allows online source separation.</p><p>Jeong and Lee proposed two extensions of the RPCA model to improve the
estimation of vocals and accompaniment from the sparse and low-rank
components [<a href="#ref-jeong14">121</a>]. Their first extension included the
Schatten (p) and (\ell_{p}) norms as generalized nuclear norm
optimizations [<a href="#ref-nie152">122</a>]. They also suggested a
pre-processing stage based on logarithmic scaling of the mixture TF
representation to enhance the RPCA.</p><p>Yang also proposed an approach based on RPCA with dictionary learning
for recovering low-rank components [<a href="#ref-yang13">123</a>]. He introduced
a multiple low-rank representation following the observation that
elements of the singing voice can also be recovered by the low-rank
component. He first incorporated online dictionary learning methods
[<a href="#ref-mairal09">124</a>] in his methodology to obtain prior information
about the structure of the sources and then incorporated them into the
RPCA model.</p><p>Chan and Yang then extended RPCA to complex and quaternionic cases with
application to singing voice separation [<a href="#ref-chan16">125</a>]. They
extended the principal component pursuit (PCP) [<a href="#ref-candes11">114</a>]
for solving the RPCA problem by presenting complex and quaternionic
proximity operators for the (\ell_{1}) and trace-norm regularizations
to account for the missing phase information.</p><h3 id="repetitions-within-the-accompaniment"><a href="#repetitions-within-the-accompaniment" aria-hidden="true" class="header-anchor">#</a> Repetitions within the accompaniment</h3><p>While the rationale behind low-rank methods for lead-accompaniment
separation is to exploit the idea that the musical background should be
redundant, adopting a low-rank model is not the only way to do it. An
alternate way to proceed is to exploit the musical <em>structure</em> of songs,
to find <em>repetitions</em> that can be utilized to perform separation. Just
like in RPCA-based methods, the accompaniment is then assumed to be the
only source for which repetitions will be found. The unique feature of
the methods described here is they combine music structure analysis
[<a href="#ref-peeters03">126</a>]–[<a href="#ref-paulus10">128</a>] with particular ways
to exploit the identification of repeated parts of the accompaniment.</p><p><img src="https://docs.google.com/drawings/d/e/2PACX-1vTsnQlO0NWOqGKwG1ksYtD8oYpf2exFzCkHV6pX5COfgGCmJVNhl3E64qcgoq3dJwdapgK9eXltAUIH/pub?w=720" alt></p><h4 id="the-approaches-based-on-a-repetition-assumption-for-accompaniment-in-a-first-analysis-step-repetitions-are-identified-then-they-are-used-to-build-an-estimate-for-the-accompaniment-spectrogram-and-proceed-to-separation"><a href="#the-approaches-based-on-a-repetition-assumption-for-accompaniment-in-a-first-analysis-step-repetitions-are-identified-then-they-are-used-to-build-an-estimate-for-the-accompaniment-spectrogram-and-proceed-to-separation" aria-hidden="true" class="header-anchor">#</a> The approaches based on a <em>repetition</em> assumption for accompaniment. In a first analysis step, repetitions are identified. Then, they are used to build an estimate for the accompaniment spectrogram and proceed to separation.</h4><p>Rafii et al. proposed the REpeating Pattern Extraction Technique (REPET)
to separate the accompaniment by assuming it is repeating
[<a href="#ref-rafii11">129</a>]–[<a href="#ref-rafii14">131</a>], which is often the case
in popular music. This approach, which is representative of this line of
research, is represented on
Figure <a href="#fig:methods_repet">[fig:methods_repet]</a>. First, a repeating
period is extracted by a music information retrieval system, such as a
beat spectrum [<a href="#ref-foote01">132</a>] in this case. Then, this extracted
information is used to estimate the spectrogram of the accompaniment
through an averaging of the identified repetitions. From this, a filter
is derived.</p><p>Seetharaman et al.[<a href="#ref-seetharaman17">133</a>] leveraged the two
dimensional Fourier transform (2DFT) of the spectrogram to create an
algorithm very similar to REPET. The properties of the 2DFT let them
separate the periodic background from the non-periodic vocal melody by
deleting peaks in the 2DFT. This eliminated the need to create an
explicit model of the periodic audio and without the need to find the
period of repetition, both of which are required in REPET.</p><p>Liutkus et al. adapted the REPET approach in [<a href="#ref-rafii11">129</a>],
[<a href="#ref-rafii13">130</a>] to handle repeating structures varying along
time by modeling the repeating patterns only locally
[<a href="#ref-rafii14">131</a>], [<a href="#ref-liutkus12">134</a>]. They first
identified a repeating period for every time frame by computing a beat
spectrogram as in [<a href="#ref-foote01">132</a>]. Then they estimated the
spectrogram of the accompaniment by averaging the time frames in the
mixture spectrogram at their local period rate, for every TF bin. From
this, they finally extracted the repeating structure by deriving a TF
mask.</p><p>Rafii et al. further extended the REPET approaches in
[<a href="#ref-rafii11">129</a>], [<a href="#ref-rafii13">130</a>] and
[<a href="#ref-liutkus12">134</a>] to handle repeating structures that are not
periodic. To do this, they proposed the REPET-SIM method in
[<a href="#ref-rafii14">131</a>], [<a href="#ref-rafii12">135</a>] to identify repeating
frames for every time frame by computing a self-similarity matrix, as in
[<a href="#ref-foote99">136</a>]. Then, they estimated the accompaniment
spectrogram at every TF bin by averaging the neighbors identified thanks
to that similarity matrix. An extension for real-time processing was
presented in [<a href="#ref-rafii133">137</a>] and a version exploiting user
interaction was proposed in [<a href="#ref-rafii15">138</a>]. A method close to
REPET-SIM was also proposed by FitzGerald in
[<a href="#ref-fitzgerald12">139</a>].</p><p>Liutkus et al. proposed the Kernel Additive modeling (KAM)
[<a href="#ref-liutkus14">140</a>], [<a href="#ref-liutkus142">141</a>] as a framework
which generalizes the REPET approaches in
[<a href="#ref-rafii11">129</a>]–[<a href="#ref-rafii14">131</a>],
[<a href="#ref-liutkus12">134</a>], [<a href="#ref-rafii12">135</a>]. They assumed that a
source at a TF location can be modeled using its values at other
locations through a specified kernel which can account for features such
as periodicity, self-similarity, stability over time or frequency, etc.
This notably enabled modeling of the accompaniment using more than one
repeating pattern. Liutkus et al. also proposed a light version using a
fast compression algorithm to make the approach more scalable
[<a href="#ref-liutkus15">142</a>]. The approach was also used for interference
reduction in music recordings [<a href="#ref-pratzlich15">143</a>],
[<a href="#ref-fanoyela17">144</a>].</p><p>With the same idea of exploiting intra-song redundancies for singing
voice separation, but through a very different methodology, Moussallam
et al. assumed in [<a href="#ref-moussallam12">145</a>] that all the sources can
be decomposed sparsely in the same dictionary and used a matching
pursuit greedy algorithm [<a href="#ref-mallat93">146</a>] to solve the problem.
They integrated the separation process in the algorithm by modifying the
atom selection criterion and adding a decision to assign a chosen atom
to the repeated source or to the lead signal.</p><p>Deif et al. proposed to use multiple median filters to separate vocals
from music recordings [<a href="#ref-deif152">147</a>]. They augmented the
approach in [<a href="#ref-fitzgerald102">148</a>] with diagonal median filters
to improve the separation of the vocal component. They also investigated
different filter lengths to further improve the separation.</p><p>Lee et al. also proposed to use the KAM approach
[<a href="#ref-lee152">149</a>]–[<a href="#ref-kim16">152</a>]. They applied the
(\beta)-order minimum mean square error (MMSE) estimation
[<a href="#ref-plourde08">153</a>] to the back-fitting algorithm in KAM to
improve the separation. They adaptively calculated a perceptually
weighting factor (\alpha) and the singular value decomposition
(SVD)-based factorized spectral amplitude exponent (\beta) for each
kernel component.</p><h3 id="shortcomings-2"><a href="#shortcomings-2" aria-hidden="true" class="header-anchor">#</a> Shortcomings</h3><p>While methods focusing on harmonic models for the lead often fall short
in their expressive power for the accompaniment, the methods we reviewed
in this section are often observed to suffer exactly from the converse
weakness, namely they do not provide an adequate model for the lead
signal. Hence, the separated vocals often will feature interference from
unpredictable parts from the accompaniment, such as some percussion or
effects which occur infrequently.</p><p>Furthermore, even if the musical accompaniment will exhibit more
redundancy, the vocals part will also be redundant to some extent, which
is poorly handled by these methods. When the lead signal is not vocals
but played by some lead instrument, its redundancy is even more
pronounced, because the notes it plays lie in a reduced set of
fundamental frequencies. Consequently, such methods would include the
redundant parts of the lead within the accompaniment estimate, for
example, a steady humming by a vocalist.</p><h2 id="joint-models-for-lead-and-accompaniment"><a href="#joint-models-for-lead-and-accompaniment" aria-hidden="true" class="header-anchor">#</a> Joint models for lead and accompaniment</h2><p>In the previous sections, we reviewed two important bodies of
literature, focused on modeling either the lead or the accompaniment
parts of music recordings, respectively. While each approach showed its
own advantages, it also featured its own drawbacks. For this reason,
some researchers devised methods combining ideas for modeling both the
lead and the accompaniment sources, and thus benefiting from both
approaches. We now review this line of research.</p><h3 id="using-music-structure-analysis-to-drive-learning"><a href="#using-music-structure-analysis-to-drive-learning" aria-hidden="true" class="header-anchor">#</a> Using music structure analysis to drive learning</h3><p>The first idea we find in the literature is to augment methods for
accompaniment modeling with the prior identification of sections where
the vocals are present or absent. In the case of the low rank models
discussed in Sections <a href="#ssec:NMF">4.1</a> and <a href="#ssec:RPCA">4.2</a>, such a
strategy indeed dramatically improves performance.</p><p>Raj et al. proposed an approach in [<a href="#ref-raj07">154</a>] that is based
on the PLCA formulation of NMF [<a href="#ref-smaragdis06">155</a>], and extends
their prior work [<a href="#ref-raj05">156</a>]. The parameters for the frequency
distribution of the background music are estimated from the background
music-only segments, and the rest of the parameters from the singing
voice+background music segments, assuming a priori identified vocal
regions.</p><p>Han and Chen also proposed a similar approach for melody extraction
based on PLCA [<a href="#ref-han11">157</a>], which includes a further estimate
of the melody from the vocals signal by an autocorrelation technique
similar to [<a href="#ref-boersma93">158</a>].</p><p>Gómez et al. proposed to separate the singing voice from the guitar
accompaniment in flamenco music to help with melody transcription
[<a href="#ref-gomez12">159</a>]. They first manually segmented the mixture into
vocal and non-vocal regions. They then learned percussive and harmonic
bases from the non-vocal regions by using an unsupervised NMF
percussive/harmonic separation approach [<a href="#ref-virtanen07">93</a>],
[<a href="#ref-ono08">160</a>]. The vocal spectrogram was estimated by keeping
the learned percussive and harmonic bases fixed.</p><p>Papadopoulos and Ellis proposed a signal-adaptive formulation of RPCA
which incorporates music content information to guide the recovery of
the sparse and low-rank components [<a href="#ref-papadopoulos14">161</a>]. Prior
musical knowledge, such as predominant melody, is used to regularize the
selection of active coefficients during the optimization procedure.</p><p>In a similar manner, Chan et al. proposed to use RPCA with vocal
activity information [<a href="#ref-chan15">162</a>]. They modified the RPCA
algorithm to constraint parts of the input spectrogram to be non-sparse
to account for the non-vocal parts of the singing voice.</p><p>A related method was proposed by Jeong and Lee in
[<a href="#ref-jeong17">163</a>], using RPCA with a weighted (l_1)-norm. They
replaced the uniform weighting between the low-rank and sparse
components in the RPCA algorithm by an adaptive weighting based on the
variance ratio between the singing voice and the accompaniment. One key
element of the method is to incorporate vocal activation information in
the weighting.</p><h3 id="factorization-with-a-known-melody"><a href="#factorization-with-a-known-melody" aria-hidden="true" class="header-anchor">#</a> Factorization with a known melody</h3><p>While using only the knowledge of vocal activity as described above
already yields an increase of performance over methods operating
blindly, many authors went further to also incorporate the fact that
vocals often have a strong melody line. Some redundant model is then
assumed for the accompaniment, while also enforcing a harmonic model for
the vocals.</p><p><img src="https://docs.google.com/drawings/d/e/2PACX-1vSnyEXv0_Zwpc25L8mcLTDhNK84jaTrOWu2L8kM4W75Whmw6xSz3KR-XTZ3wErGk9CeFgf35HHy0z5G/pub?w=720" alt></p><h4 id="factorization-informed-with-the-melody-first-melody-extraction-is-performed-on-the-mixture-then-this-information-is-used-to-drive-the-estimation-of-the-accompaniment-tf-bins-pertaining-to-the-lead-should-not-be-taken-into-account-for-estimating-the-accompaniment-model"><a href="#factorization-informed-with-the-melody-first-melody-extraction-is-performed-on-the-mixture-then-this-information-is-used-to-drive-the-estimation-of-the-accompaniment-tf-bins-pertaining-to-the-lead-should-not-be-taken-into-account-for-estimating-the-accompaniment-model" aria-hidden="true" class="header-anchor">#</a> Factorization informed with the melody. First, melody extraction is performed on the mixture. Then, this information is used to drive the estimation of the accompaniment: TF bins pertaining to the lead should not be taken into account for estimating the accompaniment model.</h4><p>An early method to achieve this is depicted in
Figure <a href="#fig:NMF_known_melody">[fig:NMF_known_melody]</a> and was
proposed by Virtanen et al. in [<a href="#ref-virtanen08">164</a>]. They
estimated the pitch of the vocals in the mixture by using a melody
transcription algorithm [<a href="#ref-ryynanen082">63</a>] and derived a binary
TF mask to identify where vocals are not present. They then applied NMF
on the remaining non-vocal segments to learn a model for the background.</p><p>Wang and Ou also proposed an approach which combines melody extraction
and NMF-based soft masking [<a href="#ref-wang11">165</a>]. They identified
accompaniment, unvoiced, and voiced segments in the mixture using an HMM
model with MFCCs and GMMs. They then estimated the pitch of the vocals
from the voiced segments using the method in [<a href="#ref-klapuri06">166</a>]
and an HMM with the Viterbi algorithm as in [<a href="#ref-hsu09">167</a>]. They
finally applied a soft mask to separate voice and accompaniment.</p><p>Rafii et al. investigated the combination of an approach for modeling
the background and an approach for modeling the melody
[<a href="#ref-rafii142">168</a>]. They modeled the background by deriving a
rhythmic mask using the REPET-SIM algorithm [<a href="#ref-rafii12">135</a>] and
the melody by deriving a harmonic mask using a pitch-based algorithm
[<a href="#ref-duan10">169</a>]. They proposed a parallel and a sequential
combination of those algorithms.</p><p>Venkataramani et al. proposed an approach combining sinusoidal modeling
and matrix decomposition, which incorporates prior knowledge about
singer and phoneme identity [<a href="#ref-venkataramani14">170</a>]. They
applied a predominant pitch algorithm on annotated sung regions
[<a href="#ref-rao10">171</a>] and performed harmonic sinusoidal modeling
[<a href="#ref-rao11">172</a>]. Then, they estimated the spectral envelope of the
vocal component from the spectral envelope of the mixture using a
phoneme dictionary. After that, a spectral envelope dictionary
representing sung vowels from song segments of a given singer was
learned using an extension of NMF [<a href="#ref-kim112">173</a>],
[<a href="#ref-zhou14">174</a>]. They finally estimated a soft mask using the
singer-vowel dictionary to refine and extract the vocal component.</p><p>Ikemiya et al. proposed to combine RPCA with pitch
estimation[<a href="#ref-ikemiya15">175</a>], [<a href="#ref-ikemiya16">176</a>]. They
derived a mask using RPCA [<a href="#ref-huang12">115</a>] to separate the
mixture spectrogram into singing voice and accompaniment components.
They then estimated the fundamental frequency contour from the singing
voice component based on [<a href="#ref-hermes88">177</a>] and derived a harmonic
mask. They integrated the two masks and resynthesized the singing voice
and accompaniment signals. Dobashi et al. then proposed to use that
singing voice separation approach in a music performance assistance
system [<a href="#ref-dobashi15">178</a>].</p><p>Hu and Liu proposed to combine approaches based on matrix decomposition
and pitch information for singer identification[<a href="#ref-hu15">179</a>].
They used non-negative matrix partial co-factorization
[<a href="#ref-kim112">173</a>], [<a href="#ref-yoo10">180</a>] which integrates prior
knowledge about the singing voice and the accompaniment, to separate the
mixture into singing voice and accompaniment portions. They then
identified the singing pitch from the singing voice portions using
[<a href="#ref-boersma01">181</a>] and derived a harmonic mask as in
[<a href="#ref-li09">182</a>], and finally reconstructed the singing voice using
a missing feature method [<a href="#ref-raj04">183</a>]. They also proposed to
add temporal and sparsity criteria to their algorithm
[<a href="#ref-hu16">184</a>].</p><p>That methodology was also adopted by Zhang et al. in
[<a href="#ref-zhang15">185</a>], that followed the framework of the pitch-based
approach in [<a href="#ref-li07">66</a>], by performing singing voice detection
using an HMM classifier, singing pitch detection using the algorithm in
[<a href="#ref-decheveigne02">186</a>], and singing voice separation using a
binary mask. Additionally, they augmented that approach by analyzing the
latent components of the TF matrix using NMF in order to refine the
singing voice and accompaniment.</p><p>Zhu et al. [<a href="#ref-zhu15">187</a>] proposed an approach which is also
representative of this body of literature, with the pitch detection
algorithm being the one in [<a href="#ref-boersma01">181</a>] and binary TF masks
used for separation after NMF.</p><h3 id="joint-factorization-and-melody-estimation"><a href="#joint-factorization-and-melody-estimation" aria-hidden="true" class="header-anchor">#</a> Joint factorization and melody estimation</h3><p>The methods presented above put together the ideas of modeling the lead
(typically the vocals) as featuring a melodic harmonic line and the
accompaniment as redundant. As such, they already exhibit significant
improvement over approaches only applying one of these ideas as
presented in Sections <a href="#sec:lead-harmonicity">3</a>
and <a href="#sec:accompaniment-redundancy">4</a>, respectively. However, these
methods above are still restricted in the sense that the analysis
performed on each side cannot help improve the other one. In other
words, the estimation of the models for the lead and the accompaniment
are done sequentially. Another idea is to proceed <em>jointly</em>.</p><p><img src="https://docs.google.com/drawings/d/e/2PACX-1vQ-ORJenf_FMWUl1CkGsNR5Vq9Gf8YapW_tkQwuAWTYGVVG6wzCablPGA_M_464l-Gig6Y6mr6nbFNX/pub?w=720" alt></p><h4 id="joint-estimation-of-the-lead-and-accompaniment-the-former-one-as-a-source-filter-model-and-the-latter-one-as-an-nmf-model"><a href="#joint-estimation-of-the-lead-and-accompaniment-the-former-one-as-a-source-filter-model-and-the-latter-one-as-an-nmf-model" aria-hidden="true" class="header-anchor">#</a> Joint estimation of the lead and accompaniment, the former one as a source-filter model and the latter one as an NMF model.</h4><p>A seminal work in this respect was done by Durrieu et al. using a
source-filter and NMF model
[<a href="#ref-durrieu08">188</a>]–[<a href="#ref-durrieu10">190</a>], depicted in
Figure <a href="#fig:methods_sourcefilter">[fig:methods_sourcefilter]</a>. Its
core idea is to decompose the mixture spectrogram as the sum of two
terms. The first term accounts for the lead and is inspired by the
source-filter model described in Section <a href="#sec:concepts">2</a>: it is the
element-wise product of an <em>excitation</em> spectrogram with a <em>filter</em>
spectrogram. The former one can be understood as harmonic combs
activated by the melodic line, while the latter one modulates the
envelope and is assumed low-rank because few phonemes are used. The
second term accounts for the accompaniment and is modeled with a
standard NMF. In [<a href="#ref-durrieu08">188</a>]–[<a href="#ref-durrieu10">190</a>],
they modeled the lead by using a GMM-based model
[<a href="#ref-ozerov07">191</a>] and a glottal source model
[<a href="#ref-klatt90">192</a>], and the accompaniment by using an instantaneous
mixture model [<a href="#ref-benaroya032">193</a>] leading to an NMF problem
[<a href="#ref-fevotte09">94</a>]. They jointly estimated the parameters of their
models by maximum likelihood estimation using an iterative algorithm
inspired by [<a href="#ref-dhillon05">194</a>] with multiplicative update rules
developed in [<a href="#ref-lee01">91</a>]. They also extracted the melody by
using an algorithm comparable to the Viterbi algorithm, before
re-estimating the parameters and finally performing source separation
using Wiener filters [<a href="#ref-benaroya06">195</a>]. In
[<a href="#ref-durrieu12">196</a>], they proposed to adapt their model for
user-guided source separation.</p><p>The joint modeling of the lead and accompaniment parts of a music signal
was also considered by Fuentes et al. in [<a href="#ref-fuentes2012">197</a>],
that introduced the idea of using a log-frequency TF representation
called the constant-Q transform (CQT)
[<a href="#ref-brown91">198</a>]–[<a href="#ref-schorkhuber10">200</a>]. The advantage
of such a representation is that a change in pitch corresponds to a
simple translation in the TF plane, instead of a scaling as in the STFT.
This idea was used along the creation of a user interface to guide the
decomposition, in line with what was done in [<a href="#ref-durrieu12">196</a>].</p><p>Joder and Schuller used the source-filter NMF model in
[<a href="#ref-durrieu11">201</a>], additionally exploiting MIDI scores
[<a href="#ref-joder12">202</a>]. They synchronized the MIDI scores to the audio
using the alignment algorithm in [<a href="#ref-joder11">203</a>]. They proposed
to exploit the score information through two types of constraints
applied in the model. In a first approach, they only made use of the
information regarding whether the leading voice is present or not in
each frame. In a second approach, they took advantage of both time and
pitch information on the aligned score.</p><p>Zhao et al. proposed a score-informed leading voice separation system
with a weighting scheme [<a href="#ref-zhao14">204</a>]. They extended the system
in [<a href="#ref-joder12">202</a>], which is based on the source-filter NMF
model in [<a href="#ref-durrieu11">201</a>], by using a Laplacian or a
Gaussian-based mask on the NMF activation matrix to enhance the
likelihood of the score-informed pitch candidates.</p><p>Jointly estimating accompaniment and lead allowed for some research in
correctly estimating the unvoiced parts of the lead, which is the main
issue with purely harmonic models, as highlighted in
Section <a href="#ssec:shortcomings_harmonics">3.3</a>. In
[<a href="#ref-durrieu11">201</a>], [<a href="#ref-durrieu092">205</a>], Durrieu et al.
extended their model to account for the unvoiced parts by adding white
noise components to the voice model.</p><p>In the same direction, Janer and Marxer proposed to separate unvoiced
fricative consonants using a semi-supervised NMF
[<a href="#ref-janer13">206</a>]. They extended the source-filter NMF model in
[<a href="#ref-durrieu11">201</a>] using a low-latency method with timbre
classification to estimate the predominant pitch
[<a href="#ref-marxer12">87</a>]. They approximated the fricative consonants as
an additive wideband component, training a model of NMF bases. They also
used the transient quality to differentiate between fricatives and
drums, after extracting transient time points using the method in
[<a href="#ref-janer12">207</a>].</p><p>Similarly, Marxer and Janer then proposed to separately model the
singing voice breathiness [<a href="#ref-marxer13">208</a>]. They estimated the
breathiness component by approximating the voice spectrum as a filtered
composition of a glottal excitation and a wideband component. They
modeled the magnitude of the voice spectrum using the model in
[<a href="#ref-degottex11">209</a>] and the envelope of the voice excitation
using the model in [<a href="#ref-klatt90">192</a>]. They estimated the pitch
using the method in [<a href="#ref-marxer12">87</a>]. This was all integrated
into the source-filter NMF model.</p><p>The body of research initiated by Durrieu et al. in
[<a href="#ref-durrieu08">188</a>] consists of using algebraic models more
sophisticated than one simple matrix product, but rather inspired by
musicological knowledge. Ozerov et al. formalized this idea through a
general framework and showed its application for singing voice
separation [<a href="#ref-ozerov102">210</a>]–[<a href="#ref-salaun14">212</a>].</p><p>Finally, Hennequin and Rigaud augmented their model to account for
long-term reverberation, with application to singing voice separation
[<a href="#ref-hennequin16">213</a>]. They extended the model in
[<a href="#ref-singh10">214</a>] which allows extraction of the reverberation of
a specific source with its dry signal. They combined this model with the
source-filter NMF model in [<a href="#ref-durrieu09">189</a>].</p><h3 id="different-constraints-for-different-sources"><a href="#different-constraints-for-different-sources" aria-hidden="true" class="header-anchor">#</a> Different constraints for different sources</h3><p>Algebraic methods that decompose the mixture spectrogram as the sum of
the lead and accompaniment spectrograms are based on the minimization of
a <em>cost</em> or <em>loss function</em> which measures the error between the
approximation and the observation. While the methods presented above for
lead and accompaniment separation did propose more sophisticated models
with parameters explicitly pertaining to the lead or the accompaniment,
another option that is also popular in the dedicated literature is to
modify the cost function of an optimization algorithm for an existing
algorithm (e.g., RPCA), so that one part of the resulting components
would preferentially account for one source or another.</p><p>This approach can be exemplified by the harmonic-percussive source
separation method (HPSS), presented in [<a href="#ref-ono08">160</a>],
[<a href="#ref-ono082">215</a>], [<a href="#ref-fitzgerald10">216</a>]. It consists in
filtering a mixture spectrogram so that horizontal lines go in a
so-called <em>harmonic</em> source, while its vertical lines go into a
<em>percussive</em> source. Separation is then done with TF masking. Of course,
such a method is not adequate for lead and accompaniment separation <em>per
se</em>, because all the harmonic content of the accompaniment is classified
as harmonic. However, it shows that <em>nonparametric</em> approaches are also
an option, provided the cost function itself is well chosen for each
source.</p><p>This idea was followed by Yang in [<a href="#ref-yang12">217</a>] who proposed an
approach based on RPCA with the incorporation of harmonicity priors and
a back-end drum removal procedure to improve the decomposition. He added
a regularization term in the algorithm to account for harmonic sounds in
the low-rank component and used an NMF-based model trained for drum
separation [<a href="#ref-ozerov12">211</a>] to eliminate percussive sounds in
the sparse component.</p><p>Jeong and Lee proposed to separate a vocal signal from a music signal
[<a href="#ref-jeong142">218</a>], extending the HPSS approach in
[<a href="#ref-ono08">160</a>], [<a href="#ref-ono082">215</a>]. Assuming that the
spectrogram of the signal can be represented as the sum of harmonic,
percussive, and vocal components, they derived an objective function
which enforces the temporal and spectral continuity of the harmonic and
percussive components, respectively, similarly to [<a href="#ref-ono08">160</a>],
but also the sparsity of the vocal component. Assuming non-negativity of
the components, they then derived iterative update rules to minimize the
objective function. Ochiai et al. extended this work in
[<a href="#ref-ochiai15">219</a>], notably by imposing harmonic constraints for
the lead.</p><p>Watanabe et al. extended RPCA for singing voice separation
[<a href="#ref-watanabe16">220</a>]. They added a harmonicity constraint in the
objective function to account for harmonic structures, such as in vocal
signals, and regularization terms to enforce the non-negativity of the
solution. They used the generalized forward-backward splitting algorithm
[<a href="#ref-raguet13">221</a>] to solve the optimization problem. They also
applied post-processing to remove the low frequencies in the vocal
spectrogram and built a TF mask to remove time frames with low energy.</p><p>Going beyond smoothness and harmonicity, Hayashi et al. proposed an NMF
with a constraint to help separate periodic components, such as a
repeating accompaniment [<a href="#ref-hayashi16">222</a>]. They defined a
periodicity constraint which they incorporated in the objective function
of the NMF algorithm to enforce the periodicity of the bases.</p><h3 id="cascaded-and-iterated-methods"><a href="#cascaded-and-iterated-methods" aria-hidden="true" class="header-anchor">#</a> Cascaded and iterated methods</h3><p>In their effort to propose separation methods for the lead and
accompaniment in music, some authors discovered that very different
methods often have complementary strengths. This motivated the
<em>combination</em> of methods. In practice, there are several ways to follow
this line of research.</p><p>One potential route to achieve better separation is to <em>cascade</em> several
methods. This is what FitzGerald and Gainza proposed in
[<a href="#ref-fitzgerald10">216</a>] with multiple median filters
[<a href="#ref-fitzgerald102">148</a>]. They used a median-filter based HPSS
approach at different frequency resolutions to separate a mixture into
harmonic, percussive, and vocal components. They also investigated the
use of STFT or CQT as the TF representation and proposed a
post-processing step to improve the separation with tensor factorization
techniques [<a href="#ref-fitzgerald09">223</a>] and non-negative partial
co-factorization [<a href="#ref-yoo10">180</a>].</p><p>The two-stage HPSS system proposed by Tachibana et al. in
[<a href="#ref-tachibana14">224</a>] proceeds the same way. It is an extension of
the melody extraction approach in [<a href="#ref-tachibana10">225</a>] and was
applied for karaoke in [<a href="#ref-tachibana16">226</a>]. It consists in using
the optimization-based HPSS algorithm from [<a href="#ref-ono08">160</a>],
[<a href="#ref-ono082">215</a>], [<a href="#ref-ono10">227</a>],
[<a href="#ref-tachibana12">228</a>] at different frequency resolutions to
separate the mixture into harmonic, percussive, and vocal components.</p><p><img src="https://docs.google.com/drawings/d/e/2PACX-1vSd3BGs_oCGRJlqPsH6O1ZDcbPPtc9ttABvNXvN8M5E2BggOcmToEVnVTmQPvAU4RMhCbm7MDerswoU/pub?w=392&h=499" alt></p><h4 id="cascading-source-separation-methods-the-results-from-method-a-is-improved-by-applying-methods-b-and-c-on-its-output-which-are-specialized-in-reducing-interferences-from-undesired-sources-in-each-signal"><a href="#cascading-source-separation-methods-the-results-from-method-a-is-improved-by-applying-methods-b-and-c-on-its-output-which-are-specialized-in-reducing-interferences-from-undesired-sources-in-each-signal" aria-hidden="true" class="header-anchor">#</a> Cascading source separation methods. The results from method A is improved by applying methods B and C on its output, which are specialized in reducing interferences from undesired sources in each signal.</h4><p>HPSS was not the only separation module considered as the building block
of combined lead and accompaniment separation approaches. Deif et al.
also proposed a multi-stage NMF-based algorithm [<a href="#ref-deif15">229</a>],
based on the approach in [<a href="#ref-zhu13">230</a>]. They used a local
spectral discontinuity measure to refine the non-pitched components
obtained from the factorization of the long window spectrogram and a
local temporal discontinuity measure to refine the non-percussive
components obtained from factorization of the short window spectrogram.</p><p>Finally, this cascading concept was considered again by Driedger and
Müller in [<a href="#ref-driedger15">231</a>], that introduces a processing
pipeline for the outputs of different methods [<a href="#ref-huang12">115</a>],
[<a href="#ref-virtanen08">164</a>], [<a href="#ref-driedger14">232</a>],
[<a href="#ref-talmon11">233</a>] to obtain an improved separation quality. Their
core idea is depicted in
Figure <a href="#fig:methods_cascading">[fig:methods_cascading]</a> and
combines the output of different methods in a specific order to improve
separation.</p><p>Another approach for improving the quality of separation when using
several separation procedures is not to restrict the number of such
iterations from one method to another, but rather to iterate them many
times until satisfactory results are obtained. This is what is proposed
in Hsu et al. in [<a href="#ref-hsu12">234</a>], extending the algorithm in
[<a href="#ref-hu10">235</a>]. They first estimated the pitch range of the
singing voice by using the HPSS method in [<a href="#ref-ono08">160</a>],
[<a href="#ref-tachibana10">225</a>]. They separated the voice given the
estimated pitch using a binary mask obtained by training a multilayer
perceptron [<a href="#ref-rumelhart86">236</a>] and re-estimated the pitch given
the separated voice. Voice separation and pitch estimation are then
iterated until convergence.</p><p>As another iterative method, Zhu et al. proposed a multi-stage NMF
[<a href="#ref-zhu13">230</a>], using harmonic and percussive separation at
different frequency resolutions similar to [<a href="#ref-tachibana10">225</a>]
and [<a href="#ref-fitzgerald10">216</a>]. The main originality of their
contribution was to iterate the refinements instead of applying it only
once.</p><p>An issue with such iterated methods lies in how to decide whether
convergence is obtained, and it is not clear whether the quality of the
separated signals will necessarily improve. For this reason, Bryan and
Mysore proposed a user-guided approach based on PLCA, which can be
applied for the separation of the vocals
[<a href="#ref-bryan13">237</a>]–[<a href="#ref-bryan133">239</a>]. They allowed a user
to make annotations on the spectrogram of a mixture, incorporated the
feedback as constraints in a PLCA model [<a href="#ref-smaragdis07">110</a>],
[<a href="#ref-raj05">156</a>], and used a posterior regularization technique
[<a href="#ref-ganchev10">240</a>] to refine the estimates, repeating the process
until the user is satisfied with the results. This is similar to the way
Ozerov et al. proposed to take user input into account in
[<a href="#ref-ozerov13">241</a>].</p><p><img src="https://docs.google.com/drawings/d/e/2PACX-1vT4NTF1Q8ctDDQGm_bui5NLHZczmdH8XGB9K8Wq9R9E93k9Ev-INRXKmDIVifcz8X4JZoGdcuij6U-F/pub?w=540" alt></p><h4 id="fusion-of-separation-methods-the-output-of-many-separation-methods-is-fed-into-a-fusion-system-that-combines-them-to-produce-a-single-estimate"><a href="#fusion-of-separation-methods-the-output-of-many-separation-methods-is-fed-into-a-fusion-system-that-combines-them-to-produce-a-single-estimate" aria-hidden="true" class="header-anchor">#</a> Fusion of separation methods. The output of many separation methods is fed into a fusion system that combines them to produce a single estimate.</h4><p>A principled way to aggregate the result of many source separation
systems to obtain one single estimate that is consistently better than
all of them was presented by Jaureguiberry et al. in their <em>fusion
framework</em>, depicted in
Figure <a href="#fig:methods_fusion">[fig:methods_fusion]</a>. It takes
advantage of multiple existing approaches, and demonstrated its
application to singing voice separation
[<a href="#ref-jaureguiberry13">242</a>]–[<a href="#ref-jaureguiberry16">244</a>]. They
investigated fusion methods based on non-linear optimization, Bayesian
model averaging [<a href="#ref-hoeting99">245</a>], and deep neural networks
(DNN).</p><p>As another attempt to design an efficient fusion method, McVicar et al.
proposed in [<a href="#ref-mcvicar16">246</a>] to combine the outputs of RPCA
[<a href="#ref-huang12">115</a>], HPSS [<a href="#ref-fitzgerald10">216</a>], Gabor
filtered spectrograms [<a href="#ref-jain90">247</a>], REPET
[<a href="#ref-rafii13">130</a>] and an approach based on deep learning
[<a href="#ref-huang14">248</a>]. To do this, they used different classification
techniques to build the aggregated TF mask, such as a logistic
regression model or a conditional random field (CRF) trained using the
method in [<a href="#ref-lacoste-julien13">249</a>] with time and/or frequency
dependencies.</p><p>Manilow et al. trained a neural network to predict quality of source
separation for three source separation algorithms, each leveraging a
different cue - repetition, spatialization, and harmonicity/pitch
proximity [<a href="#ref-manilow17">250</a>]. The method estimates separation
quality of the lead vocals for each algorithm, using only the original
audio mixture and separated source output. These estimates were used to
guide switching between algorithms along time.</p><h3 id="source-dependent-representations"><a href="#source-dependent-representations" aria-hidden="true" class="header-anchor">#</a> Source-dependent representations</h3><p>In the previous section, we stated that some authors considered
iterating separation at different frequency resolutions, i.e., using
different TF representations [<a href="#ref-fitzgerald10">216</a>],
[<a href="#ref-tachibana14">224</a>], [<a href="#ref-deif15">229</a>]. This can be seen as
a combination of different methods. However, this can also be seen from
another perspective as based on picking specific <em>representations</em>.</p><p>Wolf et al. proposed an approach using rigid motion segmentation, with
application to singing voice separation [<a href="#ref-wolf14">251</a>],
[<a href="#ref-wolf16">252</a>]. They introduced harmonic template models with
amplitude and pitch modulations defined by a velocity vector. They
applied a wavelet transform [<a href="#ref-anden14">253</a>] on the harmonic
template models to build an audio image where the amplitude and pitch
dynamics can be separated through the velocity vector. They then derived
a velocity equation, similar to the optical flow velocity equation used
in images [<a href="#ref-bernard01">254</a>], to segment velocity components.
Finally, they identified the harmonic templates which model different
sources in the mixture and separated them by approximating the velocity
field over the corresponding harmonic template models.</p><p>Yen et al. proposed an approach using spectro-temporal modulation
features [<a href="#ref-yen14">255</a>], [<a href="#ref-yen15">256</a>]. They decomposed a
mixture using a two-stage auditory model which consists of a cochlear
module [<a href="#ref-chi05">257</a>] and cortical module [<a href="#ref-chi99">258</a>].
They then extracted spectro-temporal modulation features from the TF
units and clustered the TF units into harmonic, percussive, and vocal
components using the EM algorithm and resynthesized the estimated
signals.</p><p>Chan and Yang proposed an approach using an informed group sparse
representation [<a href="#ref-chan17">259</a>]. They introduced a representation
built using a learned dictionary based on a chord sequence which
exhibits group sparsity [<a href="#ref-yuan06">260</a>] and which can incorporate
melody annotations. They derived a formulation of the problem in a
manner similar to RPCA and solved it using the alternating direction
method of multipliers [<a href="#ref-ma16">261</a>]. They also showed a relation
between their representation and the low-rank representation in
[<a href="#ref-yang13">123</a>], [<a href="#ref-liu13">262</a>].</p><h3 id="shortcomings-3"><a href="#shortcomings-3" aria-hidden="true" class="header-anchor">#</a> Shortcomings</h3><p>The large body of literature we reviewed in the preceding sections is
concentrated on choosing adequate models for the lead and accompaniment
parts of music signals in order to devise effective signal processing
methods to achieve separation. From a higher perspective, their common
feature is to guide the separation process in a <em>model-based way</em>:
first, the scientist has some idea regarding characteristics of the lead
signal and/or the accompaniment, and then an algorithm is designed to
exploit this knowledge for separation.</p><p>Model-based methods for lead and accompaniment separation are faced with
a common risk that their core assumptions will be violated for the
signal under study. For instance, the lead to be separated may not be
harmonic but saturated vocals or the accompaniment may not be repetitive
or redundant, but rather always changing. In such cases, model-based
methods are prone to large errors and poor performance.</p><h2 id="data-driven-approaches"><a href="#data-driven-approaches" aria-hidden="true" class="header-anchor">#</a> Data-driven approaches</h2><p>A way to address the potential caveats of model-based separation
behaving badly in case of violated assumptions is to avoid making
assumptions altogether, but rather to let the model be learned from a
large and representative database of examples. This line of research
leads to <em>data-driven</em> methods, for which researchers are concerned
about directly estimating a mapping between the mixture and either the
TF mask for separating the sources, or their spectrograms to be used for
designing a filter.</p><p>As may be foreseen, this strategy based on machine learning comes with
several challenges of its own. First, it requires considerable amounts
of data. Second, it typically requires a high-capacity learner (many
tunable parameters) that can be prone to over-fitting the training data
and therefore not working well on the audio it faces when deployed.</p><h3 id="algebraic-approaches"><a href="#algebraic-approaches" aria-hidden="true" class="header-anchor">#</a> Algebraic approaches</h3><p>A natural way to exploit a training database was to learn some parts of
the model to guide the estimation process into better solutions. Work on
this topic may be traced back to the suggestion of Ozerov et al. in
[<a href="#ref-ozerov05">276</a>] to learn spectral template models based on a
database of isolated sources, and then to adapt this dictionary of
templates on the mixture using the method in [<a href="#ref-tsai04">277</a>].</p><p>The exploitation of training data was formalized by Smaragdis et al. in
[<a href="#ref-smaragdis07">110</a>] in the context of source separation within
the supervised and semi-supervised PLCA framework. The core idea of this
probabilistic formulation, equivalent to NMF, is to learn some spectral
bases from the training set which are then kept fixed at separation
time.</p><p>In the same line, Ozerov et al. proposed an approach using Bayesian
models [<a href="#ref-ozerov07">191</a>]. They first segmented a song into vocal
and non-vocal parts using GMMs with MFCCs. Then, they adapted a general
music model on the non-vocal parts of a particular song by using the
maximum a posteriori (MAP) adaptation approach in
[<a href="#ref-gauvain94">278</a>]</p><p>Ozerov et al. later proposed a framework for source separation which
generalizes several approaches given prior information about the problem
and showed its application for singing voice separation
[<a href="#ref-ozerov102">210</a>]–[<a href="#ref-salaun14">212</a>]. They chose the local
Gaussian model [<a href="#ref-vincent10">279</a>] as the core of the framework
and allowed the prior knowledge about each source and its mixing
characteristics using user-specified constraints. Estimation was
performed through a generalized EM algorithm [<a href="#ref-dempster77">32</a>].</p><p>Rafii et al. proposed in [<a href="#ref-rafii132">280</a>] to address the main
drawback of the repetition-based methods described in
Section <a href="#ssec:methods_repet">4.3</a>, which is the weakness of the model
for vocals. For this purpose, they combined the REPET-SIM model
[<a href="#ref-rafii12">135</a>] for the accompaniment with a NMF-based model for
singing voice learned from a voice dataset.</p><p>As yet another example of using training data for NMF,
Boulanger-Lewandowski et al. proposed in
[<a href="#ref-boulanger-lewandowski14">281</a>] to exploit long-term temporal
dependencies in NMF, embodied using recurrent neural networks (RNN)
[<a href="#ref-rumelhart86">236</a>]. They incorporated RNN regularization into
the NMF framework to temporally constrain the activity matrix during the
decomposition, which can be seen as a generalization of the non-negative
HMM in [<a href="#ref-mysore10">282</a>]. Furthermore, they used supervised and
semi-supervised NMF algorithms on isolated sources to train the models,
as in [<a href="#ref-smaragdis07">110</a>].</p><h3 id="deep-neural-networks"><a href="#deep-neural-networks" aria-hidden="true" class="header-anchor">#</a> Deep neural networks</h3><p><img src="https://docs.google.com/drawings/d/e/2PACX-1vRXc_l4uNUTSOoAif8r4O-AKfAVBBMUSPVG_VMu79LjcZLb4xKLgFTVoSqVodvGetEvdeakfb4Nul-3/pub?w=720" alt></p><h5 id="general-architecture-for-methods-exploiting-deep-learning-the-network-inputs-the-mixture-and-outputs-either-the-sources-spectrograms-or-a-tf-mask-methods-usually-differ-in-their-choice-for-a-network-architecture-and-the-way-it-is-learned-using-the-training-data"><a href="#general-architecture-for-methods-exploiting-deep-learning-the-network-inputs-the-mixture-and-outputs-either-the-sources-spectrograms-or-a-tf-mask-methods-usually-differ-in-their-choice-for-a-network-architecture-and-the-way-it-is-learned-using-the-training-data" aria-hidden="true" class="header-anchor">#</a> General architecture for methods exploiting deep learning. The network inputs the mixture and outputs either the sources spectrograms or a TF mask. Methods usually differ in their choice for a network architecture and the way it is learned using the training data.</h5><p>Taking advantage of the recent availability of sufficiently large
databases of isolated vocals along with their accompaniment, several
researchers investigated the use of machine learning methods to directly
estimate a mapping between the mixture and the sources. Although
end-to-end systems inputting and outputting the waveforms have already
been proposed in the speech community [<a href="#ref-qian17">283</a>], they are
not yet available for music source separation. This may be due to the
relative small size of music separation databases, at most 10 h today.
Instead, most systems feature pre and post-processing steps that consist
in computing classical TF representations and building TF masks,
respectively. Although such end-to-end systems will inevitably be
proposed in the near future, the common structure of deep learning
methods for lead and accompaniment separation usually corresponds for
now to the one depicted in
Figure <a href="#fig:methods_dnn">[fig:methods_dnn]</a>. From a general
perspective, we may say that most current methods mainly differ in the
structure picked for the network, as well as in the way it is learned.</p><p>Providing a thorough introduction to deep neural networks is out of the
scope of this paper. For our purpose, it suffices to mention that they
consist of a cascade of several possibly non-linear transformations of
the input, which are learned during a training stage. They were shown to
effectively learn representations and mappings, provided enough data is
available for estimating their parameters
[<a href="#ref-deng14">284</a>]–[<a href="#ref-goodfellow16">286</a>]. Different
architectures for neural networks may be combined/cascaded together, and
many architectures were proposed in the past, such as feedforward
fully-connected neural networks (FNN), convolutional neural networks
(CNN), or RNN and variants such as the long short-term memory (LSTM) and
the gated-recurrent units (GRU). Training of such functions is achieved
by stochastic gradient descent [<a href="#ref-robbins51">287</a>] and associated
algorithms, such as backpropagation [<a href="#ref-rumelhart862">288</a>] or
backpropagation through time [<a href="#ref-rumelhart86">236</a>] for the case of
RNNs.</p><p>To the best of our knowledge, Huang et al. were the first to propose
deep neural networks, RNNs here [<a href="#ref-hermans13">289</a>],
[<a href="#ref-pascanu14">290</a>], for singing voice separation in
[<a href="#ref-huang14">248</a>], [<a href="#ref-huang15">291</a>]. They adapted their
framework from [<a href="#ref-huang142">292</a>] to model all sources
simultaneously through masking. Input and target functions were the
mixture magnitude and a joint representation of the individual sources.
The objective was to estimate jointly either singing voice and
accompaniment music, or speech and background noise from the
corresponding mixtures.</p><p>Modeling the temporal structures of both the lead and the accompaniment
is a considerable challenge, even when using DNN methods. As an
alternative to the RNN approach proposed by Huang et al. in
[<a href="#ref-huang14">248</a>], Uhlich et al. proposed the usage of FNNs
[<a href="#ref-uhlich15">293</a>] whose input consists of <em>supervectors</em> of a few
consecutive frames from the mixture spectrogram. Later in
[<a href="#ref-uhlich17">294</a>], the same authors considered the use of
bi-directional LSTMs for the same task.</p><p>In an effort to make the resulting system less computationally demanding
at separation time but still incorporating dynamic modeling of audio,
Simpson et al. proposed in [<a href="#ref-simpson15">295</a>] to predict binary
TF masks using deep CNNs, which typically utilize fewer parameters than
the FNNs. Similarly, Schlueter proposed a method trained to detect
singing voice using CNNs [<a href="#ref-schlueter16">296</a>]. In that case, the
trained network was used to compute <em>saliency maps</em> from which TF masks
can be computed for singing voice separation. Chandna et al. also
considered CNNs for lead separation in [<a href="#ref-chandna17">297</a>], with a
particular focus on low-latency.</p><p>The classical FNN, LSTM and CNN structures above served as baseline
structures over which some others tried to improve. As a first example,
Mimilakis et al. proposed to use a hybrid structure of FNNs with skip
connections to separate the lead instrument for purposes of remixing
jazz recordings [<a href="#ref-mimilakis16">298</a>]. Such skip connections allow
to propagate the input spectrogram to intermediate representations
within the network, and mask it similarly to the operation of TF masks.
As advocated, this enforces the networks to approximate a TF masking
process. Extensions to temporal data for singing voice separation were
presented in [<a href="#ref-mimilakis17">299</a>], [<a href="#ref-mimilakis172">300</a>].
Similarly, Jansson et al. proposed to propagate the spectral information
computed by convolutional layers to intermediate representations
[<a href="#ref-jansson17">301</a>]. This propagation aggregates intermediate
outputs to proceeding layer(s). The output of the last layer is
responsible for masking the input mixture spectrogram. In the same vein,
Takahashi et al. proposed to use skip connections via element-wise
addition through representations computed by
CNNs [<a href="#ref-takahashi17">302</a>].</p><p>Apart from the structure of the network, the way it is trained,
comprising how the targets are computed, has a tremendous impact on
performance. As we saw, most methods operate on defining TF masks or
estimating magnitude spectrograms. However, other methods were proposed
based on deep clustering [<a href="#ref-hershey16">303</a>],
[<a href="#ref-isik16">304</a>], where TF mask estimation is seen as a clustering
problem. Luo et al. investigated both approaches in
[<a href="#ref-luo17">305</a>] by proposing deep bidirectional LSTM networks
capable of outputting both TF masks or features to use as in deep
clustering. Kim and Smaragdis proposed in [<a href="#ref-kim15">306</a>] another
way to learn the model, in a denoising auto-encoding fashion
[<a href="#ref-vincentp10">307</a>], again utilizing short segments of the
mixture spectrogram as an input to the network, as in
[<a href="#ref-uhlich15">293</a>].</p><p>As the best network structure may vary from one track to another, some
authors considered a fusion of methods, in a manner similar to the
method [<a href="#ref-jaureguiberry13">242</a>] presented above. Grais et.
al [<a href="#ref-grais16">308</a>], [<a href="#ref-grais162">309</a>] proposed to
aggregate the results from an ensemble of feedforward DNNs to predict TF
masks for separation. An improvement was presented in
[<a href="#ref-grais17">310</a>], [<a href="#ref-grais172">311</a>] where the inputs to
the fusion network were separated signals, instead of TF masks, aiming
at enhancing the reconstruction of the separated sources.</p><p>As can be seen the use of deep learning methods for the design of lead
and accompaniment separation has already stimulated a lot of research,
although it is still in its infancy. Interestingly, we also note that
using audio and music specific knowledge appears to be fundamental in
designing effective systems. As an example of this, the contribution
from Nie et al. in [<a href="#ref-nie15">312</a>] was to include the construction
of the TF mask as an extra non-linearity included in a recurrent
network. This is an exemplar of where signal processing elements, such
as filtering through masking, are incorporated as a building block of
the machine learning method.</p><p>The network structure is not the only thing that can benefit from audio
knowledge for better separation. The design of appropriate features is
another. While we saw that supervectors of spectrogram patches offered
the ability to effectively model time-context information in
FNNs [<a href="#ref-uhlich15">293</a>], Sebastian and
Murthy [<a href="#ref-sebastian16">313</a>] proposed the use of the modified
group delay feature representation [<a href="#ref-yegnanarayana91">314</a>] in
their deep RNN architecture. They applied their approach for both
singing voice and vocal-violin separation.</p><p>Finally, as with other methods, DNN-based separation techniques can also
be combined with others to yield improved performance. As an example,
Fan et al. proposed to use DNNs to separate the singing voice and to
also exploit vocal pitch estimation [<a href="#ref-fan16">315</a>]. They first
extracted the singing voice using feedforward DNNs with sigmoid
activation functions. They then estimated the vocal pitch from the
extracted singing voice using dynamic programming.</p><h3 id="shortcomings-4"><a href="#shortcomings-4" aria-hidden="true" class="header-anchor">#</a> Shortcomings</h3><p>Data-driven methods are nowadays the topic of important research
efforts, particularly those based on DNNs. This is notably due to their
impressive performance in terms of separation quality, as can, for
instance, be noticed below in Section <a href="#sec:evaluation">8</a>. However,
they also come with some limitations.</p><p>First, we highlighted that lead and accompaniment separation in music
has the very specific problem of scarce data. Since it is very hard to
gather large amounts of training data for that application, it is hard
to fully exploit learning methods that require large training sets. This
raises very specific challenges in terms of machine learning.</p><p>Second, the lack of interpretability of model parameters is often
mentioned as a significant shortcoming when it comes to applications.
Indeed, music engineering systems are characterized by a strong
importance of human-computer interactions, because they are used in an
artistic context that may require specific needs or results. As of
today, it is unclear how to provide user interaction for controlling the
millions of parameters of DNN-based systems.</p><h2 id="including-multichannel-information"><a href="#including-multichannel-information" aria-hidden="true" class="header-anchor">#</a> Including multichannel information</h2><p>In describing the above methods, we have not discussed the fact that
music signals are typically stereophonic. On the contrary, the bulk of
methods we discussed focused on designing good spectrogram models for
the purpose of filtering mixtures that may be <em>monophonic</em>. Such a
strategy is called <em>single-channel</em> source separation and is usually
presented as more challenging than multichannel source separation.
Indeed, only TF structure may then be used to discriminate the
accompaniment from the lead. In stereo recordings, one further so-called
<em>spatial</em> dimension is introduced, which is sometimes referred to as
<em>pan</em>, that corresponds to the perceived <em>position</em> of a source in the
stereo field. Devising methods to exploit this spatial diversity for
source separation has also been the topic of an important body of
research that we review now.</p><h3 id="extracting-the-lead-based-on-panning"><a href="#extracting-the-lead-based-on-panning" aria-hidden="true" class="header-anchor">#</a> Extracting the lead based on panning</h3><p>In the case of popular music signals, a fact of paramount practical
importance is that the lead signal — such as vocals — is very often
mixed <em>in the center</em>, which means that its energy is approximately the
same in left and right channels. On the contrary, other instruments are
often mixed at positions to the left or right of the stereo field.</p><p><img src="https://docs.google.com/drawings/d/e/2PACX-1vT6DxnIs8i-BCZblC-rKuOOvI7ZRnlykXKiBznkuYrPbMANQohbPaF9vsM73J3Oobew7z3211WpaeS5/pub?w=720" alt></p><h5 id="separation-of-the-lead-based-on-panning-information-a-stereo-cue-called-panning-allows-to-design-a-tf-mask"><a href="#separation-of-the-lead-based-on-panning-information-a-stereo-cue-called-panning-allows-to-design-a-tf-mask" aria-hidden="true" class="header-anchor">#</a> Separation of the lead based on panning information. A stereo cue called panning allows to design a TF mask.</h5><p>The general structure of methods extracting the lead based on stereo
cues is displayed on
Figure <a href="#fig:methods_panning">[fig:methods_panning]</a>, introduced by
Avendano, who proposed to separate sources in stereo mixtures by using a
panning index [<a href="#ref-avendano03">316</a>]. He derived a two-dimensional
map by comparing left and right channels in the TF domain to identify
the different sources based on their panning position
[<a href="#ref-avendano02">317</a>]. The same methodology was considered by Barry
et al. in [<a href="#ref-barry04">318</a>] in his Azimuth Discrimination and
Resynthesis (ADRess) approach, with panning indexes computed with
differences instead of ratios.</p><p>Vinyes et al. also proposed to unmix commercially produced music
recordings thanks to stereo cues [<a href="#ref-vinyes06">319</a>]. They designed
an interface similar to [<a href="#ref-barry04">318</a>] where a user can set
some parameters to generate different TF filters in real time. They
showed applications for extracting various instruments, including
vocals.</p><p>Cobos and López proposed to separate sources in stereo mixtures by using
TF masking and multilevel thresholding [<a href="#ref-cobos082">320</a>]. They
based their approach on the Degenerate Unmixing Estimation Technique
(DUET) [<a href="#ref-yilmaz04">321</a>]. They first derived histograms by
measuring the amplitude relationship between TF points in left and right
channels. Then, they obtained several thresholds using the multilevel
extension of Otsu’s method [<a href="#ref-otsu79">322</a>]. Finally, TF points
were assigned to their related sources to produce TF masks.</p><p>Sofianos et al. proposed to separate the singing voice from a stereo
mixture using ICA [<a href="#ref-sofianos10">323</a>]–[<a href="#ref-sofianos12">325</a>].
They assumed that most commercial songs have the vocals panned to the
center and that they dominate the other sources in amplitude. In
[<a href="#ref-sofianos10">323</a>], they proposed to combine a modified version
of ADRess with ICA to filter out the other instruments. In
[<a href="#ref-sofianos102">324</a>], they proposed a modified version without
ADRess.</p><p>Kim et al. proposed to separate centered singing voice in stereo music
by exploiting binaural cues, such as inter-channel level and
inter-channel phase difference [<a href="#ref-kim11">326</a>]. To this end, they
build the pan-based TF mask through an EM algorithm, exploiting a GMM
model on these cues.</p><h3 id="augmenting-models-with-stereo"><a href="#augmenting-models-with-stereo" aria-hidden="true" class="header-anchor">#</a> Augmenting models with stereo</h3><p>As with using only a harmonic model for the lead signal, using stereo
cues in isolation is not always sufficient for good separation, as there
can often be multiple sources at the same spatial location. Combining
stereo cues with other methods improves performance in these cases.</p><p>Cobos and López proposed to extract singing voice by combining panning
information and pitch tracking [<a href="#ref-cobos08">327</a>]. They first
obtained an estimate for the lead thanks to a pan-based method such as
[<a href="#ref-avendano03">316</a>], and refined the singing voice by using a TF
binary mask based on comb-filtering method as in
Section <a href="#ssec:harmonicity-combfiltering">3.2</a>. The same combination
was proposed by Marxer et al. in [<a href="#ref-marxer12">87</a>] in a
low-latency context, with different methods used for the binaural cues
and pitch tracking blocks.</p><p>FitzGerald proposed to combine approaches based on repetition and
panning to extract stereo vocals [<a href="#ref-fitzgerald13">328</a>]. He first
used his nearest neighbors median filtering algorithm
[<a href="#ref-fitzgerald12">139</a>] to separate vocals and accompaniment from a
stereo mixture. He then used the ADRess algorithm
[<a href="#ref-barry04">318</a>] and a high-pass filter to refine the vocals and
improve the accompaniment. In a somewhat different manner, FitzGerald
and Jaiswal also proposed to combine approaches based on repetition and
panning to improve stereo accompaniment recovery
[<a href="#ref-fitzgerald132">329</a>]. They presented an audio inpainting scheme
[<a href="#ref-alder12">330</a>] based on the nearest neighbors and median
filtering algorithm [<a href="#ref-fitzgerald12">139</a>] to recover TF regions
of the accompaniment assigned to the vocals after using a source
separation algorithm based on panning information.</p><p>In a more theoretically grounded manner, several methods based on a
probabilistic model were generalized to the multichannel case. For
instance, Durrieu et al. extended their source-filter model in
[<a href="#ref-durrieu11">201</a>], [<a href="#ref-durrieu092">205</a>] to handle stereo
signals, by incorporating the panning coefficients as model parameters
to be estimated.</p><p>Ozerov and Févotte proposed a multichannel NMF framework with
application to source separation, including vocals and music
[<a href="#ref-ozerov09">331</a>], [<a href="#ref-ozerov10">332</a>]. They adopted a
statistical model where each source is represented as a sum of Gaussian
components [<a href="#ref-benaroya032">193</a>], and where maximum likelihood
estimation of the parameters is equivalent to NMF with the Itakura-Saito
divergence [<a href="#ref-fevotte09">94</a>]. They proposed two methods for
estimating the parameters of their model, one that maximized the
likelihood of the multichannel data using EM, and one that maximized the
sum of the likelihoods of all channels using a multiplicative update
algorithm inspired by NMF [<a href="#ref-lee99">90</a>].</p><p>Ozerov et al. then proposed a multichannel non-negative tensor
factorization (NTF) model with application to user-guided source
separation [<a href="#ref-ozerov11">333</a>]. They modeled the sources jointly by
a 3-valence tensor (time/frequency/source) as in
[<a href="#ref-liutkus10">334</a>] which extends the multichannel NMF model in
[<a href="#ref-ozerov10">332</a>]. They used a generalized EM algorithm based on
multiplicative updates [<a href="#ref-fevotte10">335</a>] to minimize the
objective function. They incorporated information about the temporal
segmentation of the tracks and the number of components per track.
Ozerov et al. later proposed weighted variants of NMF and NTF with
application to user-guided source separation, including separation of
vocals and music [<a href="#ref-ozerov13">241</a>], [<a href="#ref-ozerov14">336</a>].</p><p>Sawada et al. also proposed multichannel extensions of NMF, tested for
separating stereo mixtures of multiple sources, including vocals and
accompaniment [<a href="#ref-sawada11">337</a>]–[<a href="#ref-sawada13">339</a>]. They
first defined multichannel extensions of the cost function, namely,
Euclidean distance and Itakura-Saito divergence, and derived
multiplicative update rules accordingly. They then proposed two
techniques for clustering the bases, one built into the NMF model and
one performing sequential pair-wise merges.</p><p>Finally, multichannel information was also used with DNN models. Nugraha
et al. addressed the problem of multichannel source separation for
speech enhancement [<a href="#ref-sivasankaran15">340</a>],
[<a href="#ref-nugraha162">341</a>] and music separation
[<a href="#ref-nugraha15">342</a>], [<a href="#ref-nugraha16">343</a>]. In this framework,
DNNs are still used for the spectrograms, while more classical EM
algorithms [<a href="#ref-duong10">344</a>], [<a href="#ref-ozerov112">345</a>] are used
for estimating the spatial parameters.</p><h3 id="shortcomings-5"><a href="#shortcomings-5" aria-hidden="true" class="header-anchor">#</a> Shortcomings</h3><p>When compared to simply processing the different channels independently,
incorporating spatial information in the separation method often comes
at the cost of additional computational complexity. The resulting
methods are indeed usually more demanding in terms of computing power,
because they involve the design of beamforming filters and inversion of
covariance matrices. While this is not really an issue for stereophonic
music, this may become prohibiting in configurations with higher numbers
of channels</p><h2 id="references"><a href="#references" aria-hidden="true" class="header-anchor">#</a> References</h2><div id="refs" class="references"><div id="ref-kalakota00"><p>[1] R. Kalakota and M. Robinson, <em>E-business 2.0: Roadmap for
success</em>. Addison-Wesley Professional, 2000.</p></div><div id="ref-lam01"><p>[2] C. K. Lam and B. C. Tan, “The Internet is changing the music
industry,” <em>Communications of the ACM</em>, vol. 44, no. 8, pp. 62–68, 2001.</p></div><div id="ref-common10"><p>[3] P. Common and C. Jutten, <em>Handbook of blind source separation</em>.
Academic Press, 2010.</p></div><div id="ref-naik14"><p>[4] G. R. Naik and W. Wang, <em>Blind source separation</em>. Springer-Verlag
Berlin Heidelberg, 2014.</p></div><div id="ref-hyvarinen99"><p>[5] A. Hyvärinen, “Fast and robust fixed-point algorithm for
independent component analysis,” <em>IEEE Transactions on Neural Networks</em>,
vol. 10, no. 3, pp. 626–634, May 1999.</p></div><div id="ref-hyvarinen00"><p>[6] A. Hyvärinen and E. Oja, “Independent component analysis:
Algorithms and applications,” <em>Neural Networks</em>, vol. 13, nos. 4-5, pp.
411–430, Jun. 2000.</p></div><div id="ref-makino07"><p>[7] S. Makino, T.-W. Lee, and H. Sawada, <em>Blind speech separation</em>.
Springer Netherlands, 2007.</p></div><div id="ref-vincent18"><p>[8] E. Vincent, T. Virtanen, and S. Gannot, <em>Audio source separation
and speech enhancement</em>. Wiley, 2018.</p></div><div id="ref-loizou13"><p>[9] P. C. Loizou, <em>Speech enhancement: Theory and practice</em>. CRC
Press, 1990.</p></div><div id="ref-liutkus13"><p>[10] A. Liutkus, J.-L. Durrieu, L. Daudet, and G. Richard, “An
overview of informed audio source separation,” in <em>14th international
workshop on image analysis for multimedia interactive services</em>, 2013.</p></div><div id="ref-vincent14"><p>[11] E. Vincent, N. Bertin, R. Gribonval, and F. Bimbot, “From blind
to guided audio source separation: How models and side information can
improve the separation of sound,” <em>IEEE Signal Processing Magazine</em>,
vol. 31, no. 3, pp. 107–115, May 2014.</p></div><div id="ref-zolzer11"><p>[12] U. Zölzer, <em>DAFX - digital audio effects</em>. Wiley, 2011.</p></div><div id="ref-muller2015"><p>[13] M. Müller, <em>Fundamentals of music processing: Audio, analysis,
algorithms, applications</em>. Springer, 2015.</p></div><div id="ref-jaynes2003probability"><p>[14] E. T. Jaynes, <em>Probability theory: The logic of science</em>.
Cambridge university press, 2003.</p></div><div id="ref-cappe2005"><p>[15] O. Cappé, E. Moulines, and T. Ryden, <em>Inference in hidden markov
models (springer series in statistics)</em>. Secaucus, NJ, USA:
Springer-Verlag New York, Inc., 2005.</p></div><div id="ref-mcaulay86"><p>[16] R. J. McAulay and T. F. Quatieri, “Speech analysis/synthesis
based on a sinusoidal representation,” <em>IEEE Transactions on Audio,
Speech, and Language Processing</em>, vol. 34, no. 4, pp. 744–754, Aug.
1986.</p></div><div id="ref-rickard02"><p>[17] S. Rickard and O. Yilmaz, “On the approximate w-disjoint
orthogonality of speech,” in <em>IEEE international conference on
acoustics, speech, and signal processing</em>, 2002.</p></div><div id="ref-boll1979"><p>[18] S. Boll, “Suppression of acoustic noise in speech using spectral
subtraction,” <em>IEEE Transactions on acoustics, speech, and signal
processing</em>, vol. 27, no. 2, pp. 113–120, 1979.</p></div><div id="ref-wiener1975"><p>[19] N. Wiener, “Extrapolation, interpolation, and smoothing of
stationary time series,” 1975.</p></div><div id="ref-liutkus15c"><p>[20] A. Liutkus and R. Badeau, “Generalized Wiener filtering with
fractional power spectrograms,” in <em>IEEE international conference on
acoustics, speech and signal processing</em>, 2015.</p></div><div id="ref-fant70"><p>[21] G. Fant, <em>Acoustic theory of speech production</em>. Walter de
Gruyter, 1970.</p></div><div id="ref-bogert1963"><p>[22] B. P. Bogert, M. J. R. Healy, and J. W. Tukey, “The quefrency
alanysis of time series for echoes: Cepstrum pseudo-autocovariance,
cross-cepstrum, and saphe cracking,” <em>Proceedings of a symposium on time
series analysis</em>, pp. 209–243, 1963.</p></div><div id="ref-noll64"><p>[23] A. M. Noll, “Short-time spectrum and ‘cepstrum’ techniques for
vocal-pitch detection,” <em>Journal of the Acoustical Society of America</em>,
vol. 36, no. 2, pp. 296–302, 1964.</p></div><div id="ref-noll67"><p>[24] A. M. Noll, “Cepstrum pitch determination,” <em>Journal of the
Acoustical Society of America</em>, vol. 41, no. 2, pp. 293–309, 1967.</p></div><div id="ref-david80"><p>[25] S. B. Davis and P. Mermelstein, “Comparison of parametric
representations for monosyllabic word recognition in continuously spoken
sentences,” <em>IEEE Transactions on Audio, Speech, and Language
Processing</em>, vol. 28, no. 4, pp. 357–366, Aug. 1980.</p></div><div id="ref-oppenheim69"><p>[26] A. V. Oppenheim, “Speech analysis-synthesis system based on
homomorphic filtering,” <em>Journal of the Acoustical Society of America</em>,
vol. 45, no. 2, pp. 458–465, 1969.</p></div><div id="ref-durrett2010probability"><p>[27] R. Durrett, <em>Probability: Theory and examples</em>. Cambridge
university press, 2010.</p></div><div id="ref-schwarz78"><p>[28] G. Schwarz, “Estimating the dimension of a model,” <em>Annals of
Statistics</em>, vol. 6, no. 2, pp. 461–464, Mar. 1978.</p></div><div id="ref-rabiner89"><p>[29] L. R. Rabiner, “A tutorial on hidden Markov models and selected
applications in speech recognition,” <em>Proceedings of the IEEE</em>, vol. 77,
no. 2, pp. 257–286, Feb. 1989.</p></div><div id="ref-viterbi2006"><p>[30] A. J. Viterbi, “A personal history of the Viterbi algorithm,”
<em>IEEE Signal Processing Magazine</em>, vol. 23, no. 4, pp. 120–142, 2006.</p></div><div id="ref-bishop96"><p>[31] C. Bishop, <em>Neural networks for pattern recognition</em>. Clarendon
Press, 1996.</p></div><div id="ref-dempster77"><p>[32] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood
from incomplete data via the EM algorithm,” <em>Journal of the Royal
Statistical Society</em>, vol. 39, no. 1, pp. 1–38, 1977.</p></div><div id="ref-salamon14"><p>[33] J. Salamon, E. Gómez, D. Ellis, and G. Richard, “Melody
extraction from polyphonic music signals: Approaches, applications and
challenges,” <em>IEEE Signal Processing Magazine</em>, vol. 31, 2014.</p></div><div id="ref-miller73"><p>[34] N. J. Miller, “Removal of noise from a voice signal by
synthesis,” Utah University, 1973.</p></div><div id="ref-oppenheim68"><p>[35] A. V. Oppenheim and R. W. Schafer, “Homomorphic analysis of
speech,” <em>IEEE Transactions on Audio and Electroacoustics</em>, vol. 16, no.
2, pp. 221–226, Jun. 1968.</p></div><div id="ref-maher89"><p>[36] R. C. Maher, “An approach for the separation of voices in
composite musical signals,” PhD thesis, University of Illinois at
Urbana-Champaign, 1989.</p></div><div id="ref-wang94"><p>[37] A. L. Wang, “Instantaneous and frequency-warped techniques for
auditory source separation,” PhD thesis, Stanford University, 1994.</p></div><div id="ref-wang95"><p>[38] A. L. Wang, “Instantaneous and frequency-warped techniques for
source separation and signal parametrization,” in <em>IEEE workshop on
applications of signal processing to audio and acoustics</em>, 1995.</p></div><div id="ref-meron98"><p>[39] Y. Meron and K. Hirose, “Separation of singing and piano sounds,”
in <em>5th international conference on spoken language processing</em>, 1998.</p></div><div id="ref-quatieri92"><p>[40] T. F. Quatieri, “Shape invariant time-scale and pitch
modification of speech,” <em>IEEE Transactions on Signal Processing</em>, vol.
40, no. 3, pp. 497–510, Mar. 1992.</p></div><div id="ref-ben-shalom04"><p>[41] A. Ben-Shalom and S. Dubnov, “Optimal filtering of an instrument
sound in a mixed recording given approximate pitch prior,” in
<em>International computer music conference</em>, 2004.</p></div><div id="ref-shalev-shwartz02"><p>[42] S. Shalev-Shwartz, S. Dubnov, N. Friedman, and Y. Singer, “Robust
temporal and spectral modeling for query by melody,” in <em>25th annual
international acm sigir conference on research and development in
information retrieval</em>, 2002.</p></div><div id="ref-serra97"><p>[43] X. Serra, “Musical sound modeling with sinusoids plus noise,” in
<em>Musical signal processing</em>, Swets &amp; Zeitlinger, 1997, pp. 91–122.</p></div><div id="ref-vanveen97"><p>[44] B. V. Veen and K. M. Buckley, “Beamforming techniques for spatial
filtering,” in <em>The digital signal processing handbook</em>, CRC Press,
1997, pp. 1–22.</p></div><div id="ref-zhang05"><p>[45] Y.-G. Zhang and C.-S. Zhang, “Separation of voice and music by
harmonic structure stability analysis,” in <em>IEEE international
conference on multimedia and expo</em>, 2005.</p></div><div id="ref-zhang06"><p>[46] Y.-G. Zhang and C.-S. Zhang, “Separation of music signals by
harmonic structure modeling,” in <em>Advances in neural information
processing systems 18</em>, MIT Press, 2006, pp. 1617–1624.</p></div><div id="ref-terhardt79"><p>[47] E. Terhardt, “Calculating virtual pitch,” <em>Hearing Research</em>,
vol. 1, no. 2, pp. 155–182, Mar. 1979.</p></div><div id="ref-zhang03"><p>[48] Y.-G. Zhang, C.-S. Zhang, and S. Wang, “Clustering in knowledge
embedded space,” in <em>Machine learning: ECML 2003</em>, Springer Berlin
Heidelberg, 2003, pp. 480–491.</p></div><div id="ref-fujihara05"><p>[49] H. Fujihara, T. Kitahara, M. Goto, K. Komatani, T. Ogata, and H.
G. Okuno, “Singer identification based on accompaniment sound reduction
and reliable frame selection,” in <em>6th international conference on music
information retrieval</em>, 2005.</p></div><div id="ref-fujihara10"><p>[50] H. Fujihara, M. Goto, T. Kitahara, and H. G. Okuno, “A modeling
of singing voice robust to accompaniment sounds and its application to
singer identification and vocal-timbre-similarity-based music
information retrieval,” <em>IEEE Transactions on Audio, Speech, and
Language Processing</em>, vol. 18, no. 3, pp. 638–648, Mar. 2010.</p></div><div id="ref-goto04"><p>[51] M. Goto, “A real-time music-scene-description system:
Predominant-F0 estimation for detecting melody and bass lines in
real-world audio signals,” <em>Speech Communication</em>, vol. 43, no. 4, pp.
311–329, Sep. 2004.</p></div><div id="ref-moorer05"><p>[52] J. A. Moorer, “Signal processing aspects of computer music: A
survey,” <em>Proceedings of the IEEE</em>, vol. 65, no. 8, pp. 1108–1137, Aug.
2005.</p></div><div id="ref-mesaros07"><p>[53] A. Mesaros, T. Virtanen, and A. Klapuri, “Singer identification
in polyphonic music using vocal separation and pattern recognition
methods,” in <em>7th international conference on music information
retrieval</em>, 2007.</p></div><div id="ref-ryynanen06"><p>[54] M. Ryynänen and A. Klapuri, “Transcription of the singing melody
in polyphonic music,” in <em>7th international conference on music
information retrieval</em>, 2006.</p></div><div id="ref-duan08"><p>[55] Z. Duan, Y.-F. Zhang, C.-S. Zhang, and Z. Shi, “Unsupervised
single-channel music source separation by average harmonic structure
modeling,” <em>IEEE Transactions on Audio, Speech, and Language
Processing</em>, vol. 16, no. 4, pp. 766–778, May 2008.</p></div><div id="ref-rodet97"><p>[56] X. Rodet, “Musical sound signal analysis/synthesis:
Sinusoidal+Residual and elementary waveform models,” in <em>IEEE
time-frequency and time-scale workshop</em>, 1997.</p></div><div id="ref-smith87"><p>[57] J. O. Smith and X. Serra, “PARSHL: An analysis/synthesis program
for non-harmonic sounds based on a sinusoidal representation,” in
<em>International computer music conference</em>, 1987.</p></div><div id="ref-slaney94"><p>[58] M. Slaney, D. Naar, and R. F. Lyon, “Auditory model inversion for
sound separation,” in <em>IEEE international conference on acoustics,
speech and signal processing</em>, 1994.</p></div><div id="ref-lagrange07"><p>[59] M. Lagrange and G. Tzanetakis, “Sound source tracking and
formation using normalized cuts,” in <em>IEEE international conference on
acoustics, speech and signal processing</em>, 2007.</p></div><div id="ref-lagrange08"><p>[60] M. Lagrange, L. G. Martins, J. Murdoch, and G. Tzanetakis,
“Normalized cuts for predominant melodic source separation,” <em>IEEE
Transactions on Audio, Speech, and Language Processing</em>, vol. 16, no. 2,
pp. 278–290, Feb. 2008.</p></div><div id="ref-shi00"><p>[61] J. Shi and J. Malik, “Normalized cuts and image segmentation,”
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol.
22, no. 8, pp. 888–905, Aug. 2000.</p></div><div id="ref-ryynanen08"><p>[62] M. Ryynänen, T. Virtanen, J. Paulus, and A. Klapuri,
“Accompaniment separation and karaoke application based on automatic
melody transcription,” in <em>IEEE international conference on multimedia
and expo</em>, 2008.</p></div><div id="ref-ryynanen082"><p>[63] M. Ryynänen and A. Klapuri, “Automatic transcription of melody,
bass line, and chords in polyphonic music,” <em>Computer Music Journal</em>,
vol. 32, no. 3, pp. 72–86, Sep. 2008.</p></div><div id="ref-ding97"><p>[64] Y. Ding and X. Qian, “Processing of musical tones using a
combined quadratic polynomial-phase sinusoid and residual (QUASAR)
signal model,” <em>Journal of the Audio Engineering Society</em>, vol. 45, no.
7/8, pp. 571–584, Jul. 1997.</p></div><div id="ref-li06"><p>[65] Y. Li and D. Wang, “Singing voice separation from monaural
recordings,” in <em>7th international conference on music information
retrieval</em>, 2006.</p></div><div id="ref-li07"><p>[66] Y. Li and D. Wang, “Separation of singing voice from music
accompaniment for monaural recordings,” <em>IEEE Transactions on Audio,
Speech, and Language Processing</em>, vol. 15, no. 4, pp. 1475–1487, May
2007.</p></div><div id="ref-duxbury03"><p>[67] C. Duxbury, J. P. Bello, M. Davies, and M. Sandler, “Complex
domain onset detection for musical signals,” in <em>6th international
conference on digital audio effects</em>, 2003.</p></div><div id="ref-li05"><p>[68] Y. Li and D. Wang, “Detecting pitch of singing voice in
polyphonic audio,” in <em>IEEE international conference on acoustics,
speech and signal processing</em>, 2005.</p></div><div id="ref-wu03"><p>[69] M. Wu, D. Wang, and G. J. Brown, “A multipitch tracking algorithm
for noisy speech,” <em>IEEE Transactions on Audio, Speech, and Language
Processing</em>, vol. 11, no. 3, pp. 229–241, May 2003.</p></div><div id="ref-hu02"><p>[70] G. Hu and D. Wang, “Monaural speech segregation based on pitch
tracking and amplitude modulation,” <em>IEEE Transactions on Neural
Networks</em>, vol. 15, no. 5, pp. 1135–1150, Sep. 2002.</p></div><div id="ref-han07"><p>[71] Y. Han and C. Raphael, “Desoloing monaural audio using mixture
models,” in <em>7th international conference on music information
retrieval</em>, 2007.</p></div><div id="ref-roweis01"><p>[72] S. T. Roweis, “One microphone source separation,” in <em>Advances in
neural information processing systems 13</em>, MIT Press, 2001, pp. 793–799.</p></div><div id="ref-hsu08"><p>[73] C.-L. Hsu, J.-S. R. Jang, and T.-L. Tsai, “Separation of singing
voice from music accompaniment with unvoiced sounds reconstruction for
monaural recordings,” in <em>AES 125th convention</em>, 2008.</p></div><div id="ref-hsu10"><p>[74] C.-L. Hsu and J.-S. R. Jang, “On the improvement of singing voice
separation for monaural recordings using the MIR-1K dataset,” <em>IEEE
Transactions on Audio, Speech, and Language Processing</em>, vol. 18, no. 2,
pp. 310–319, Feb. 2010.</p></div><div id="ref-dressler062"><p>[75] K. Dressler, “Sinusoidal extraction using an efficient
implementation of a multi-resolution FFT,” in <em>9th international
conference on digital audio effects</em>, 2006.</p></div><div id="ref-scalart96"><p>[76] P. Scalart and J. V. Filho, “Speech enhancement based on a priori
signal to noise estimation,” in <em>IEEE international conference on
acoustics, speech and signal processing</em>, 1996.</p></div><div id="ref-raphael08"><p>[77] C. Raphael and Y. Han, “A classifier-based approach to
score-guided music audio source separation,” <em>Computer Music Journal</em>,
vol. 32, no. 1, pp. 51–59, 2008.</p></div><div id="ref-breiman84"><p>[78] L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen,
<em>Classification and regression trees</em>. Chapman; Hall/CRC, 1984.</p></div><div id="ref-cano09"><p>[79] E. Cano and C. Cheng, “Melody line detection and source
separation in classical saxophone recordings,” in <em>12th international
conference on digital audio effects</em>, 2009.</p></div><div id="ref-grollmisch11"><p>[80] S. Grollmisch, E. Cano, and C. Dittmar, “Songs2See: Learn to play
by playing,” in <em>AES 41st conference: Audio for games</em>, 2011, pp. P2–3.</p></div><div id="ref-dittmar12"><p>[81] C. Dittmar, E. Cano, J. Abeßer, and S. Grollmisch, “Music
information retrieval meets music education,” in <em>Multimodal music
processing</em>, Dagstuhl Publishing, 2012, pp. 95–120.</p></div><div id="ref-cano12"><p>[82] E. Cano, C. Dittmar, and G. Schuller, “Efficient implementation
of a system for solo and accompaniment separation in polyphonic music,”
in <em>20th european signal processing conference</em>, 2012.</p></div><div id="ref-dressler11"><p>[83] K. Dressler, “Pitch estimation by the pair-wise evaluation of
spectral peaks,” in <em>42nd aes conference on semantic audio</em>, 2011.</p></div><div id="ref-cano13"><p>[84] E. Cano, C. Dittmar, and G. Schuller, “Re-thinking sound
separation: Prior information and additivity constraints in separation
algorithms,” in <em>16th international conference on digital audio
effects</em>, 2013.</p></div><div id="ref-cano14"><p>[85] E. Cano, G. Schuller, and C. Dittmar, “Pitch-informed solo and
accompaniment separation towards its use in music education
applications,” <em>EURASIP Journal on Advances in Signal Processing</em>, vol.
2014, no. 23, Sep. 2014.</p></div><div id="ref-bosch12"><p>[86] J. J. Bosch, K. Kondo, R. Marxer, and J. Janer, “Score-informed
and timbre independent lead instrument separation in real-world
scenarios,” in <em>20th european signal processing conference</em>, 2012.</p></div><div id="ref-marxer12"><p>[87] R. Marxer, J. Janer, and J. Bonada, “Low-latency instrument
separation in polyphonic audio using timbre models,” in <em>10th
international conference on latent variable analysis and signal
separation</em>, 2012.</p></div><div id="ref-vaneph16"><p>[88] A. Vaneph, E. McNeil, and F. Rigaud, “An automated source
separation technology and its practical applications,” in <em>140th aes
convention</em>, 2016.</p></div><div id="ref-leglaive15"><p>[89] S. Leglaive, R. Hennequin, and R. Badeau, “Singing voice
detection with deep recurrent neural networks,” in <em>IEEE international
conference on acoustics, speech and signal processing</em>, 2015.</p></div><div id="ref-lee99"><p>[90] D. D. Lee and H. S. Seung, “Learning the parts of objects by
non-negative matrix factorization,” <em>Nature</em>, vol. 401, pp. 788–791,
Oct. 1999.</p></div><div id="ref-lee01"><p>[91] D. D. Lee and H. S. Seung, “Algorithms for non-negative matrix
factorization,” in <em>Advances in neural information processing systems
13</em>, MIT Press, 2001, pp. 556–562.</p></div><div id="ref-smaragdis03"><p>[92] P. Smaragdis and J. C. Brown, “Non-negative matrix factorization
for polyphonic music transcription,” in <em>IEEE workshop on applications
of signal processing to audio and acoustics</em>, 2003.</p></div><div id="ref-virtanen07"><p>[93] T. Virtanen, “Monaural sound source separation by nonnegative
matrix factorization with temporal continuity and sparseness criteria,”
<em>IEEE Transactions on Audio, Speech, and Language Processing</em>, vol. 15,
no. 3, pp. 1066–1074, Mar. 2007.</p></div><div id="ref-fevotte09"><p>[94] C. Févotte, “Nonnegative matrix factorization with the
Itakura-Saito divergence: With application to music analysis,” <em>Neural
Computation</em>, vol. 21, no. 3, pp. 793–830, Mar. 2009.</p></div><div id="ref-common94"><p>[95] P. Common, “Independent component analysis, a new concept?”
<em>Signal Processing</em>, vol. 36, no. 3, pp. 287–314, Apr. 1994.</p></div><div id="ref-vembu05"><p>[96] S. Vembu and S. Baumann, “Separation of vocals from polyphonic
audio recordings,” in <em>6th international conference on music information
retrieval</em>, 2005.</p></div><div id="ref-hermansky90"><p>[97] H. Hermansky, “Perceptual linear predictive (PLP) analysis of
speech,” <em>Journal of the Acoustical Society of America</em>, vol. 87, no. 4,
pp. 1738–1752, Apr. 1990.</p></div><div id="ref-nwe04"><p>[98] T. L. Nwe and Y. Wang, “Automatic detection of vocal segments in
popular songs,” in <em>5th international conference for music information
retrieval</em>, 2004.</p></div><div id="ref-casey00"><p>[99] M. A. Casey and A. Westner, “Separation of mixed audio sources by
independent subspace analysis,” in <em>International computer music
conference</em>, 2000.</p></div><div id="ref-chanrungutai08"><p>[100] A. Chanrungutai and C. A. Ratanamahatana, “Singing voice
separation for mono-channel music using non-negative matrix
factorization,” in <em>International conference on advanced technologies
for communications</em>, 2008.</p></div><div id="ref-chanrungutai082"><p>[101] A. Chanrungutai and C. A. Ratanamahatana, “Singing voice
separation in mono-channel music,” in <em>International symposium on
communications and information technologies</em>, 2008.</p></div><div id="ref-tikhonov63"><p>[102] A. N. Tikhonov, “Solution of incorrectly formulated problems and
the regularization method,” <em>Soviet Mathematics</em>, vol. 4, pp. 1035–1038,
1963.</p></div><div id="ref-marxer122"><p>[103] R. Marxer and J. Janer, “A Tikhonov regularization method for
spectrum decomposition in low latency audio source separation,” in <em>IEEE
international conference on acoustics, speech and signal processing</em>,
2012.</p></div><div id="ref-yang14"><p>[104] P.-K. Yang, C.-C. Hsu, and J.-T. Chien, “Bayesian singing-voice
separation,” in <em>15th international society for music information
retrieval conference</em>, 2014.</p></div><div id="ref-chien15"><p>[105] J.-T. Chien and P.-K. Yang, “Bayesian factorization and learning
for monaural source separation,” <em>IEEE/ACM Transactions on Audio,
Speech, and Language Processing</em>, vol. 24, no. 1, pp. 185–195, Jan.
2015.</p></div><div id="ref-cemgil09"><p>[106] A. T. Cemgil, “Bayesian inference for nonnegative matrix
factorisation models,” <em>Computational Intelligence and Neuroscience</em>,
vol. 2009, no. 4, pp. 1–17, Jan. 2009.</p></div><div id="ref-schmidt09"><p>[107] M. N. Schmidt, O. Winther, and L. K. Hansen, “Bayesian
non-negative matrix factorization,” in <em>8th international conference on
independent component analysis and signal separation</em>, 2009.</p></div><div id="ref-spiertz09"><p>[108] M. Spiertz and V. Gnann, “Source-filter based clustering for
monaural blind source separation,” in <em>12th international conference on
digital audio effects</em>, 2009.</p></div><div id="ref-smaragdis09"><p>[109] P. Smaragdis and G. J. Mysore, “Separation by ‘humming’:
User-guided sound extraction from monophonic mixtures,” in <em>IEEE
workshop on applications of signal processing to audio and acoustics</em>,
2009.</p></div><div id="ref-smaragdis07"><p>[110] P. Smaragdis, B. Raj, and M. Shashanka, “Supervised and
semi-supervised separation of sounds from single-channel mixtures,” in
<em>7th international conference on independent component analysis and
signal separation</em>, 2007.</p></div><div id="ref-nakamuray15"><p>[111] T. Nakamuray and H. Kameoka, “(L_p)-norm non-negative matrix
factorization and its application to singing voice enhancement,” in
<em>IEEE international conference on acoustics, speech and signal
processing</em>, 2015.</p></div><div id="ref-ortega70"><p>[112] J. M. Ortega and W. C. Rheinboldt, <em>Iterative solution of
nonlinear equations in several variables</em>. Academic Press, 1970.</p></div><div id="ref-kameoka06"><p>[113] H. Kameoka, M. Goto, and S. Sagayama, “Selective amplifier of
periodic and non-periodic components in concurrent audio signals with
spectral control envelopes,” Information Processing Society of Japan,
2006.</p></div><div id="ref-candes11"><p>[114] E. J. Candès, X. Li, Y. Ma, and J. Wright, “Robust principal
component analysis?” <em>Journal of the ACM</em>, vol. 58, no. 3, pp. 1–37, May
2011.</p></div><div id="ref-huang12"><p>[115] P.-S. Huang, S. D. Chen, P. Smaragdis, and M. Hasegawa-Johnson,
“Singing-voice separation from monaural recordings using robust
principal component analysis,” in <em>IEEE international conference on
acoustics, speech and signal processing</em>, 2012.</p></div><div id="ref-sprechmann12"><p>[116] P. Sprechmann, A. Bronstein, and G. Sapiro, “Real-time online
singing voice separation from monaural recordings using robust low-rank
modeling,” in <em>13th international society for music information
retrieval conference</em>, 2012.</p></div><div id="ref-recht10"><p>[117] B. Recht, M. Fazel, and P. A. Parrilo, “Guaranteed minimum-rank
solutions of linear matrix equations via nuclear norm minimization,”
<em>SIAM Review</em>, vol. 52, no. 3, pp. 471–501, Aug. 2010.</p></div><div id="ref-recht13"><p>[118] B. Recht and C. Ré, “Parallel stochastic gradient algorithms for
large-scale matrix completion,” <em>Mathematical Programming Computation</em>,
vol. 5, no. 2, pp. 201–226, Jun. 2013.</p></div><div id="ref-gregor10"><p>[119] K. Gregor and Y. LeCun, “Learning fast approximations of sparse
coding,” in <em>27th international conference on machine learning</em>, 2010.</p></div><div id="ref-zhang11"><p>[120] L. Zhang, Z. Chen, M. Zheng, and X. He, “Robust non-negative
matrix factorization,” <em>Frontiers of Electrical Electronic Engineering
China</em>, vol. 6, no. 2, pp. 192–200, Jun. 2011.</p></div><div id="ref-jeong14"><p>[121] I.-Y. Jeong and K. Lee, “Vocal separation using extended robust
principal component analysis with Schatten (P)/(L_p)-norm and scale
compression,” in <em>International workshop on machine learning for signal
processing</em>, 2014.</p></div><div id="ref-nie152"><p>[122] F. Nie, H. Wang, and H. Huang, “Joint Schatten (p)-norm and
(l_p)-norm robust matrix completion for missing value recovery,”
<em>Knowledge and Information Systems</em>, vol. 42, no. 3, pp. 525–544, Mar.
2015.</p></div><div id="ref-yang13"><p>[123] Y.-H. Yang, “Low-rank representation of both singing voice and
music accompaniment via learned dictionaries,” in <em>14th international
society for music information retrieval conference</em>, 2013.</p></div><div id="ref-mairal09"><p>[124] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online dictionary
learning for sparse coding,” in <em>26th annual international conference on
machine learning</em>, 2009.</p></div><div id="ref-chan16"><p>[125] T.-S. T. Chan and Y.-H. Yang, “Complex and quaternionic
principal component pursuit and its application to audio separation,”
<em>IEEE Signal Processing Letters</em>, vol. 23, no. 2, pp. 287–291, Feb.
2016.</p></div><div id="ref-peeters03"><p>[126] G. Peeters, “Deriving musical structures from signal analysis
for music audio summary generation: &quot;Sequence&quot; and &quot;state&quot; approach,” in
<em>International symposium on computer music multidisciplinary research</em>,
2003.</p></div><div id="ref-dannenberg08"><p>[127] R. B. Dannenberg and M. Goto, “Music structure analysis from
acoustic signals,” in <em>Handbook of signal processing in acoustics</em>,
Springer New York, 2008, pp. 305–331.</p></div><div id="ref-paulus10"><p>[128] J. Paulus, M. Müller, and A. Klapuri, “Audio-based music
structure analysis,” in <em>11th international society for music
information retrieval conference</em>, 2010.</p></div><div id="ref-rafii11"><p>[129] Z. Rafii and B. Pardo, “A simple music/voice separation system
based on the extraction of the repeating musical structure,” in <em>IEEE
international conference on acoustics, speech and signal processing</em>,
2011.</p></div><div id="ref-rafii13"><p>[130] Z. Rafii and B. Pardo, “REpeating Pattern Extraction Technique
(REPET): A simple method for music/voice separation,” <em>IEEE Transactions
on Audio, Speech, and Language Processing</em>, vol. 21, no. 1, pp. 73–84,
Jan. 2013.</p></div><div id="ref-rafii14"><p>[131] Z. Rafii, A. Liutkus, and B. Pardo, “REPET for
background/foreground separation in audio,” in <em>Blind source
separation</em>, Springer Berlin Heidelberg, 2014, pp. 395–411.</p></div><div id="ref-foote01"><p>[132] J. Foote and S. Uchihashi, “The beat spectrum: A new approach to
rhythm analysis,” in <em>IEEE international conference on multimedia and
expo</em>, 2001.</p></div><div id="ref-seetharaman17"><p>[133] P. Seetharaman, F. Pishdadian, and B. Pardo, “Music/voice
separation using the 2D Fourier transform,” in <em>IEEE workshop on
applications of signal processing to audio and acoustics</em>, 2017.</p></div><div id="ref-liutkus12"><p>[134] A. Liutkus, Z. Rafii, R. Badeau, B. Pardo, and G. Richard,
“Adaptive filtering for music/voice separation exploiting the
repeating musical structure,” in <em>IEEE international conference on
acoustics, speech and signal processing</em>, 2012.</p></div><div id="ref-rafii12"><p>[135] Z. Rafii and B. Pardo, “Music/voice separation using the
similarity matrix,” in <em>13th international society for music information
retrieval conference</em>, 2012.</p></div><div id="ref-foote99"><p>[136] J. Foote, “Visualizing music and audio using self-similarity,”
in <em>7th acm international conference on multimedia</em>, 1999.</p></div><div id="ref-rafii133"><p>[137] Z. Rafii and B. Pardo, “Online REPET-SIM for real-time speech
enhancement,” in <em>IEEE international conference on acoustics, speech and
signal processing</em>, 2013.</p></div><div id="ref-rafii15"><p>[138] Z. Rafii, A. Liutkus, and B. Pardo, “A simple user interface
system for recovering patterns repeating in time and frequency in
mixtures of sounds,” in <em>IEEE international conference on acoustics,
speech and signal processing</em>, 2015.</p></div><div id="ref-fitzgerald12"><p>[139] D. FitzGerald, “Vocal separation using nearest neighbours and
median filtering,” in <em>23rd iet irish signals and systems conference</em>,
2012.</p></div><div id="ref-liutkus14"><p>[140] A. Liutkus, Z. Rafii, B. Pardo, D. FitzGerald, and L. Daudet,
“Kernel spectrogram models for source separation,” in <em>4th joint
workshop on hands-free speech communication microphone arrays</em>, 2014.</p></div><div id="ref-liutkus142"><p>[141] A. Liutkus, D. FitzGerald, Z. Rafii, B. Pardo, and L. Daudet,
“Kernel additive models for source separation,” <em>IEEE Transactions on
Signal Processing</em>, vol. 62, no. 16, pp. 4298–4310, Aug. 2014.</p></div><div id="ref-liutkus15"><p>[142] A. Liutkus, D. FitzGerald, and Z. Rafii, “Scalable audio
separation with light kernel additive modelling,” in <em>IEEE international
conference on acoustics, speech and signal processing</em>, 2015.</p></div><div id="ref-pratzlich15"><p>[143] T. Prätzlich, R. Bittner, A. Liutkus, and M. Müller, “Kernel
additive modeling for interference reduction in multi-channel music
recordings,” in <em>IEEE international conference on acoustics, speech and
signal processing</em>, 2015.</p></div><div id="ref-fanoyela17"><p>[144] D. F. Yela, S. Ewert, D. FitzGerald, and M. Sandler,
“Interference reduction in music recordings combining kernel additive
modelling and non-negative matrix factorization,” in <em>IEEE international
conference on acoustics, speech and signal processing</em>, 2017.</p></div><div id="ref-moussallam12"><p>[145] M. Moussallam, G. Richard, and L. Daudet, “Audio source
separation informed by redundancy with greedy multiscale
decompositions,” in <em>20th european signal processing conference</em>,
2012.</p></div><div id="ref-mallat93"><p>[146] S. G. Mallat and Z. Zhang, “Matching pursuits with
time-frequency dictionaries,” <em>IEEE Transactions on Signal Processing</em>,
vol. 41, no. 12, pp. 3397–3415, Dec. 1993.</p></div><div id="ref-deif152"><p>[147] H. Deif, D. FitzGerald, W. Wang, and L. Gan, “Separation of
vocals from monaural music recordings using diagonal median filters and
practical time-frequency parameters,” in <em>IEEE international symposium
on signal processing and information technology</em>, 2015.</p></div><div id="ref-fitzgerald102"><p>[148] D. FitzGerald and M. Gainza, “Single channel vocal separation
using median filtering and factorisation techniques,” <em>ISAST
Transactions on Electronic and Signal Processing</em>, vol. 4, no. 1, pp.
62–73, Jan. 2010.</p></div><div id="ref-lee152"><p>[149] J.-Y. Lee and H.-G. Kim, “Music and voice separation using
log-spectral amplitude estimator based on kernel spectrogram models
backfitting,” <em>Journal of the Acoustical Society of Korea</em>, vol. 34, no.
3, pp. 227–233, 2015.</p></div><div id="ref-lee15"><p>[150] J.-Y. Lee, H.-S. Cho, and H.-G. Kim, “Vocal separation from
monaural music using adaptive auditory filtering based on kernel
back-fitting,” in <em>Interspeech</em>, 2015.</p></div><div id="ref-cho15"><p>[151] H.-S. Cho, J.-Y. Lee, and H.-G. Kim, “Singing voice separation
from monaural music based on kernel back-fitting using beta-order
spectral amplitude estimation,” in <em>16th international society for music
information retrieval conference</em>, 2015.</p></div><div id="ref-kim16"><p>[152] H.-G. Kim and J. Y. Kim, “Music/voice separation based on kernel
back-fitting using weighted (\beta)-order MMSE estimation,” <em>ETRI
Journal</em>, vol. 38, no. 3, pp. 510–517, Jun. 2016.</p></div><div id="ref-plourde08"><p>[153] E. Plourde and B. Champagne, “Auditory-based spectral amplitude
estimators for speech enhancement,” <em>IEEE Transactions on Audio, Speech,
and Language Processing</em>, vol. 16, no. 8, pp. 1614–1623, Nov. 2008.</p></div><div id="ref-raj07"><p>[154] B. Raj, P. Smaragdis, M. Shashanka, and R. Singh, “Separating a
foreground singer from background music,” in <em>International symposium on
frontiers of research on speech and music</em>, 2007.</p></div><div id="ref-smaragdis06"><p>[155] P. Smaragdis and B. Raj, “Shift-invariant probabilistic latent
component analysis,” MERL, 2006.</p></div><div id="ref-raj05"><p>[156] B. Raj and P. Smaragdis, “Latent variable decomposition of
spectrograms for single channel speaker separation,” in <em>IEEE workshop
on applications of signal processing to audio and acoustics</em>, 2005.</p></div><div id="ref-han11"><p>[157] J. Han and C.-W. Chen, “Improving melody extraction using
probabilistic latent component analysis,” in <em>IEEE international
conference on acoustics, speech and signal processing</em>, 2011.</p></div><div id="ref-boersma93"><p>[158] P. Boersma, “Accurate short-term analysis of the fundamental
frequency and the harmonics-to-noise ratio of a sampled sound,” in <em>IFA
proceedings 17</em>, 1993.</p></div><div id="ref-gomez12"><p>[159] E. Gómez, F. J. C. Quesada, J. Salamon, J. Bonada, P. V. Candea,
and P. C. Molero, “Predominant fundamental frequency estimation vs
singing voice separation for the automatic transcription of accompanied
flamenco singing,” in <em>13th international society for music information
retrieval conference</em>, 2012.</p></div><div id="ref-ono08"><p>[160] N. Ono, K. Miyamoto, J. L. Roux, H. Kameoka, and S. Sagayama,
“Separation of a monaural audio signal into harmonic/percussive
components by complementary diffusion on spectrogram,” in <em>16th european
signal processing conference</em>, 2008.</p></div><div id="ref-papadopoulos14"><p>[161] H. Papadopoulos and D. P. Ellis, “Music-content-adaptive robust
principal component analysis for a semantically consistent separation of
foreground and background in music audio signals,” in <em>17th
international conference on digital audio effects</em>, 2014.</p></div><div id="ref-chan15"><p>[162] T.-S. Chan <em>et al.</em>, “Vocal activity informed singing voice
separation with the iKala dataset,” in <em>IEEE international conference on
acoustics, speech and signal processing</em>, 2015.</p></div><div id="ref-jeong17"><p>[163] I.-Y. Jeong and K. Lee, “Singing voice separation using RPCA
with weighted (l_1)-norm,” in <em>13th international conference on latent
variable analysis and signal separation</em>, 2017.</p></div><div id="ref-virtanen08"><p>[164] T. Virtanen, A. Mesaros, and M. Ryynänen, “Combining pitch-based
inference and non-negative spectrogram factorization in separating
vocals from polyphonic music,” in <em>ISCA tutorial and research workshop
on statistical and perceptual audition</em>, 2008.</p></div><div id="ref-wang11"><p>[165] Y. Wang and Z. Ou, “Combining HMM-based melody extraction and
NMF-based soft masking for separating voice and accompaniment from
monaural audio,” in <em>IEEE international conference on acoustics, speech
and signal processing</em>, 2011.</p></div><div id="ref-klapuri06"><p>[166] A. Klapuri, “Multiple fundamental frequency estimation by
summing harmonic amplitudes,” in <em>7th international conference on music
information retrieval</em>, 2006.</p></div><div id="ref-hsu09"><p>[167] C.-L. Hsu, L.-Y. Chen, J.-S. R. Jang, and H.-J. Li, “Singing
pitch extraction from monaural polyphonic songs by contextual audio
modeling and singing harmonic enhancement,” in <em>10th international
society for music information retrieval conference</em>, 2009.</p></div><div id="ref-rafii142"><p>[168] Z. Rafii, Z. Duan, and B. Pardo, “Combining rhythm-based and
pitch-based methods for background and melody separation,” <em>IEEE/ACM
Transactions on Audio, Speech, and Language Processing</em>, vol. 22, no.
12, pp. 1884–1893, Sep. 2014.</p></div><div id="ref-duan10"><p>[169] Z. Duan and B. Pardo, “Multiple fundamental frequency estimation
by modeling spectral peaks and non-peak regions,” <em>IEEE Transactions on
Audio, Speech, and Language Processing</em>, vol. 18, no. 8, pp. 2121–2133,
Nov. 2010.</p></div><div id="ref-venkataramani14"><p>[170] S. Venkataramani, N. Nayak, P. Rao, and R. Velmurugan, “Vocal
separation using singer-vowel priors obtained from polyphonic audio,” in
<em>15th international society for music information retrieval conference</em>,
2014.</p></div><div id="ref-rao10"><p>[171] V. Rao and P. Rao, “Vocal melody extraction in the presence of
pitched accompaniment in polyphonic music,” <em>IEEE Transactions on Audio,
Speech, and Language Processing</em>, vol. 18, no. 8, pp. 2145–2154, Nov.
2010.</p></div><div id="ref-rao11"><p>[172] V. Rao, C. Gupta, and P. Rao, “Context-aware features for
singing voice detection in polyphonic music,” in <em>International workshop
on adaptive multimedia retrieval</em>, 2011.</p></div><div id="ref-kim112"><p>[173] M. Kim, J. Yoo, K. Kang, and S. Choi, “Nonnegative matrix
partial co-factorization for spectral and temporal drum source
separation,” <em>IEEE Journal of Selected Topics in Signal Processing</em>,
vol. 5, no. 6, pp. 1192–1204, Oct. 2011.</p></div><div id="ref-zhou14"><p>[174] L. Zhang, Z. Chen, M. Zheng, and X. He, “Nonnegative matrix and
tensor factorizations: An algorithmic perspective,” <em>IEEE Signal
Processing Magazine</em>, vol. 31, no. 3, pp. 54–65, May 2014.</p></div><div id="ref-ikemiya15"><p>[175] Y. Ikemiya, K. Yoshii, and K. Itoyama, “Singing voice analysis
and editing based on mutually dependent F0 estimation and source
separation,” in <em>IEEE international conference on acoustics, speech and
signal processing</em>, 2015.</p></div><div id="ref-ikemiya16"><p>[176] Y. Ikemiya, K. Itoyama, and K. Yoshii, “Singing voice separation
and vocal F0 estimation based on mutual combination of robust principal
component analysis and subharmonic summation,” <em>IEEE/ACM Transactions on
Audio, Speech, and Language Processing</em>, vol. 24, no. 11, pp. 2084–2095,
Nov. 2016.</p></div><div id="ref-hermes88"><p>[177] D. J. Hermes, “Measurement of pitch by subharmonic summation,”
<em>Journal of the Acoustical Society of America</em>, vol. 83, no. 1, pp.
257–264, Jan. 1988.</p></div><div id="ref-dobashi15"><p>[178] A. Dobashi, Y. Ikemiya, K. Itoyama, and K. Yoshii, “A music
performance assistance system based on vocal, harmonic, and percussive
source separation and content visualization for music audio signals,” in
<em>12th sound and music computing conference</em>, 2015.</p></div><div id="ref-hu15"><p>[179] Y. Hu and G. Liu, “Separation of singing voice using nonnegative
matrix partial co-factorization for singer identification,” <em>IEEE
Transactions on Audio, Speech, and Language Processing</em>, vol. 23, no. 4,
pp. 643–653, Apr. 2015.</p></div><div id="ref-yoo10"><p>[180] J. Yoo, M. Kim, K. Kang, and S. Choi, “Nonnegative matrix
partial co-factorization for drum source separation,” in <em>IEEE
international conference on acoustics, speech and signal processing</em>,
2010.</p></div><div id="ref-boersma01"><p>[181] P. Boersma, “PRAAT, a system for doing phonetics by computer,”
<em>Glot International</em>, vol. 5, no. 9/10, pp. 341–347, Dec. 2001.</p></div><div id="ref-li09"><p>[182] Y. Li, J. Woodruff, and D. Wang, “Monaural musical sound
separation based on pitch and common amplitude modulation,” <em>IEEE
Transactions on Audio, Speech, and Language Processing</em>, vol. 17, no. 7,
pp. 1361–1371, Sep. 2009.</p></div><div id="ref-raj04"><p>[183] B. Raj, M. L. Seltzer, and R. M. Stern, “Reconstruction of
missing features for robust speech recognition,” <em>Speech Communication</em>,
vol. 43, no. 4, pp. 275–296, Sep. 2004.</p></div><div id="ref-hu16"><p>[184] Y. Hu and G. Liu, “Monaural singing voice separation by
non-negative matrix partial co-factorization with temporal continuity
and sparsity criteria,” in <em>12th international conference on intelligent
computing</em>, 2016.</p></div><div id="ref-zhang15"><p>[185] X. Zhang, W. Li, and B. Zhu, “Latent time-frequency component
analysis: A novel pitch-based approach for singing voice separation,” in
<em>IEEE international conference on acoustics, speech and signal
processing</em>, 2015.</p></div><div id="ref-decheveigne02"><p>[186] A. de Cheveigné and H. Kawahara, “YIN, a fundamental frequency
estimator for speech and music,” <em>Journal of the Acoustical Society of
America</em>, vol. 111, no. 4, pp. 1917–1930, Apr. 2002.</p></div><div id="ref-zhu15"><p>[187] B. Zhu, W. Li, and L. Li, “Towards solving the bottleneck of
pitch-based singing voice separation,” in <em>23rd acm international
conference on multimedia</em>, 2015.</p></div><div id="ref-durrieu08"><p>[188] J.-L. Durrieu, G. Richard, and B. David, “Singer melody
extraction in polyphonic signals using source separation methods,” in
<em>IEEE international conference on acoustics, speech and signal
processing</em>, 2008.</p></div><div id="ref-durrieu09"><p>[189] J.-L. Durrieu, G. Richard, and B. David, “An iterative approach
to monaural musical mixture de-soloing,” in <em>IEEE international
conference on acoustics, speech and signal processing</em>, 2009.</p></div><div id="ref-durrieu10"><p>[190] J.-L. Durrieu, G. Richard, B. David, and C. Févotte,
“Source/filter model for unsupervised main melody extraction from
polyphonic audio signals,” <em>IEEE Transactions on Audio, Speech, and
Language Processing</em>, vol. 18, no. 3, pp. 564–575, Mar. 2010.</p></div><div id="ref-ozerov07"><p>[191] A. Ozerov, P. Philippe, F. Bimbot, and R. Gribonval, “Adaptation
of Bayesian models for single-channel source separation and its
application to voice/music separation in popular songs,” <em>IEEE
Transactions on Audio, Speech, and Language Processing</em>, vol. 15, no. 5,
pp. 1564–1578, Jul. 2007.</p></div><div id="ref-klatt90"><p>[192] D. H. Klatt and L. C. Klatt, “Analysis, synthesis, and
perception of voice quality variations among female and male talkers,”
<em>Journal of the Acoustical Society of America</em>, vol. 87, no. 2, pp.
820–857, Feb. 1990.</p></div><div id="ref-benaroya032"><p>[193] L. Benaroya, L. Mcdonagh, F. Bimbot, and R. Gribonval, “Non
negative sparse representation for Wiener based source separation with a
single sensor,” in <em>IEEE international conference on acoustics, speech
and signal processing</em>, 2003.</p></div><div id="ref-dhillon05"><p>[194] I. S. Dhillon and S. Sra, “Generalized nonnegative matrix
approximations with Bregman divergences,” in <em>Advances in neural
information processing systems 18</em>, MIT Press, 2005, pp. 283–290.</p></div><div id="ref-benaroya06"><p>[195] L. Benaroya, F. Bimbot, and R. Gribonval, “Audio source
separation with a single sensor,” <em>IEEE Transactions on Audio, Speech,
and Language Processing</em>, vol. 14, no. 1, pp. 191–199, Jan. 2006.</p></div><div id="ref-durrieu12"><p>[196] J.-L. Durrieu and J.-P. Thiran, “Musical audio source separation
based on user-selected F0 track,” in <em>10th international conference on
latent variable analysis and signal separation</em>, 2012.</p></div><div id="ref-fuentes2012"><p>[197] B. Fuentes, R. Badeau, and G. Richard, “Blind harmonic adaptive
decomposition applied to supervised source separation,” in <em>Signal
processing conference (eusipco), 2012 proceedings of the 20th european</em>,
2012, pp. 2654–2658.</p></div><div id="ref-brown91"><p>[198] J. C. Brown, “Calculation of a constant Q spectral transform,”
<em>Journal of the Acoustical Society of America</em>, vol. 89, no. 1, pp.
425–434, Jan. 1991.</p></div><div id="ref-brown92"><p>[199] J. C. Brown and M. S. Puckette, “An efficient algorithm for the
calculation of a constant Q transform,” <em>Journal of the Acoustical
Society of America</em>, vol. 92, no. 5, pp. 2698–2701, Nov. 1992.</p></div><div id="ref-schorkhuber10"><p>[200] C. Schörkhuber and A. Klapuri, “Constant-Q transform toolbox,”
in <em>7th sound and music computing conference</em>, 2010.</p></div><div id="ref-durrieu11"><p>[201] J.-L. Durrieu, B. David, and G. Richard, “A musically motivated
mid-level representation for pitch estimation and musical audio source
separation,” <em>IEEE Journal of Selected Topics in Signal Processing</em>,
vol. 5, no. 6, pp. 1180–1191, Oct. 2011.</p></div><div id="ref-joder12"><p>[202] C. Joder and B. Schuller, “Score-informed leading voice
separation from monaural audio,” in <em>13th international society for
music information retrieval conference</em>, 2012.</p></div><div id="ref-joder11"><p>[203] C. Joder, S. Essid, and G. Richard, “A conditional random field
framework for robust and scalable audio-to-score matching,” <em>IEEE
Transactions on Audio, Speech, and Language Processing</em>, vol. 19, no. 8,
pp. 2385–2397, Nov. 2011.</p></div><div id="ref-zhao14"><p>[204] R. Zhao, S. Lee, D.-Y. Huang, and M. Dong, “Soft constrained
leading voice separation with music score guidance,” in <em>9th
international symposium on chinese spoken language</em>, 2014.</p></div><div id="ref-durrieu092"><p>[205] J.-L. Durrieu, A. Ozerov, C. Févotte, G. Richard, and B. David,
“Main instrument separation from stereophonic audio signals using a
source/filter model,” in <em>17th european signal processing conference</em>,
2009.</p></div><div id="ref-janer13"><p>[206] J. Janer and R. Marxer, “Separation of unvoiced fricatives in
singing voice mixtures with semi-supervised NMF,” in <em>16th international
conference on digital audio effects</em>, 2013.</p></div><div id="ref-janer12"><p>[207] J. Janer, R. Marxer, and K. Arimoto, “Combining a harmonic-based
NMF decomposition with transient analysis for instantaneous percussion
separation,” in <em>IEEE international conference on acoustics, speech and
signal processing</em>, 2012.</p></div><div id="ref-marxer13"><p>[208] R. Marxer and J. Janer, “Modelling and separation of singing
voice breathiness in polyphonic mixtures,” in <em>16th international
conference on digital audio effects</em>, 2013.</p></div><div id="ref-degottex11"><p>[209] G. Degottex, A. Roebel, and X. Rodet, “Pitch transposition and
breathiness modification using a glottal source model and its adapted
vocal-tract filter,” in <em>IEEE international conference on acoustics,
speech and signal processing</em>, 2011.</p></div><div id="ref-ozerov102"><p>[210] A. Ozerov, E. Vincent, and F. Bimbot, “A general modular
framework for audio source separation,” in <em>9th international conference
on latent variable analysis and signal separation</em>, 2010.</p></div><div id="ref-ozerov12"><p>[211] A. Ozerov, E. Vincent, and F. Bimbot, “A general flexible
framework for the handling of prior information in audio source
separation,” <em>IEEE Transactions on Audio, Speech, and Language
Processing</em>, vol. 20, no. 4, pp. 1118–1133, May 2012.</p></div><div id="ref-salaun14"><p>[212] Y. Salaün <em>et al.</em>, “The flexible audio source separation
toolbox version 2.0,” in <em>IEEE international conference on acoustics,
speech and signal processing</em>, 2014.</p></div><div id="ref-hennequin16"><p>[213] R. Hennequin and F. Rigaud, “Long-term reverberation modeling
for under-determined audio source separation with application to vocal
melody extraction,” in <em>17th international society for music information
retrieval conference</em>, 2016.</p></div><div id="ref-singh10"><p>[214] R. Singh, B. Raj, and P. Smaragdis, “Latent-variable
decomposition based dereverberation of monaural and multi-channel
signals,” in <em>IEEE international conference on acoustics, speech and
signal processing</em>, 2010.</p></div><div id="ref-ono082"><p>[215] N. Ono, K. Miyamoto, H. Kameoka, and S. Sagayama, “A real-time
equalizer of harmonic and percussive components in music signals,” in
<em>9th international conference on music information retrieval</em>, 2008.</p></div><div id="ref-fitzgerald10"><p>[216] D. FitzGerald, “Harmonic/percussive separation using median
filtering,” in <em>13th international conference on digital audio effects</em>,
2010.</p></div><div id="ref-yang12"><p>[217] Y.-H. Yang, “On sparse and low-rank matrix decomposition for
singing voice separation,” in <em>20th acm international conference on
multimedia</em>, 2012.</p></div><div id="ref-jeong142"><p>[218] I.-Y. Jeong and K. Lee, “Vocal separation from monaural music
using temporal/spectral continuity and sparsity constraints,” <em>IEEE
Signal Processing Letters</em>, vol. 21, no. 10, pp. 1197–1200, Jun. 2014.</p></div><div id="ref-ochiai15"><p>[219] E. Ochiai, T. Fujisawa, and M. Ikehara, “Vocal separation by
constrained non-negative matrix factorization,” in <em>Asia-pacific signal
and information processing association annual summit and conference</em>,
2015.</p></div><div id="ref-watanabe16"><p>[220] T. Watanabe, T. Fujisawa, and M. Ikehara, “Vocal separation
using improved robust principal component analysis and post-processing,”
in <em>IEEE 59th international midwest symposium on circuits and systems</em>,
2016.</p></div><div id="ref-raguet13"><p>[221] H. Raguet, J. Fadili, and and Gabriel Peyré, “A generalized
forward-backward splitting,” <em>SIAM Journal on Imaging Sciences</em>, vol. 6,
no. 3, pp. 1199–1226, Jul. 2013.</p></div><div id="ref-hayashi16"><p>[222] A. Hayashi, H. Kameoka, T. Matsubayashi, and H. Sawada,
“Non-negative periodic component analysis for music source
separation,” in <em>Asia-pacific signal and information processing
association annual summit and conference</em>, 2016.</p></div><div id="ref-fitzgerald09"><p>[223] D. FitzGerald, M. Cranitch, and E. Coyle, “Using tensor
factorisation models to separate drums from polyphonic music,” in <em>12th
international conference on digital audio effects</em>, 2009.</p></div><div id="ref-tachibana14"><p>[224] H. Tachibana, N. Ono, and S. Sagayama, “Singing voice
enhancement in monaural music signals based on two-stage
harmonic/percussive sound separation on multiple resolution
spectrograms,” <em>IEEE/ACM Transactions on Audio, Speech and Language
Processing</em>, vol. 22, no. 1, pp. 228–237, Jan. 2014.</p></div><div id="ref-tachibana10"><p>[225] H. Tachibana, T. Ono, N. Ono, and S. Sagayama, “Melody line
estimation in homophonic music audio signals based on
temporal-variability of melodic source,” in <em>IEEE international
conference on acoustics, speech and signal processing</em>, 2010.</p></div><div id="ref-tachibana16"><p>[226] H. Tachibana, N. Ono, and S. Sagayama, “A real-time
audio-to-audio karaoke generation system for monaural recordings based
on singing voice suppression and key conversion techniques,” <em>Journal of
Information Processing</em>, vol. 24, no. 3, pp. 470–482, May 2016.</p></div><div id="ref-ono10"><p>[227] N. Ono <em>et al.</em>, “Harmonic and percussive sound separation and
its application to MIR-related tasks,” in <em>Advances in music information
retrieval</em>, Springer Berlin Heidelberg, 2010, pp. 213–236.</p></div><div id="ref-tachibana12"><p>[228] H. Tachibana, H. Kameoka, N. Ono, and S. Sagayama, “Comparative
evaluations of multiple harmonic/percussive sound separation techniques
based on anisotropic smoothness of spectrogram,” in <em>IEEE international
conference on acoustics, speech and signal processing</em>, 2012.</p></div><div id="ref-deif15"><p>[229] H. Deif, W. Wang, L. Gan, and S. Alhashmi, “A local
discontinuity based approach for monaural singing voice separation from
accompanying music with multi-stage non-negative matrix factorization,”
in <em>IEEE global conference on signal and information processing</em>, 2015.</p></div><div id="ref-zhu13"><p>[230] B. Zhu, W. Li, R. Li, and X. Xue, “Multi-stage non-negative
matrix factorization for monaural singing voice separation,” <em>IEEE
Transactions on Audio, Speech, and Language Processing</em>, vol. 21, no.
10, pp. 2096–2107, Oct. 2013.</p></div><div id="ref-driedger15"><p>[231] J. Driedger and M. Müller, “Extracting singing voice from music
recordings by cascading audio decomposition techniques,” in <em>IEEE
international conference on acoustics, speech and signal processing</em>,
2015.</p></div><div id="ref-driedger14"><p>[232] J. Driedger, M. Müller, and S. Disch, “Extending
harmonic-percussive separation of audio signals,” in <em>15th international
society for music information retrieval conference</em>, 2014.</p></div><div id="ref-talmon11"><p>[233] R. Talmon, I. Cohen, and S. Gannot, “Transient noise reduction
using nonlocal diffusion filters,” <em>IEEE/ACM Transactions on Audio,
Speech and Language Processing</em>, vol. 19, no. 6, pp. 1584–1599, Aug.
2011.</p></div><div id="ref-hsu12"><p>[234] C.-L. Hsu, D. Wang, J.-S. R. Jang, and K. Hu, “A tandem
algorithm for singing pitch extraction and voice separation from music
accompaniment,” <em>IEEE Transactions on Audio, Speech, and Language
Processing</em>, vol. 20, no. 5, pp. 1482–1491, Jul. 2012.</p></div><div id="ref-hu10"><p>[235] G. Hu and D. Wang, “A tandem algorithm for pitch estimation and
voiced speech segregation,” <em>IEEE Transactions on Audio, Speech, and
Language Processing</em>, vol. 18, no. 8, pp. 2067–2079, Nov. 2010.</p></div><div id="ref-rumelhart86"><p>[236] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning
internal representations by error propagation,” in <em>Parallel distributed
processing: Explorations in the microstructure of cognition, vol. 1</em>,
MIT Press Cambridge, 1986, pp. 318–362.</p></div><div id="ref-bryan13"><p>[237] N. J. Bryan and G. J. Mysore, “Interactive user-feedback for
sound source separation,” in <em>International conference on intelligent
user-interfaces, workshop on interactive machine learning</em>, 2013.</p></div><div id="ref-bryan132"><p>[238] N. J. Bryan and G. J. Mysore, “An efficient posterior
regularized latent variable model for interactive sound source
separation,” in <em>30th international conference on machine learning</em>,
2013.</p></div><div id="ref-bryan133"><p>[239] N. J. Bryan and G. J. Mysore, “Interactive refinement of
supervised and semi-supervised sound source separation estimates,” in
<em>IEEE international conference on acoustics, speech, and signal
processing</em>, 2013.</p></div><div id="ref-ganchev10"><p>[240] K. Ganchev, J. Graça, J. Gillenwater, and B. Taskar, “Posterior
regularization for structured latent variable models,” <em>Journal of
Machine Learning Research</em>, vol. 11, pp. 2001–2049, Mar. 2010.</p></div><div id="ref-ozerov13"><p>[241] A. Ozerov, N. Duong, and L. Chevallier, “Weighted nonnegative
tensor factorization: On monotonicity of multiplicative update rules and
application to user-guided audio source separation,” Technicolor, 2013.</p></div><div id="ref-jaureguiberry13"><p>[242] X. Jaureguiberry, G. Richard, P. Leveau, R. Hennequin, and E.
Vincent, “Introducing a simple fusion framework for audio source
separation,” in <em>IEEE international workshop on machine learning for
signal processing</em>, 2013.</p></div><div id="ref-jaureguiberry14"><p>[243] X. Jaureguiberry, E. Vincent, and G. Richard, “Variational
Bayesian model averaging for audio source separation,” in <em>IEEE workshop
on statistical signal processing workshop</em>, 2014.</p></div><div id="ref-jaureguiberry16"><p>[244] X. Jaureguiberry, E. Vincent, and G. Richard, “Fusion methods
for speech enhancement and audio source separation,” <em>IEEE/ACM
Transactions on Audio, Speech, and Language Processing</em>, vol. 24, no. 7,
pp. 1266–1279, Jul. 2016.</p></div><div id="ref-hoeting99"><p>[245] J. A. Hoeting, D. Madigan, A. E. Raftery, and C. T. Volinsky,
“Bayesian model averaging: A tutorial,” <em>Statistical Science</em>, vol.
14, no. 4, pp. 382–417, Nov. 1999.</p></div><div id="ref-mcvicar16"><p>[246] M. McVicar, R. Santos-Rodriguez, and T. D. Bie, “Learning to
separate vocals from polyphonic mixtures via ensemble methods and
structured output prediction,” in <em>IEEE international conference on
acoustics, speech and signal processing</em>, 2016.</p></div><div id="ref-jain90"><p>[247] A. K. Jain and F. Farrokhnia, “Unsupervised texture segmentation
using Gabor filters,” in <em>IEEE international conference on systems, man
and cybernetics</em>, 1990.</p></div><div id="ref-huang14"><p>[248] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,
“Singing-voice separation from monaural recordings using deep
recurrent neural networks,” in <em>15th international society for music
information retrieval conference</em>, 2014.</p></div><div id="ref-lacoste-julien13"><p>[249] S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher,
“Block-coordinate Frank-Wolfe optimization for structural SVMs,” in
<em>30th international conference on machine learning</em>, 2013.</p></div><div id="ref-manilow17"><p>[250] E. Manilow, P. Seetharaman, F. Pishdadian, and B. Pardo,
“Predicting algorithm efficacy for adaptive, multi-cue source
separation,” in <em>IEEE workshop on applications of signal processing to
audio and acoustics</em>, 2017.</p></div><div id="ref-wolf14"><p>[251] G. Wolf, S. Mallat, and S. Shamma, “Audio source separation with
time-frequency velocities,” in <em>IEEE international workshop on machine
learning for signal processing</em>, 2014.</p></div><div id="ref-wolf16"><p>[252] G. Wolf, S. Mallat, and S. Shamma, “Rigid motion model for audio
source separation,” <em>IEEE Transactions on Signal Processing</em>, vol. 64,
no. 7, pp. 1822–1831, Apr. 2016.</p></div><div id="ref-anden14"><p>[253] J. Andén and S. Mallat, “Deep scattering spectrum,” <em>IEEE
Transactions on Signal Processing</em>, vol. 62, no. 16, pp. 4114–4128, Aug.
2014.</p></div><div id="ref-bernard01"><p>[254] C. P. Bernard, “Discrete wavelet analysis for fast optic flow
computation,” <em>Applied and Computational Harmonic Analysis</em>, vol. 11,
no. 1, pp. 32–63, Jul. 2001.</p></div><div id="ref-yen14"><p>[255] F. Yen, Y.-J. Luo, and T.-S. Chi, “Singing voice separation
using spectro-temporal modulation features,” in <em>15th international
society for music information retrieval conference</em>, 2014.</p></div><div id="ref-yen15"><p>[256] F. Yen, M.-C. Huang, and T.-S. Chi, “A two-stage singing voice
separation algorithm using spectro-temporal modulation features,” in
<em>Interspeech</em>, 2015.</p></div><div id="ref-chi05"><p>[257] T. Chi, P. Rub, and S. A. Shamma, “Multiresolution
spectrotemporal analysis of complex sounds,” <em>Journal of the Acoustical
Society of America</em>, vol. 118, no. 2, pp. 887–906, Aug. 2005.</p></div><div id="ref-chi99"><p>[258] T. Chi, Y. Gao, M. C. Guyton, P. Ru, and S. Shamma,
“Spectro-temporal modulation transfer functions and speech
intelligibility,” <em>Journal of the Acoustical Society of America</em>, vol.
106, no. 5, pp. 2719–2732, Nov. 1999.</p></div><div id="ref-chan17"><p>[259] T.-S. T. Chan and Y.-H. Yang, “Informed group-sparse
representation for singing voice separation,” <em>IEEE Signal Processing
Letters</em>, vol. 24, no. 2, pp. 156–160, Feb. 2017.</p></div><div id="ref-yuan06"><p>[260] M. Yuan and Y. Lin, “Model selection and estimation in
regression with grouped variables,” <em>Journal of the Royal Statistical
Society Series B</em>, vol. 68, no. 1, pp. 49–67, Dec. 2006.</p></div><div id="ref-ma16"><p>[261] S. Ma, “Alternating proximal gradient method for convex
minimization,” <em>Journal of Scientific Computing</em>, vol. 68, no. 2, pp.
546–572, Aug. 2016.</p></div><div id="ref-liu13"><p>[262] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, “Robust
recovery of subspace structures by low-rank representation,” <em>IEEE
Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 35, no.
1, pp. 171–184, Jan. 2007.</p></div><div id="ref-varga93"><p>[263] A. Varga and H. J. Steeneken, “Assessment for automatic speech
recognition: II. NOISEX-92: A database and an experiment to study the
effect of additive noise on speech recognition systems,” <em>Speech
Communication</em>, vol. 12, no. 3, pp. 247–251, Jul. 1993.</p></div><div id="ref-garofolo93"><p>[264] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, and D.
S. Pallett, “DARPA TIMIT acoustic-phonetic continuous speech corpus
CD-ROM. NIST speech disc 1-1.1,” <em>NASA STI/Recon technical report n</em>.
1993.</p></div><div id="ref-sturmel12"><p>[265] N. Sturmel <em>et al.</em>, “Linear mixing models for active listening
of music productions in realistic studio conditions,” in <em>132nd aes
convention</em>, 2012.</p></div><div id="ref-MTGMASSdb"><p>[266] M. Vinyes, “MTG MASS database.” 2008.</p></div><div id="ref-vincent09"><p>[267] E. Vincent, S. Araki, and P. Bofill, “The 2008 signal separation
evaluation campaign: A community-based approach to large-scale
evaluation,” in <em>8th international conference on independent component
analysis and signal separation</em>, 2009.</p></div><div id="ref-araki10"><p>[268] S. Araki <em>et al.</em>, “The 2010 signal separation evaluation
campaign (SiSEC2010): - audio source separation -,” in <em>9th
international conference on latent variable analysis and signal
separation</em>, 2010.</p></div><div id="ref-araki12"><p>[269] S. Araki <em>et al.</em>, “The 2011 signal separation evaluation
campaign (SiSEC2011): - audio source separation -,” in <em>10th
international conference on latent variable analysis and signal
separation</em>, 2012.</p></div><div id="ref-vincent12"><p>[270] E. Vincent <em>et al.</em>, “The signal separation evaluation campaign
(2007-2010): Achievements and remaining challenges,” <em>Signal
Processing</em>, vol. 92, no. 8, pp. 1928–1936, Aug. 2012.</p></div><div id="ref-ono15"><p>[271] N. Ono, Z. Rafii, D. Kitamura, N. Ito, and A. Liutkus, “The 2015
signal separation evaluation campaign,” in <em>12th international
conference on latent variable analysis and signal separation</em>, 2015.</p></div><div id="ref-liutkus17"><p>[272] A. Liutkus <em>et al.</em>, “The 2016 signal separation evaluation
campaign,” in <em>13th international conference on latent variable analysis
and signal separation</em>, 2017.</p></div><div id="ref-liutkus11"><p>[273] A. Liutkus, R. Badeau, and G. Richard, “Gaussian processes for
underdetermined source separation,” <em>IEEE Transactions on Audio, Speech,
and Language Processing</em>, vol. 59, no. 7, pp. 3155–3167, Feb. 2011.</p></div><div id="ref-bittner14"><p>[274] R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam, and and
Juan P. Bello, “MedleyDB: A multitrack dataset for annotation-intensive
mir research,” in <em>15th international society for music information
retrieval conference</em>, 2014.</p></div><div id="ref-rafii17"><p>[275] Z. Rafii, A. Liutkus, F.-R. Stöter, S. I. Mimilakis, and R.
Bittner, “MUSDB18, a dataset for audio source separation.” Dec-2017.</p></div><div id="ref-ozerov05"><p>[276] A. Ozerov, P. Philippe, R. Gribonval, and F. Bimbot, “One
microphone singing voice separation using source-adapted models,” in
<em>IEEE workshop on applications of signal processing to audio and
acoustics</em>, 2005.</p></div><div id="ref-tsai04"><p>[277] W.-H. Tsai, D. Rogers, and H.-M. Wang, “Blind clustering of
popular music recordings based on singer voice characteristics,”
<em>Computer Music Journal</em>, vol. 28, no. 3, pp. 68–78, 2004.</p></div><div id="ref-gauvain94"><p>[278] J.-L. Gauvain and C.-H. Lee, “Maximum a posteriori estimation
for multivariate Gaussian mixture observations of Markov chains,” <em>IEEE
Transactions on Audio, Speech, and Language Processing</em>, vol. 2, no. 2,
pp. 291–298, Apr. 1994.</p></div><div id="ref-vincent10"><p>[279] E. Vincent, M. Jafari, S. Abdallah, M. Plumbley, and M. Davies,
“Probabilistic modeling paradigms for audio source separation,” in
<em>Machine audition: Principles, algorithms and systems</em>, IGI Global,
2010, pp. 162–185.</p></div><div id="ref-rafii132"><p>[280] Z. Rafii, D. L. Sun, F. G. Germain, and G. J. Mysore, “Combining
modeling of singing voice and background music for automatic separation
of musical mixtures,” in <em>14th international society for music
information retrieval conference</em>, 2013.</p></div><div id="ref-boulanger-lewandowski14"><p>[281] N. Boulanger-Lewandowski, G. J. Mysore, and M. Hoffman,
“Exploiting long-term temporal dependencies in NMF using recurrent
neural networks with application to source separation,” in <em>IEEE
international conference on acoustics, speech and signal processing</em>,
2014.</p></div><div id="ref-mysore10"><p>[282] G. J. Mysore, P. Smaragdis, and B. Raj, “Non-negative hidden
Markov modeling of audio with application to source separation,” in <em>9th
international conference on latent variable analysis and signal
separation</em>, 2010.</p></div><div id="ref-qian17"><p>[283] K. Qian, Y. Zhang, S. Chang, X. Yang, D. Florêncio, and M.
Hasegawa-Johnson, “Speech enhancement using bayesian wavenet,” <em>Proc.
Interspeech 2017</em>, pp. 2013–2017, 2017.</p></div><div id="ref-deng14"><p>[284] L. Deng and D. Yu, “Deep learning: Methods and applications,”
<em>Foundations and Trends in Signal Processing</em>, vol. 7, nos. 3-4, pp.
197–387, Jun. 2014.</p></div><div id="ref-lecun15"><p>[285] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” <em>Nature</em>,
vol. 521, pp. 436–444, May 2015.</p></div><div id="ref-goodfellow16"><p>[286] I. Goodfellow, Y. Bengio, and A. Courville, <em>Deep learning</em>. MIT
Press, 2016.</p></div><div id="ref-robbins51"><p>[287] H. Robbins and S. Monro, “A stochastic approximation method,”
<em>Annals of Mathematical Statistics</em>, vol. 22, no. 3, pp. 400–407, Sep.
1951.</p></div><div id="ref-rumelhart862"><p>[288] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning
representations by back-propagating errors,” <em>Nature</em>, vol. 323, pp.
533–536, Oct. 1986.</p></div><div id="ref-hermans13"><p>[289] M. Hermans and B. Schrauwen, “Training and analysing deep
recurrent neural networks,” in <em>26th international conference on neural
information processing systems</em>, 2013.</p></div><div id="ref-pascanu14"><p>[290] R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio, “How to
construct deep recurrent neural networks,” in <em>International conference
on learning representations</em>, 2014.</p></div><div id="ref-huang15"><p>[291] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,
“Joint optimization of masks and deep recurrent neural networks for
monaural source separation,” <em>IEEE/ACM Transactions on Audio, Speech,
and Language Processing</em>, vol. 23, 2015.</p></div><div id="ref-huang142"><p>[292] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,
“Deep learning for monaural speech separation,” in <em>IEEE international
conference on acoustics, speech and signal processing</em>, 2014.</p></div><div id="ref-uhlich15"><p>[293] S. Uhlich, F. Giron, and Y. Mitsufuji, “Deep neural network
based instrument extraction from music,” in <em>IEEE international
conference on acoustics, speech and signal processing</em>, 2015.</p></div><div id="ref-uhlich17"><p>[294] S. Uhlich <em>et al.</em>, “Improving music source separation based on
deep neural networks through data augmentation and network blending,” in
<em>IEEE international conference on acoustics, speech and signal
processing</em>, 2017.</p></div><div id="ref-simpson15"><p>[295] A. J. R. Simpson, G. Roma, and M. D. Plumbley, “Deep karaoke:
Extracting vocals from musical mixtures using a convolutional deep
neural network,” in <em>12th international conference on latent variable
analysis and signal separation</em>, 2015.</p></div><div id="ref-schlueter16"><p>[296] J. Schlüter, “Learning to pinpoint singing voice from weakly
labeled examples,” in <em>17th international society for music information
retrieval conference</em>, 2016.</p></div><div id="ref-chandna17"><p>[297] P. Chandna, M. Miron, J. Janer, and E. Gómez, “Monoaural audio
source separation using deep convolutional neural networks,” in <em>13th
international conference on latent variable analysis and signal
separation</em>, 2017.</p></div><div id="ref-mimilakis16"><p>[298] S. I. Mimilakis, E. Cano, J. Abeßer, and G. Schuller, “New
sonorities for jazz recordings: Separation and mixing using deep neural
networks,” in <em>2nd aes workshop on intelligent music production</em>, 2016.</p></div><div id="ref-mimilakis17"><p>[299] S. I. Mimilakis, K. Drossos, T. Virtanen, and G. Schuller, “A
recurrent encoder-decoder approach with skip-filtering connections for
monaural singing voice separation,” in <em>IEEE international workshop on
machine learning for signal processing</em>, 2017.</p></div><div id="ref-mimilakis172"><p>[300] S. I. Mimilakis, K. Drossos, J. F. Santos, G. Schuller, T.
Virtanen, and Y. Bengio, “Monaural singing voice separation with
skip-filtering connections and recurrent inference of time-frequency
mask,” in <em>IEEE international conference on acoustics, speech and signal
processing</em>, 2018.</p></div><div id="ref-jansson17"><p>[301] A. Jansson, E. Humphrey, N. Montecchio, R. Bittner, A. Kumar,
and T. Weyde, “Singing voice separation with deep U-Net convolutional
networks,” in <em>18th international society for music information
retrieval conferenceng</em>, 2017.</p></div><div id="ref-takahashi17"><p>[302] N. Takahashi and Y. Mitsufuji, “Multi-scale multi-band densenets
for audio source separation,” in <em>IEEE workshop on applications of
signal processing to audio and acoustics</em>, 2017.</p></div><div id="ref-hershey16"><p>[303] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep
clustering: Discriminative embeddings for segmentation and separation,”
in <em>IEEE international conference on acoustics, speech and signal
processing</em>, 2016.</p></div><div id="ref-isik16"><p>[304] Y. Isik, J. L. Roux, Z. Chen, S. Watanabe, and J. R. Hershey,
“Single-channel multispeaker separation using deep clustering,” in
<em>Interspeech</em>, 2016.</p></div><div id="ref-luo17"><p>[305] Y. Luo, Z. Chen, J. R. Hershey, J. L. Roux, and N. Mesgarani,
“Deep clustering and conventional networks for music separation:
Stronger together,” in <em>IEEE international conference on acoustics,
speech and signal processing</em>, 2017.</p></div><div id="ref-kim15"><p>[306] M. Kim and P. Smaragdis, “Adaptive denoising autoencoders: A
fine-tuning scheme to learn from test mixtures,” in <em>12th international
conference on latent variable analysis and signal separation</em>, 2015.</p></div><div id="ref-vincentp10"><p>[307] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A.
Manzagol, “Stacked denoising autoencoders: Learning useful
representations in a deep network with a local denoising criterion,”
<em>Journal of Machine Learning Research</em>, vol. 11, pp. 3371–3408, Dec.
2010.</p></div><div id="ref-grais16"><p>[308] E. M. Grais, G. Roma, A. J. R. Simpson, and M. D. Plumbley,
“Single channel audio source separation using deep neural network
ensembles,” in <em>140th aes convention</em>, 2016.</p></div><div id="ref-grais162"><p>[309] E. M. Grais, G. Roma, A. J. R. Simpson, and M. D. Plumbley,
“Combining mask estimates for single channel audio source separation
using deep neural networks,” in <em>Interspeech</em>, 2016.</p></div><div id="ref-grais17"><p>[310] E. M. Grais, G. Roma, A. J. R. Simpson, and M. D. Plumbley,
“Discriminative enhancement for single channel audio source separation
using deep neural networks,” in <em>13th international conference on latent
variable analysis and signal separation</em>, 2017.</p></div><div id="ref-grais172"><p>[311] E. M. Grais, G. Roma, A. J. R. Simpson, and M. D. Plumbley,
“Two-stage single-channel audio source separation using deep neural
networks,” <em>IEEE/ACM Transactions on Audio, Speech, and Language
Processing</em>, vol. 25, no. 9, pp. 1773–1783, Sep. 2017.</p></div><div id="ref-nie15"><p>[312] S. Nie <em>et al.</em>, “Joint optimization of recurrent networks
exploiting source auto-regression for source separation,” in
<em>Interspeech</em>, 2015.</p></div><div id="ref-sebastian16"><p>[313] J. Sebastian and H. A. Murthy, “Group delay based music source
separation using deep recurrent neural networks,” in <em>International
conference on signal processing and communications</em>, 2016.</p></div><div id="ref-yegnanarayana91"><p>[314] B. Yegnanarayana, H. A. Murthy, and V. R. Ramachandran,
“Processing of noisy speech using modified group delay functions,” in
<em>IEEE international conference on acoustics, speech and signal
processing</em>, 1991.</p></div><div id="ref-fan16"><p>[315] Z.-C. Fan, J.-S. R. Jang, and C.-L. Lu, “Singing voice
separation and pitch extraction from monaural polyphonic audio music via
DNN and adaptive pitch tracking,” in <em>IEEE international conference on
multimedia big data</em>, 2016.</p></div><div id="ref-avendano03"><p>[316] C. Avendano, “Frequency-domain source identification and
manipulation in stereo mixes for enhancement, suppression and re-panning
applications,” in <em>IEEE workshop on applications of signal processing to
audio and acoustics</em>, 2003.</p></div><div id="ref-avendano02"><p>[317] C. Avendano and J.-M. Jot, “Frequency domain techniques for
stereo to multichannel upmix,” in <em>AES 22nd international conference</em>,
2002.</p></div><div id="ref-barry04"><p>[318] D. Barry, B. Lawlor, and E. Coyle, “Sound source separation:
Azimuth discrimination and resynthesis,” in <em>7th international
conference on digital audio effects</em>, 2004.</p></div><div id="ref-vinyes06"><p>[319] M. Vinyes, J. Bonada, and A. Loscos, “Demixing commercial music
productions via human-assisted time-frequency masking,” in <em>120th aes
convention</em>, 2006.</p></div><div id="ref-cobos082"><p>[320] M. Cobos and J. J. López, “Stereo audio source separation based
on time-frequency masking and multilevel thresholding,” <em>Digital Signal
Processing</em>, vol. 18, no. 6, pp. 960–976, Nov. 2008.</p></div><div id="ref-yilmaz04"><p>[321] Ö. Yilmaz and S. Rickard, “Blind separation of speech mixtures
via time-frequency masking,” <em>IEEE Transactions on Signal Processing</em>,
vol. 52, no. 7, pp. 1830–1847, Jul. 2004.</p></div><div id="ref-otsu79"><p>[322] N. Otsu, “A threshold selection method from gray-level
histograms,” <em>IEEE Transactions on Systems, Man, and Cybernetics</em>, vol.
9, no. 1, pp. 62–66, Jan. 1979.</p></div><div id="ref-sofianos10"><p>[323] S. Sofianos, A. Ariyaeeinia, and R. Polfreman, “Towards
effective singing voice extraction from stereophonic recordings,” in
<em>IEEE international conference on acoustics, speech and signal
processing</em>, 2010.</p></div><div id="ref-sofianos102"><p>[324] S. Sofianos, A. Ariyaeeinia, and R. Polfreman, “Singing voice
separation based on non-vocal independent component subtraction,” in
<em>13th international conference on digital audio effects</em>, 2010.</p></div><div id="ref-sofianos12"><p>[325] S. Sofianos, A. Ariyaeeinia, R. Polfreman, and R. Sotudeh,
“H-semantics: A hybrid approach to singing voice separation,” <em>Journal
of the Audio Engineering Society</em>, vol. 60, no. 10, pp. 831–841, Oct.
2012.</p></div><div id="ref-kim11"><p>[326] M. Kim, S. Beack, K. Choi, and K. Kang, “Gaussian mixture model
for singing voice separation from stereophonic music,” in <em>AES 43rd
conference</em>, 2011.</p></div><div id="ref-cobos08"><p>[327] M. Cobos and J. J. López, “Singing voice separation combining
panning information and pitch tracking,” in <em>AES 124th convention</em>,
2008.</p></div><div id="ref-fitzgerald13"><p>[328] D. FitzGerald, “Stereo vocal extraction using ADRess and nearest
neighbours median filtering,” in <em>16th international conference on
digital audio effects</em>, 2013.</p></div><div id="ref-fitzgerald132"><p>[329] D. FitzGerald and R. Jaiswal, “Improved stereo instrumental
track recovery using median nearest-neighbour inpainting,” in <em>24th iet
irish signals and systems conference</em>, 2013.</p></div><div id="ref-alder12"><p>[330] A. Adler, V. Emiya, M. G. Jafari, M. Elad, R. Gribonval, and M.
D. Plumbley, “Audio inpainting,” <em>IEEE Transactions on Audio, Speech,
and Language Processing</em>, vol. 20, no. 3, pp. 922–932, Mar. 2012.</p></div><div id="ref-ozerov09"><p>[331] A. Ozerov and C. Févotte, “Multichannel nonnegative matrix
factorization in convolutive mixtures with application to blind audio
source separation,” in <em>IEEE international conference on acoustics,
speech and signal processing</em>, 2009.</p></div><div id="ref-ozerov10"><p>[332] A. Ozerov and C. Févotte, “Multichannel nonnegative matrix
factorization in convolutive mixtures for audio source separation,”
<em>IEEE Transactions on Audio, Speech, and Language Processing</em>, vol. 18,
no. 3, pp. 550–563, Mar. 2010.</p></div><div id="ref-ozerov11"><p>[333] A. Ozerov, C. Févotte, R. Blouet, and J.-L. Durrieu,
“Multichannel nonnegative tensor factorization with structured
constraints for user-guided audio source separation,” in <em>IEEE
international conference on acoustics, speech and signal processing</em>,
2011.</p></div><div id="ref-liutkus10"><p>[334] A. Liutkus, R. Badeau, and G. Richard, “Informed source
separation using latent components,” in <em>9th international conference on
latent variable analysis and signal separation</em>, 2010.</p></div><div id="ref-fevotte10"><p>[335] C. Févotte and A. Ozerov, “Notes on nonnegative tensor
factorization of the spectrogram for audio source separation:
Statistical insights and towards self-clustering of the spatial cues,”
in <em>7th international symposium on computer music modeling and
retrieval</em>, 2010.</p></div><div id="ref-ozerov14"><p>[336] A. Ozerov, N. Duong, and L. Chevallier, “On monotonicity of
multiplicative update rules for weighted nonnegative tensor
factorization,” in <em>International symposium on nonlinear theory and its
applications</em>, 2014.</p></div><div id="ref-sawada11"><p>[337] H. Sawada, H. Kameoka, S. Araki, and N. Ueda, “New formulations
and efficient algorithms for multichannel NMF,” in <em>IEEE workshop on
applications of signal processing to audio and acoustics</em>, 2011.</p></div><div id="ref-sawada12"><p>[338] H. Sawada, H. Kameoka, S. Araki, and N. Ueda, “Efficient
algorithms for multichannel extensions of Itakura-Saito nonnegative
matrix factorization,” in <em>IEEE international conference on acoustics,
speech and signal processing</em>, 2012.</p></div><div id="ref-sawada13"><p>[339] H. Sawada, H. Kameoka, S. Araki, and N. Ueda, “Multichannel
extensions of non-negative matrix factorization with complex-valued
data,” <em>IEEE Transactions on Audio, Speech, and Language Processing</em>,
vol. 21, no. 5, pp. 971–982, May 2013.</p></div><div id="ref-sivasankaran15"><p>[340] S. Sivasankaran <em>et al.</em>, “Robust ASR using neural network based
speech enhancement and feature simulation,” in <em>IEEE automatic speech
recognition and understanding workshop</em>, 2015.</p></div><div id="ref-nugraha162"><p>[341] A. A. Nugraha, A. Liutkus, and E. Vincent, “Multichannel audio
source separation with deep neural networks,” <em>IEEE/ACM Transactions on
Audio, Speech, and Language Processing</em>, vol. 24, no. 9, pp. 1652–1664,
Sep. 2016.</p></div><div id="ref-nugraha15"><p>[342] A. A. Nugraha, A. Liutkus, and E. Vincent, “Multichannel audio
source separation with deep neural networks,” Inria, 2015.</p></div><div id="ref-nugraha16"><p>[343] A. A. Nugraha, A. Liutkus, and E. Vincent, “Multichannel music
separation with deep neural networks,” in <em>24th european signal
processing conference</em>, 2016.</p></div><div id="ref-duong10"><p>[344] N. Q. K. Duong, E. Vincent, and R. Gribonval, “Under-determined
reverberant audio source separation using a full-rank spatial covariance
model,” <em>IEEE Transactions on Audio, Speech, and Language Processing</em>,
vol. 18, no. 7, pp. 1830–1840, Sep. 2010.</p></div><div id="ref-ozerov112"><p>[345] A. Ozerov, A. Liutkus, R. Badeau, and G. Richard, “Informed
source separation: Source coding meets source separation,” in <em>IEEE
workshop on applications of signal processing to audio and acoustics</em>,
2011.</p></div><div id="ref-zwicker13"><p>[346] E. Zwicker and H. Fastl, <em>Psychoacoustics: Facts and models</em>.
Springer-Verlag Berlin Heidelberg, 2013.</p></div><div id="ref-rix01"><p>[347] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra,
“Perceptual evaluation of speech quality (PESQ)-a new method for
speech quality assessment of telephone networks and codecs,” in <em>IEEE
international conference on acoustics, speech and signal processing</em>,
2001.</p></div><div id="ref-wang09"><p>[348] Z. Wang and A. C. Bovik, “Mean squared error: Love it or leave
it? A new look at signal fidelity measures,” <em>IEEE Signal Processing
Magazine</em>, vol. 26, no. 1, pp. 98–117, Jan. 2009.</p></div><div id="ref-barker15"><p>[349] J. Barker, R. Marxer, E. Vincent, and S. Watanabe, “The third
‘CHiME’ speech separation and recognition challenge: Dataset, task and
baselines,” in <em>IEEE workshop on automatic speech recognition and
understanding</em>, 2015.</p></div><div id="ref-recommendation2001MUSHRA"><p>[350] I. Recommendation, “Bs. 1534-1. method for the subjective
assessment of intermediate sound quality (MUSHRA),” <em>International
Telecommunications Union, Geneva</em>, 2001.</p></div><div id="ref-vincent062"><p>[351] E. Vincent, M. Jafari, and M. Plumbley, “Preliminary guidelines
for subjective evaluation of audio source separation algorithms,” in
<em>ICA research network international workshop</em>, 2006.</p></div><div id="ref-cano11"><p>[352] E. Cano, C. Dittmar, and G. Schuller, “Influence of phase,
magnitude and location of harmonic components in the perceived quality
of extracted solo signals,” in <em>AES 42nd conference on semantic audio</em>,
2011.</p></div><div id="ref-fevotte05"><p>[353] C. Févotte, R. Gribonval, and E. Vinvent, “BSS_EVAL toolbox
user guide - revision 2.0,” IRISA, 2005.</p></div><div id="ref-vincent06"><p>[354] E. Vincent, R. Gribonval, and C. Févotte, “Performance
measurement in blind audio source separation,” <em>IEEE Transactions on
Audio, Speech, and Language Processing</em>, vol. 14, no. 4, pp. 1462–1469,
Jul. 2006.</p></div><div id="ref-fox07"><p>[355] B. Fox, A. Sabin, B. Pardo, and A. Zopf, “Modeling perceptual
similarity of audio signals for blind source separation evaluation,” in
<em>7th international conference on latent variable analysis and signal
separation</em>, 2007.</p></div><div id="ref-fox072"><p>[356] B. Fox and B. Pardo, “Towards a model of perceived quality of
blind audio source separation,” in <em>IEEE international conference on
multimedia and expo</em>, 2007.</p></div><div id="ref-kornycky08"><p>[357] J. Kornycky, B. Gunel, and A. Kondoz, “Comparison of subjective
and objective evaluation methods for audio source separation,” <em>Journal
of the Acoustical Society of America</em>, vol. 4, no. 1, 2008.</p></div><div id="ref-emiya10"><p>[358] V. Emiya, E. Vincent, N. Harlander, and V. Hohmann,
“Multi-criteria subjective and objective evaluation of audio source
separation,” in <em>38th international aes conference</em>, 2010.</p></div><div id="ref-emiya11"><p>[359] V. Emiya, E. Vincent, N. Harlander, and V. Hohmann, “Subjective
and objective quality assessment of audio source separation,” <em>IEEE
Transactions on Audio, Speech, and Language Processing</em>, vol. 19, no. 7,
pp. 2046–2057, Sep. 2011.</p></div><div id="ref-vincent122"><p>[360] E. Vincent, “Improved perceptual metrics for the evaluation of
audio source separation,” in <em>10th international conference on latent
variable analysis and signal separation</em>, 2012.</p></div><div id="ref-cartwright16"><p>[361] M. Cartwright, B. Pardo, G. J. Mysore, and M. Hoffman, “Fast and
easy crowdsourced perceptual audio evaluation,” in <em>IEEE international
conference on acoustics, speech and signal processing</em>, 2016.</p></div><div id="ref-gupta15"><p>[362] U. Gupta, E. Moore, and A. Lerch, “On the perceptual relevance
of objective source separation measures for singing voice separation,”
in <em>IEEE workshop on applications of signal processing to audio and
acoustics</em>, 2005.</p></div><div id="ref-stoter16"><p>[363] F.-R. Stöter, A. Liutkus, R. Badeau, B. Edler, and P. Magron,
“Common fate model for unison source separation,” in <em>IEEE
international conference on acoustics, speech and signal processing</em>,
2016.</p></div><div id="ref-roma16"><p>[364] G. Roma, E. M. Grais, A. J. Simpson, I. Sobieraj, and M. D.
Plumbley, “Untwist: A new toolbox for audio source separation,” in <em>17th
international society on music information retrieval conference</em>, 2016.</p></div></div></div><div class="content edit-link"><a href="https://github.com/sigsep/tmp_website/edit/master/content/literature/README.md" target="_blank" rel="noopener noreferrer">Edit this page on github</a><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></div><!----></div></div></div>
    <script src="/tmp_website/assets/js/3.ffd6b151.js" defer></script><script src="/tmp_website/assets/js/app.ff25cdb7.js" defer></script>
  </body>
</html>
